# Cortex Core MVP – Application Integration and Startup Guide

## Overview

In this mini-project, we tie together all the components built in previous steps to create a fully functional application startup. **Application Integration and Startup (Mini-Project #9)** focuses on assembling the Cortex Core MVP – wiring up the FastAPI app, registering background tasks, initializing external service clients, and ensuring the system starts and shuts down cleanly. By the end of this step, the Cortex Core will run end-to-end, with all major parts (event bus, orchestrator, service clients, authentication) integrated in a simple, reliable way.

**Purpose and Scope:** This guide assumes that mini-projects #1–8 have implemented the core pieces (API endpoints, data models, in-memory event bus, response handler logic, memory & cognition service clients, etc.). Our goal now is to integrate these pieces into the FastAPI application lifecycle. We will:

- **Compose the FastAPI app** with all API routers included (auth, input, output, config), and apply authentication dependencies to protected routes.
- **Instantiate global components** like the internal EventBus and service clients for Memory and Cognition, attaching them to the app for use across the system.
- **Configure startup events** to initialize resources: connect to external services via the FastMCP clients, start the asynchronous ResponseHandler loop, and (optionally) launch the memory and cognition microservice processes for development convenience.
- **Configure shutdown events** to gracefully terminate background tasks and connections: stop the ResponseHandler loop and disconnect service clients (and stop any spawned processes) to avoid resource leaks.
- **Ensure clean separation of concerns:** The integration code will remain straightforward, acting as “glue” that connects existing components without introducing new business logic. We will strictly follow the project’s minimalist architecture principles – no unnecessary abstractions or global state beyond what’s needed, and using FastAPI’s built-in mechanisms for lifecycle management.

By following this guide, an AI assistant or developer can implement the application startup sequence in isolation, confident that each component is integrated in a clear, testable manner. The result will be a **fully wired application** where a client can send a message to the `/input` endpoint, the system processes it in the background (using Memory and Cognition services), and the answer is streamed back through the `/output/stream` SSE endpoint – all with proper initialization and cleanup around these flows.

## Integration Approach

Our approach to integration is to **keep it simple and idiomatic**. We will use FastAPI’s events and state to hook things up, rather than inventing custom frameworks. Each part of the startup sequence corresponds to a well-defined responsibility:

- **Application Composition:** The `app/main.py` will create the FastAPI app and include all routers, just as done in earlier scaffolding, but now also configure global dependencies like authentication for routes that need it.
- **Global Component Initialization:** We ensure one instance of the EventBus is created and accessible, and the FastMCP client connections to Memory and Cognition services are prepared. We’ll attach these instances to `app.state` to clearly associate them with the app’s lifecycle (this also makes them easy to access throughout the codebase or to replace in tests). This avoids relying on hidden globals and makes ownership explicit.
- **Startup Event:** Using `@app.on_event("startup")`, we’ll perform asynchronous initialization when the server starts:
  - Connect to the Memory and Cognition services via their clients (unless those clients use lazy connection on first call). This ensures we fail fast if the services are unavailable, or log a successful connection for visibility.
  - Launch the **ResponseHandler** background task that listens on the EventBus for new input events and processes them. This is done with `asyncio.create_task` so it runs in the background without blocking the startup. By the time any input arrives, this handler will be up and waiting.
  - (Optional) If we want a one-command setup for development, we can also start the Memory and Cognition service processes here (as separate threads or subprocesses). This is purely a dev convenience – in a real deployment those would run separately. We will show how to do this in a contained way so it doesn’t blur the architecture boundaries.
  - Log or print a message indicating the application is ready, possibly including important endpoint URLs for quick reference.
- **Shutdown Event:** Using `@app.on_event("shutdown")`, we’ll define cleanup steps when the application is stopping:
  - Disconnect the FastMCP clients from Memory and Cognition services (if the library has explicit close methods) to free sockets and resources.
  - Signal the ResponseHandler task to stop. We’ll cancel the async task or set a flag so that the loop in the handler can break out gracefully.
  - If we started the external service processes in startup, we’ll also terminate them here.
  - This ensures no background threads or connections are left hanging after the server stops.
- **Minimal Glue Code:** The integration will be done with straightforward code in `main.py` and slight enhancements to existing modules. We avoid any “magic” or overly clever injection patterns – everything is either attached to `app.state` or imported directly as a singleton. This makes the startup sequence very transparent. It should read almost like a script that an engineer could have written by hand to assemble the system. This aligns with our **Implementation Philosophy** of _ruthless simplicity_ and _direct integration_: we use FastAPI and the libraries in their standard way, with no custom frameworks or excessive indirection.
- **Testability:** By structuring integration through `app.state` and clear module interfaces, we keep the system test-friendly. For example, one could replace `app.state.memory_client` with a dummy in a test client, or inject a fake EventBus if needed. The ResponseHandler loop and service clients are simple functions/classes that can be invoked or stubbed in isolation. We’ll note places where this design benefits testing.

With this plan, let’s implement the necessary changes and additions in each relevant file. Below, we provide the final content of each file (or updated content, if it existed as a stub) along with explanations of their roles in the startup process.

### Application Assembly – Main FastAPI App (`app/main.py`)

First, we update the main application file to initialize the FastAPI app and integrate all components. In earlier steps we created the app and included routers; now we will also configure the authentication dependency for protected routes, attach global objects to `app.state`, and define startup/shutdown events.

**File: `app/main.py`**

```python
import os
import asyncio
import logging
from fastapi import FastAPI, Depends
from fastapi.middleware.cors import CORSMiddleware

# Import API routers
from app.api import auth, input, output, config
# Import utils for authentication
from app.utils.auth import get_current_user
# Import core components and backend clients
from app.core import event_bus
from app.backend.memory_client import MemoryClient, MEMORY_SERVICE_URL
from app.backend.cognition_client import CognitionClient, COGNITION_SERVICE_URL
# If needed, import service launcher (optional)
# from app.backend import memory_service, cognition_service

# Initialize FastAPI app
app = FastAPI(title="Cortex Core MVP")

# Apply CORS middleware if needed (allow frontend to call APIs) – optional
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Instantiate global components
# Create a single EventBus instance for the application
app.state.event_bus = event_bus.EventBus()  # in-memory pub/sub
# Instantiate Memory and Cognition service clients (FastMCP clients)
app.state.memory_client = MemoryClient(MEMORY_SERVICE_URL)
app.state.cognition_client = CognitionClient(COGNITION_SERVICE_URL)

# Include API routers
# Public/auth routes (no dependency on auth)
app.include_router(auth.router)
# Protected routes (require JWT auth) – use dependencies to enforce auth
app.include_router(input.router, dependencies=[Depends(get_current_user)])
app.include_router(output.router, dependencies=[Depends(get_current_user)])
app.include_router(config.router, dependencies=[Depends(get_current_user)])

# Simple health check endpoint (unprotected)
@app.get("/health", tags=["system"])
async def health_check():
    """Health check endpoint to verify that the service is running."""
    return {"status": "ok"}

# Startup event: initialize connections and background tasks
@app.on_event("startup")
async def on_startup():
    # Connect to external Memory & Cognition services via FastMCP clients
    try:
        await app.state.memory_client.connect()
        await app.state.cognition_client.connect()
        logging.info("Connected to Memory and Cognition services.")
    except Exception as e:
        logging.error(f"Failed to connect to external services: {e}")
        # We choose not to raise here; the app can still run with limited functionality

    # Optionally, start backend services in-process for dev (if not running externally)
    # This is a convenience to run the whole system with one command.
    if os.getenv("START_EMBEDDED_SERVICES") == "true":
        import subprocess
        logging.info("Launching Memory and Cognition services as subprocesses...")
        app.state._memory_proc = subprocess.Popen(["python", "-m", "app.backend.memory_service"])
        app.state._cognition_proc = subprocess.Popen(["python", "-m", "app.backend.cognition_service"])

    # Launch the background response handler task
    from app.core.response_handler import response_handler
    app.state.response_task = asyncio.create_task(response_handler(
        event_bus=app.state.event_bus,
        memory_client=app.state.memory_client,
        cognition_client=app.state.cognition_client
    ))
    logging.info("Response handler task started. Cortex Core is fully initialized.")

# Shutdown event: clean up connections and tasks
@app.on_event("shutdown")
async def on_shutdown():
    logging.info("Shutting down Cortex Core...")
    # Cancel the background response handler task gracefully
    task = app.state.response_task
    if task:
        task.cancel()
        try:
            await task
        except asyncio.CancelledError:
            logging.info("Response handler task cancelled.")
    # Disconnect service clients
    try:
        await app.state.memory_client.disconnect()
        await app.state.cognition_client.disconnect()
        logging.info("Disconnected from Memory and Cognition services.")
    except Exception as e:
        logging.warning(f"Error during disconnect: {e}")
    # Terminate any subprocesses if started
    if getattr(app.state, "_memory_proc", None):
        app.state._memory_proc.terminate()
    if getattr(app.state, "_cognition_proc", None):
        app.state._cognition_proc.terminate()
    logging.info("Shutdown complete. Goodbye.")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

**Explanation:** We create the FastAPI `app` as usual and then **attach global components** to it:

- An `EventBus` instance (`app.state.event_bus`) is created to handle in-memory publish/subscribe of events. This is the same EventBus class defined in `app.core.event_bus`, but by instantiating it here and storing it in `app.state` we make its lifecycle tied to the app. Any part of the application can import or retrieve this instance to publish or subscribe to events (for example, the input endpoint will publish events to it, and the ResponseHandler will subscribe to it).
- We instantiate the **MemoryClient** and **CognitionClient** (from the `app.backend` package) and store them in `app.state` as well. These clients manage the connections to the external Memory Service and Cognition Service (via the FastMCP protocol). By doing this in `main.py`, we ensure only one connection to each service is used throughout the app, and we can perform connection setup/teardown in the startup and shutdown events. The service URLs are read from constants (or environment variables) defined in those modules. No complex dependency injection framework is needed – we simply keep references on `app.state` and import them where needed.

Next, we include all the API routers. For **protected routes** (input, output, config), we apply the `Depends(get_current_user)` dependency at the router level. This means every endpoint in those routers will require a valid JWT token (verified by our Auth0 helper) before proceeding. Public routes (like auth routes for login/verification, or the health check) are included without this dependency. This keeps authentication enforcement centralized and consistent.

We also define a basic `/health` endpoint (as done in initial setup) to confirm the server is running; it remains unsecured for ease of use by health monitors.

The **startup event handler** `on_startup` performs integration tasks when the server starts:

- It calls `await memory_client.connect()` and `await cognition_client.connect()`. These are async methods (defined in our client classes) that establish connections to the external services (e.g., opening a network connection or handshake via FastMCP). We wrap this in a try/except to log errors but not crash the app if they fail – in an MVP, we might allow the core to run in a degraded mode (e.g., if Memory service isn’t available, perhaps the assistant will still try to answer without context). In a real scenario, you might choose to `raise` and stop startup on failure.
- It optionally launches the Memory and Cognition service processes if an environment variable (`START_EMBEDDED_SERVICES`) is set. This is a development convenience: it uses Python’s `subprocess.Popen` to start each service module in a separate process. This way, a developer can set `START_EMBEDDED_SERVICES=true` and simply run `uvicorn app.main:app` to bring up the entire system (the core plus the two service backends). We log that these are being launched. We store the process handles in `app.state` (`_memory_proc` and `_cognition_proc`) so we can terminate them later on shutdown. **Note:** In a production or more strictly separated environment, you would run the Memory and Cognition services independently, not via the core’s startup. We keep this out-of-line (guarded by an env flag) so it doesn’t affect the normal operation if not needed.
- It creates the **Response Handler** background task by calling `asyncio.create_task(...)` on the `response_handler()` coroutine (which we will implement in `core/response_handler.py`). We pass in the necessary components (event bus and clients) as arguments to the handler function so it knows where to get events and how to call services. Storing the task in `app.state.response_task` allows us to manage it later. This background task will run indefinitely, listening for new input events and processing them – essentially powering the AI’s response generation loop in the background.
- Finally, we log that the system started successfully. (Here we used Python’s logging for simplicity; you can also just use `print` statements if preferred. Logging provides level control and can integrate with UVicorn’s logs.)

The **shutdown event handler** `on_shutdown` runs when the application is about to stop (for example, when you hit Ctrl+C or send a terminate signal to Uvicorn). In this function:

- We retrieve the background response task and cancel it. Cancelling the task will raise a cancellation exception inside the task’s loop (as we’ll handle in the Response Handler code) and allow it to exit. We `await task` to let it finish cleanup (if any) and catch the `CancelledError` to avoid it being treated as an error.
- We call `await memory_client.disconnect()` and `await cognition_client.disconnect()` to close connections to the external services. This ensures we don’t leave sockets open. If the FastMCP library requires or provides a close, we invoke it; otherwise this might be a no-op. We log the result.
- If we started the service subprocesses in startup (detected by checking `app.state._memory_proc`), we terminate those processes. This prevents orphaned processes from continuing to run after the core app stops. We don’t wait for them to finish explicitly (for MVP), but in a real scenario you might ensure they shut down cleanly.
- We log that shutdown is complete. The application process will then exit fully.

With `app/main.py` set up this way, we have a central place that **assembles the application**: it brings together the routers (API layer), the core components (event bus, background tasks), and external integrations (service clients, Auth0 auth). The code remains straightforward and in line with FastAPI’s recommended patterns. By using `app.state` and event handlers, we maintain a clear lifecycle: all initialization is done before serving requests, and cleanup happens at the end, keeping resource management explicit.

### In-Memory Event Bus (`app/core/event_bus.py`)

The EventBus is a core piece that enables decoupling between the API layer and the response generation logic. In earlier steps we outlined an `EventBus` class for publish/subscribe messaging within the app. Now we ensure it’s fully implemented and ready to use.

**File: `app/core/event_bus.py`**

```python
import asyncio
from typing import Any, Dict, List

class EventBus:
    """Simple in-memory event bus for pub/sub communication within the app."""
    def __init__(self):
        # Dictionary mapping event type to list of subscriber queues
        self._subscribers: Dict[str, List[asyncio.Queue]] = {}
        # Optionally, a list for subscribers to all events can be added (not used in MVP)

    def subscribe(self, event_type: str) -> asyncio.Queue:
        """Subscribe to events of a given type. Returns an asyncio Queue to receive events."""
        queue: asyncio.Queue = asyncio.Queue()
        if event_type not in self._subscribers:
            self._subscribers[event_type] = []
        self._subscribers[event_type].append(queue)
        return queue

    def publish(self, event: Dict[str, Any]) -> None:
        """Publish a new event to all subscribers of that event's type."""
        event_type = event.get("type")
        if not event_type:
            # If event has no type, we cannot route it; ignore or log a warning
            print("EventBus: Published event without type; ignoring.")
            return
        # If no subscribers for this type, we can drop the event or log it
        queues = self._subscribers.get(event_type, [])
        if not queues:
            # No one is listening for this event type
            # (We won't raise an error; just no effect. Could log if needed.)
            return
        # Distribute event to all subscriber queues (don't block on slow consumers)
        for queue in queues:
            # Use put_nowait; if a queue is full, this will raise and we catch it
            try:
                queue.put_nowait(event)
            except asyncio.QueueFull:
                # If a subscriber's queue is full, skip it (to avoid hanging the bus)
                print(f"EventBus: Queue full for event type '{event_type}', dropping event for that subscriber.")
                continue

    def unsubscribe(self, queue: asyncio.Queue, event_type: str) -> None:
        """Unsubscribe a queue from a given event type."""
        if event_type in self._subscribers:
            try:
                self._subscribers[event_type].remove(queue)
            except ValueError:
                pass  # Queue not found in list (maybe already removed)
```

**Explanation:** The `EventBus` class provides a minimal pub/sub mechanism:

- It maintains an internal dictionary `_subscribers` where each key is an event type (a string) and the value is a list of asyncio Queues. Each subscriber gets its own `asyncio.Queue` to receive events.
- `subscribe(event_type)` creates a new `asyncio.Queue`, registers it under the given event type, and returns it. The subscriber (caller) can then `await queue.get()` on this queue to receive events as they are published. In our system, for example, the ResponseHandler will call `event_bus.subscribe("input")` to get a queue of incoming input events, and the SSE output endpoint might call `event_bus.subscribe("output")` to get a queue of outgoing messages for streaming.
- `publish(event)` takes a dictionary representing the event. We expect every event to have a `"type"` key to indicate its type (e.g., `"input"`, `"output"`). The method looks up all queues subscribed to that event type and puts a copy of the event into each queue. We use `queue.put_nowait` to avoid awaiting (which could pause the publisher if a consumer is slow). In case a queue is full (if we ever set a max size), we catch the `QueueFull` exception and drop the event for that subscriber, ensuring one slow consumer can’t block the others. For MVP, our queues default to infinite size, so this likely won’t happen.
- If an event has no type or has no subscribers, `publish` simply ignores it (or logs a message). This keeps things simple – publishing an event when nobody is listening is not an error in our design.
- We also provide an `unsubscribe(queue, event_type)` method to remove a subscription. This is used to clean up subscriber queues when they are no longer needed. For example, when an SSE client disconnects, the output streaming endpoint can unsubscribe its queue to avoid memory leaks. The method simply removes the queue from the list for that event type if present.

This EventBus has **no external dependencies** and keeps all operations in memory. It doesn’t guarantee ordering across different event types (each type has its own list of queues), but within a single queue, events will be received in the order they were published. We don’t implement complex features like wildcards or persistence – not needed for our use cases. This aligns with the **minimal implementation philosophy**: just enough functionality to decouple components. The EventBus allows the input endpoint to hand off a message without waiting for processing, and the ResponseHandler to process messages at its own pace, and then similarly hand off results to any output subscribers.

We will use the EventBus instance created in `main.py` (accessible via `app.state.event_bus`) throughout the app. For convenience, you may also import and use the same instance in modules (for example, some code might do `from app.main import app` and then `app.state.event_bus.publish(...)` or import the `event_bus` module and use a global instance if one is created there). Our approach, however, has been to keep references in `app.state` so it’s explicit and tied to the app’s lifecycle. This also helps in testing – you could replace `app.state.event_bus` with a dummy event bus if needed to simulate events.

### Background Response Handler (`app/core/response_handler.py`)

The Response Handler (sometimes called the orchestrator) is an asynchronous loop that waits for new input events and processes them to generate outputs. In mini-project #8, we outlined how this should work. Now we implement the final version and ensure it integrates with the rest of the system. This component will run as a background task (started in the startup event as we saw) and will utilize the EventBus and service clients to perform its job.

**File: `app/core/response_handler.py`**

```python
import asyncio
import logging
from typing import Any, Dict

# Import the event bus and service clients types (actual instances will be passed in)
from app.core.event_bus import EventBus
from app.backend.memory_client import MemoryClient
from app.backend.cognition_client import CognitionClient

# Optionally, import an LLM SDK (like OpenAI) if real LLM integration is possible
USE_LLM = False
try:
    import openai
    if "OPENAI_API_KEY" in openai.api_key or openai.api_key:
        USE_LLM = True
except ImportError:
    USE_LLM = False

async def handle_event(event: Dict[str, Any], memory_client: MemoryClient, cognition_client: CognitionClient, event_bus: EventBus):
    """Process a single input event: fetch context, get LLM response, and publish an output event."""
    # Basic validation/extraction
    event_type = event.get("type")
    if event_type != "input":
        # We only handle input events here (others could be handled by other handlers if needed)
        return
    user_id = event.get("user_id")
    conv_id = event.get("conversation_id")
    message_content = event.get("content")
    if not message_content:
        logging.warning("Received input event with no content; skipping.")
        return

    # Retrieve context from Memory and Cognition services (MVP: simple and always fetch)
    history = None
    context_summary = None
    try:
        history = await memory_client.get_history(user_id, conv_id)
    except Exception as e:
        logging.error(f"Error fetching history from Memory service: {e}")
    try:
        context_summary = await cognition_client.get_context(user_id, conv_id)
    except Exception as e:
        logging.error(f"Error fetching context from Cognition service: {e}")

    # Construct a prompt for the LLM (if using an LLM) - simple approach
    if history and isinstance(history, list):
        # Use the last few messages as context (for simplicity, join them)
        recent_history = " ".join([msg.get("content", "") for msg in history[-5:]])
    else:
        recent_history = ""
    if context_summary:
        # Context summary from cognition might be a string summarizing the conversation
        recent_history += f" Summary: {context_summary}"

    user_question = message_content
    full_prompt = f"Conversation context: {recent_history}\nUser asks: {user_question}\nAssistant answer:"

    # Get a response from the LLM (or a placeholder if no real LLM)
    answer_text = ""
    try:
        if USE_LLM:
            # Call external LLM API (e.g., OpenAI)
            openai.api_key = openai.api_key  # ensure API key is set via environment
            response = await openai.Completion.create(
                model="text-davinci-003",
                prompt=full_prompt,
                max_tokens=150,
                n=1,
                stop=None,
                temperature=0.7
            )
            answer_text = response['choices'][0]['text'].strip()
        else:
            # No real LLM available; use a simple echo or canned response
            answer_text = f"(Stubbed answer) You said: '{message_content}'"
    except Exception as e:
        logging.error(f"LLM call failed: {e}")
        # Fallback behavior: at least return a minimal response
        answer_text = "(Sorry, I cannot answer right now.)"

    # Publish the output event with the answer so that SSE subscribers (the client) get it
    output_event = {
        "type": "output",
        "user_id": user_id,
        "conversation_id": conv_id,
        "content": answer_text
    }
    event_bus.publish(output_event)
    logging.info(f"Published output event for conversation {conv_id} (user {user_id}).")

async def response_handler(event_bus: EventBus, memory_client: MemoryClient, cognition_client: CognitionClient):
    """Background task that continuously listens for input events and processes them."""
    # Subscribe to "input" events
    queue = event_bus.subscribe("input")
    logging.info("ResponseHandler: Subscribed to input events.")
    try:
        while True:
            # Wait for the next input event
            event = await queue.get()
            try:
                await handle_event(event, memory_client, cognition_client, event_bus)
            except asyncio.CancelledError:
                # If the task is cancelled (shutdown signal), break out of loop
                logging.info("ResponseHandler: received cancellation signal, exiting loop.")
                break
            except Exception as e:
                # Log any processing errors and continue to next event
                logging.error(f"Error processing event {event}: {e}", exc_info=True)
                continue
    finally:
        # Cleanup if needed (not much to clean in this simple case)
        event_bus.unsubscribe(queue, "input")
        logging.info("ResponseHandler: Unsubscribed from input events and stopped.")
```

**Explanation:** The `response_handler` module implements two main functions:

- `handle_event(event, memory_client, cognition_client, event_bus)`: This encapsulates the logic for handling a single input event (i.e., processing one user message). It performs the steps to generate a response:

  1. **Validate and parse the event:** It expects the event to be of type `"input"` and contain a `user_id`, a `conversation_id`, and the message `content`. (These would have been supplied by the `/input` API endpoint when it published the event to the bus.) If the content is missing, we log a warning and skip processing.
  2. **Retrieve context from Memory Service:** It calls `memory_client.get_history(user_id, conv_id)` to fetch recent conversation history. This is an asynchronous RPC to the Memory Service. We wrap it in a try/except to catch any errors (for instance, if the service is down) and simply proceed with `history=None` if it fails. The Memory service (as implemented in the backend for MVP) stores previous messages, so this could return a list of message dicts.
  3. **Retrieve context from Cognition Service:** Similarly, it calls `cognition_client.get_context(user_id, conv_id)`. In our MVP backend, this might return a summarized context or some analysis of the conversation (or just a placeholder string). Again, if it fails, we log an error and proceed without it.
  4. **Compose the LLM prompt:** We use a simple strategy (the “proactive context” approach). We take the last few messages from the history (say, 5 messages) and join their content as a brief context string. We append any summary from the cognition service if available. Then we form a prompt string that includes this context and the user’s latest question. This prompt is what we’ll send to the language model. (This step is simplistic and meant to illustrate context inclusion; it could be more sophisticated or skipped entirely for MVP).
  5. **Call the LLM or generate a response:** If an actual LLM integration is available, we use it to get an answer. In our code, we attempt to use the OpenAI API (`openai` library) if it’s installed and an API key is configured. The code sets up a completion request with a chosen model (e.g., text-davinci-003), and awaits the result. If this succeeds, we extract the generated text as the assistant’s answer. If no real LLM is available (common in isolated/offline development or testing), we fall back to a stubbed response. Here, we simply echo the user’s message in a format like `"(Stubbed answer) You said: '...'"` to clearly indicate it’s a placeholder. This ensures our system can function end-to-end even without external AI service access. We also catch any exceptions from the LLM call – for example, network issues or API errors – and provide a fallback answer string in that case (to avoid breaking the whole event processing).
  6. **Publish an output event:** We create a new event dict of type `"output"`, including the `user_id`, `conversation_id`, and the `content` of the answer. This event is published via `event_bus.publish(output_event)`. By doing so, any component subscribed to `"output"` events will receive it. In our architecture, the SSE endpoint (the client’s long-running connection) is subscribed to output events for that user’s conversation and will immediately send this data down to the frontend. We log that we published an output event for debugging/traceability.

  This `handle_event` function covers the core logic of generating a response. It deliberately avoids any loops or waiting beyond the single event – it does one complete cycle per event. This keeps it straightforward. There is also no shared state between events; all context is pulled fresh via the services, and the event carries any needed identifiers. This stateless approach makes the handler easier to reason about and test (you could call `handle_event` with a fabricated event and dummy clients to simulate a scenario).

- `response_handler(event_bus, memory_client, cognition_client)`: This is the long-running coroutine that continuously listens for new input events and dispatches them to `handle_event`. When we start this as a task on startup, it will run until the application shuts down. The logic is:

  - Subscribe to `"input"` events on the provided EventBus. We get an `asyncio.Queue` back.
  - Enter an infinite loop (`while True`) and `await queue.get()` to pick up the next event. This will pause until an event is available. Once an event is fetched, we call `await handle_event(event, ...)` to process it.
  - We have special handling for cancellation: if the loop is interrupted by a task cancellation (which happens on shutdown when we call `task.cancel()`), an `asyncio.CancelledError` will be raised at the await. We catch that specifically and break out of the loop. This allows the handler to stop gracefully when the application is shutting down.
  - We also catch any other exceptions during processing of an event so that one bad event (or a bug) doesn’t crash the entire loop. We log the error and continue to the next iteration, effectively skipping that event.
  - In a `finally` block after breaking out of the loop, we make sure to unsubscribe our queue from `"input"` events. This cleanup step removes the queue from the EventBus’s subscriber list, preventing memory leaks (otherwise, the EventBus might hold a reference to a queue that no one consumes after shutdown).

  Note that we log key events in this loop (subscribed, unsubscribed, cancellation, etc.) to help with debugging and understanding the system’s behavior when running.

The ResponseHandler exemplifies the **separation of concerns** principle: it uses the EventBus and client interfaces to do its job, but it doesn’t involve itself with HTTP or FastAPI directly at all. It’s pure background logic. The API layer (endpoints) and this core loop communicate only via events, which decouples them nicely. The code is also quite linear and clear – each step is done in sequence, which matches our _direct path_ philosophy (no complex state machines or multi-step callbacks, just straightforward calls).

**Testability:** Because we pass in the `event_bus` and clients to the `response_handler`, one can imagine swapping those out in tests. For example, a test could pass a fake EventBus or dummy clients that return preset data. Additionally, the `USE_LLM` flag and stubbed answer logic means the system can produce a deterministic response without requiring real AI calls, which is useful for automated tests. This design ensures that even though we have concurrency and external integration, we can test the response handling logic in isolation by invoking `handle_event` directly with sample events and checking that the output event is published as expected (perhaps by using a test EventBus that records published events).

Now that the event bus and response handler are integrated, whenever a new user message comes in through the API, the flow will be:

1. The `/input` endpoint (in `app/api/input.py`, implemented earlier) will verify the user’s token, then likely call `app.state.memory_client.store_message(...)` to save the message and then do `app.state.event_bus.publish({"type": "input", "user_id": ..., "conversation_id": ..., "content": ...})` to emit the event.
2. Our `response_handler` loop, already running in the background, will pick up this event and call `handle_event` to process it.
3. After processing, `handle_event` publishes an `"output"` event with the answer.
4. The `/output/stream` SSE endpoint (in `app/api/output.py`) is subscribed to output events. It will get the event from its queue and send it down to the client as a Server-Sent Event (likely formatted as JSON data).
5. The client receives the answer in real-time without the HTTP POST request having to wait for it. This achieves the asynchronous response flow.

All of this happens because we properly initialized the event bus and started the handler in our startup. If any part of this chain wasn’t set up (e.g., if we forgot to start the handler, or didn’t attach the event bus), the flow would break. This integration step ensures the chain is unbroken.

### Memory Service Client (`app/backend/memory_client.py`)

The MemoryClient is our interface to the external Memory Service. By now, the Memory Service (running separately via FastMCP) is assumed to be implemented (from a previous mini-project) and offering methods like `store_message` and `get_history`. We will use the FastMCP library to call those methods. The MemoryClient here is a thin wrapper around that library’s connection, focusing on simplicity and direct usage.

**File: `app/backend/memory_client.py`**

```python
import os
import logging
# Hypothetical FastMCP client library import
try:
    from fastmcp import Client as MCPClient
except ImportError:
    MCPClient = None  # In case the library is not available for offline development

# Configuration: Memory service endpoint (could be URL or host/port depending on FastMCP)
MEMORY_SERVICE_URL = os.getenv("MEMORY_SERVICE_URL", "localhost:9100")

class MemoryClient:
    """Client to interact with the Memory Service via FastMCP."""
    def __init__(self, service_address: str):
        self.service_address = service_address
        self._client = None  # Will hold the MCP client connection

    async def connect(self):
        """Establish connection to the Memory Service. To be called on startup."""
        if MCPClient is None:
            logging.warning("FastMCP library not installed; MemoryClient cannot connect.")
            return
        try:
            # Assume MCPClient.connect or similar coroutine to connect to service
            # For example, this could perform a handshake or open a persistent connection
            self._client = await MCPClient.connect(self.service_address)
            logging.info(f"MemoryClient: Connected to Memory service at {self.service_address}.")
        except Exception as e:
            logging.error(f"MemoryClient: Connection failed: {e}")
            raise

    async def store_message(self, user_id: str, conversation_id: str, content: str):
        """Store a user message in memory. (Calls Memory Service 'store_message')."""
        if self._client is None:
            raise RuntimeError("MemoryClient: Not connected to Memory service.")
        try:
            # Invoke the remote procedure via MCP
            result = await self._client.call("store_message", user_id, conversation_id, content)
            return result  # possibly an acknowledgment or stored message ID
        except Exception as e:
            logging.error(f"MemoryClient: Error calling store_message: {e}")
            raise

    async def get_history(self, user_id: str, conversation_id: str):
        """Retrieve conversation history from Memory Service."""
        if self._client is None:
            raise RuntimeError("MemoryClient: Not connected to Memory service.")
        try:
            history = await self._client.call("get_history", user_id, conversation_id)
            return history  # expected to be a list of message dicts
        except Exception as e:
            logging.error(f"MemoryClient: Error calling get_history: {e}")
            raise

    async def disconnect(self):
        """Close connection to Memory Service."""
        if self._client:
            try:
                await self._client.close()
                logging.info("MemoryClient: Disconnected from Memory service.")
            except Exception as e:
                logging.warning(f"MemoryClient: Error during disconnect: {e}")
```

**Explanation:** The `MemoryClient` class encapsulates communication with the Memory Service:

- We define `MEMORY_SERVICE_URL` (defaulting to `"localhost:9100"` or whatever host/port the memory service uses). This can be overridden via environment variable to point to the correct location. The Memory service is presumably running an MCP server on that address.
- In `__init__`, we take a service address (which could be a URL or host:port) and initialize `_client` to `None`. `_client` will be an instance of the FastMCP client connection once connected.
- `connect()` is an async method to establish the connection. We attempt to use `fastmcp.Client.connect` (or a similar call) to connect to the Memory service. We assume `MCPClient.connect` returns a connection object (perhaps of type `MCPClient` itself or a separate connection handle) and that it’s an async operation. If the library isn’t available or the connection fails, we log an error. If connection succeeds, we store the client in `self._client`.
  - We guard the use of FastMCP with a check in case the library is not installed (for example, if someone is running this code without the actual dependency, we don’t want to crash at import time).
- `store_message(user_id, conversation_id, content)` calls the remote `store_message` method on the Memory service via the MCP client. We require that `_client` is set (connected). We then `await self._client.call("store_message", ...)` and return whatever result the service provides. In the Memory service implementation, this likely stores the message in memory and returns maybe a confirmation or the message ID. We don’t necessarily use the result for anything in the core, but we return it just in case.
- `get_history(user_id, conversation_id)` similarly calls the Memory service’s `get_history` method and returns the history (likely a list of messages). This is used by the ResponseHandler to get context.
- Both methods have basic error handling: if the call raises an exception (e.g., due to network issues or the service returning an error), we log it and re-raise (so that the caller knows something went wrong). The ResponseHandler currently catches broad exceptions around these calls, so an error here would result in a logged error but not crash the loop.
- `disconnect()` closes the connection if open. We assume the MCP client has a `close()` method that can be awaited. We call it and log success or warn on error. This will be invoked during app shutdown to tidy up.

Crucially, the MemoryClient does **not** implement any caching or complex logic – it’s a straight pass-through to the service. This adheres to our _Occam’s Razor_ approach: the Memory service is responsible for storing and retrieving data; the core just asks it to do so. We avoid duplicating that logic in the core. The thin client also means if the Memory service changes its interface or if we wanted to mock it, there’s only a small surface area to adjust.

This class is instantiated in `app/main.py` and stored in `app.state.memory_client` on startup. All parts of the system that need to store or load memories (e.g., the `/input` route might call `store_message` to save the incoming message immediately, and the ResponseHandler calls `get_history`) will access it via that instance.

### Cognition Service Client (`app/backend/cognition_client.py`)

Likewise, the CognitionClient connects to the Cognition Service to fetch any additional context or analysis (like a summary). It’s implemented similarly to MemoryClient, calling the remote `get_context` (and possibly other functions if needed).

**File: `app/backend/cognition_client.py`**

```python
import os
import logging
# Using the same FastMCP client library
try:
    from fastmcp import Client as MCPClient
except ImportError:
    MCPClient = None

COGNITION_SERVICE_URL = os.getenv("COGNITION_SERVICE_URL", "localhost:9101")

class CognitionClient:
    """Client to interact with the Cognition Service via FastMCP."""
    def __init__(self, service_address: str):
        self.service_address = service_address
        self._client = None

    async def connect(self):
        """Connect to the Cognition Service."""
        if MCPClient is None:
            logging.warning("FastMCP library not installed; CognitionClient cannot connect.")
            return
        try:
            self._client = await MCPClient.connect(self.service_address)
            logging.info(f"CognitionClient: Connected to Cognition service at {self.service_address}.")
        except Exception as e:
            logging.error(f"CognitionClient: Connection failed: {e}")
            raise

    async def get_context(self, user_id: str, conversation_id: str):
        """Retrieve context or summary from Cognition Service."""
        if self._client is None:
            raise RuntimeError("CognitionClient: Not connected to Cognition service.")
        try:
            context = await self._client.call("get_context", user_id, conversation_id)
            return context  # likely a string or data structure
        except Exception as e:
            logging.error(f"CognitionClient: Error calling get_context: {e}")
            raise

    # If the Cognition service had other methods (like a tool invocation), they would be added similarly.

    async def disconnect(self):
        """Disconnect from Cognition Service."""
        if self._client:
            try:
                await self._client.close()
                logging.info("CognitionClient: Disconnected from Cognition service.")
            except Exception as e:
                logging.warning(f"CognitionClient: Error during disconnect: {e}")
```

**Explanation:** The `CognitionClient` is almost identical in structure to `MemoryClient`, but tailored to the Cognition service:

- It uses a separate configuration `COGNITION_SERVICE_URL` (default `"localhost:9101"` assuming the Cognition service listens on port 9101).
- The main method provided is `get_context(user_id, conversation_id)`, which calls the Cognition service’s `get_context` RPC via FastMCP. According to our earlier design, this might return a summary of the conversation or some relevant context info. In the MVP Cognition service implementation, it could even just return a placeholder or fetch recent messages from Memory (depending on how it was built). But from the core’s perspective, we treat it as a black box that given a user and conversation, returns some context string or data.
- We include a `connect()` and `disconnect()` just like in MemoryClient, to manage the connection.
- If in the future the Cognition service offers more functionality (for example, maybe an endpoint to analyze text or to request an external tool), we would add corresponding methods to this client. For the MVP, `get_context` is enough.

By having separate MemoryClient and CognitionClient classes, we keep the responsibilities separate and clear, even though they are similar. This is a straightforward design without generalizing into a single “MCPClientManager” or such, which would add unnecessary abstraction. Each client deals with one service and one set of RPC calls, making the code more readable and specific.

These clients are used inside the ResponseHandler (as seen in `handle_event`) to get needed data. The `/input` endpoint also uses MemoryClient to store the incoming message immediately (to ensure it’s saved even if the response is still being processed). The design ensures **consistency**: all calls to Memory or Cognition go through these clients, which in turn all go through the FastMCP library’s `call` interface.

Because we attach instances of these clients to `app.state`, if needed in other parts of the code (say, in a future feature), we can easily retrieve them or even swap them out (for example, in a test scenario, one could replace `app.state.memory_client` with a dummy object that has the same `store_message` and `get_history` methods).

### Auth0 JWT Authentication Utility (`app/utils/auth.py`)

To protect our endpoints, we integrate Auth0 for JWT verification. By this stage, we need a real implementation that checks the token’s signature and decodes the user information, rather than a stub. We’ll use a standard JWT verification approach with the Auth0 public keys (JWKS). This utility will provide a dependency function `get_current_user` that FastAPI can use to enforce auth on routes.

**File: `app/utils/auth.py`**

```python
import os
import logging
from fastapi import Depends, HTTPException
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

# Use python-jose for JWT verification (or authlib, similar usage)
from jose import jwt, jwk
import requests  # to fetch JWKS from Auth0

# Configuration for Auth0
AUTH0_DOMAIN = os.getenv("AUTH0_DOMAIN", "")
AUTH0_AUDIENCE = os.getenv("AUTH0_AUDIENCE", "")

# Prepare JWKS (JSON Web Key Set) from Auth0, if domain is provided
JWKS = None
if AUTH0_DOMAIN:
    jwks_url = f"https://{AUTH0_DOMAIN}/.well-known/jwks.json"
    try:
        resp = requests.get(jwks_url)
        resp.raise_for_status()
        JWKS = resp.json()
        logging.info(f"Fetched JWKS from Auth0 domain: {AUTH0_DOMAIN}")
    except Exception as e:
        logging.error(f"Failed to fetch JWKS: {e}")
        JWKS = None

# FastAPI security scheme for Bearer token
token_scheme = HTTPBearer(auto_error=False)

async def get_current_user(credentials: HTTPAuthorizationCredentials = Depends(token_scheme)):
    """
    FastAPI dependency to verify JWT and return the token payload (user info).
    If validation fails, raises HTTP 401.
    """
    if not credentials or not credentials.credentials:
        # No token provided
        raise HTTPException(status_code=401, detail="Not authenticated")
    token = credentials.credentials

    # If Auth0 config is not provided, allow a dev mode token verification
    if not AUTH0_DOMAIN or not JWKS:
        # Development mode: accept any token (or verify with a static secret if set)
        dev_secret = os.getenv("DEV_JWT_SECRET")
        if dev_secret:
            try:
                # If a development secret is provided, verify using HS256 with that secret
                payload = jwt.decode(token, dev_secret, algorithms=["HS256"])
            except Exception as e:
                raise HTTPException(status_code=401, detail="Invalid token")
        else:
            # No secret - just parse without verification (NOT secure, only for dev/test)
            try:
                payload = jwt.get_unverified_claims(token)
            except Exception as e:
                raise HTTPException(status_code=401, detail="Invalid token format")
        # In dev mode, we don't have real user info; fabricate minimal identity if needed
        if isinstance(payload, dict):
            payload.setdefault("sub", "dev|anonymous")
        return payload

    # Auth0 mode: Verify RS256 token using Auth0's public keys
    try:
        # Decode header to get the key ID (kid)
        header = jwt.get_unverified_header(token)
    except Exception as e:
        raise HTTPException(status_code=401, detail="Invalid token header")
    kid = header.get("kid")
    if not kid or not JWKS:
        raise HTTPException(status_code=401, detail="Invalid token (no key available)")

    # Find the matching key in JWKS
    key_data = None
    for key in JWKS.get("keys", []):
        if key.get("kid") == kid:
            key_data = key
            break
    if key_data is None:
        raise HTTPException(status_code=401, detail="Invalid token (unrecognized key)")

    # Construct public key from JWKS data
    try:
        public_key = jwk.construct(key_data)
    except Exception as e:
        logging.error(f"Failed to construct public key: {e}")
        raise HTTPException(status_code=401, detail="Invalid token key")

    # Now verify the token signature and claims
    try:
        payload = jwt.decode(
            token,
            public_key,
            algorithms=[key_data.get("alg", "RS256")],
            audience=AUTH0_AUDIENCE if AUTH0_AUDIENCE else None,
            issuer=f"https://{AUTH0_DOMAIN}/" if AUTH0_DOMAIN else None
        )
    except Exception as e:
        raise HTTPException(status_code=401, detail="Token validation failed")
    # Token is valid at this point; return the claims (could map to a User model if needed)
    return payload
```

**Explanation:** Our auth utility sets up JWT verification in a straightforward manner:

- We configure the Auth0 domain and audience from environment variables (`AUTH0_DOMAIN`, `AUTH0_AUDIENCE`). These should be provided if we are integrating with a real Auth0 tenant. The domain would be something like `your-tenant.auth0.com` and the audience is the expected identifier for the JWT (typically the API identifier).
- If an Auth0 domain is configured, we attempt to fetch the JWKS (the public keys) from Auth0’s well-known URL. We use `requests.get` to retrieve the JSON. This is done at import time for simplicity. If this fails (network issue or no domain configured), `JWKS` stays `None` and we’ll handle it accordingly.
- We set up a FastAPI `HTTPBearer` security scheme called `token_scheme`. This will extract the `Authorization: Bearer <token>` header and provide a `HTTPAuthorizationCredentials` object. We set `auto_error=False` because we want to handle the error messaging ourselves in the dependency function.
- The core function `get_current_user` is a dependency that will be used in FastAPI routes. It works as follows:
  1. It first checks if credentials were provided. If not (no token), it raises a 401 Unauthorized.
  2. If we are not configured for Auth0 (i.e., no `AUTH0_DOMAIN` or no JWKS keys loaded), we assume we’re in **development mode**. In dev mode, we either:
     - Accept any token if no specific verification is set, or
     - If an environment variable `DEV_JWT_SECRET` is set, we try to decode the token using that secret (assuming the token might be signed with HS256 using a known secret for testing).
     - If decoding with secret fails or no secret is provided, we fall back to just parsing the token’s claims without verifying the signature (`jwt.get_unverified_claims`). This is obviously insecure and only meant for local testing where you might not care about true auth, but want to simulate a user context.
     - In dev mode, if we got a payload (claims), we ensure there is at least a `"sub"` (subject) field. We add a default `"dev|anonymous"` sub if none is present, to represent an anonymous test user.
     - Then we return the payload (which could include `sub`, `name`, etc. if the token had those claims).
  3. If Auth0 is configured properly (we have JWKS), we go through the real verification:
     - We use `jwt.get_unverified_header(token)` from python-jose to read the token header and extract the `kid` (Key ID). This `kid` tells us which public key was used to sign the token.
     - We look up the `kid` in the JWKS keys we fetched. If not found, we reject the token.
     - Using the key data, we construct a public key object (`jwk.construct(key_data)`). Python-jose’s `jwk.construct` can create a RSA public key from the JWKS info (n, e, etc.).
     - Then we use `jwt.decode` to verify the token’s signature and decode the claims. We pass in the public key, the allowed algorithm (from the key data, typically RS256), and if available, we specify the expected audience and issuer. The issuer is `https://<AUTH0_DOMAIN>/` and the audience is our API’s audience. If these don’t match the token’s claims, decoding will fail.
     - If any error occurs in this process (bad signature, expired token, wrong audience, etc.), we raise 401.
     - If decode succeeds, we get the `payload` which is a dict of the token’s claims. This will typically include the user’s `sub` (Auth0 user ID), and possibly other information like `name` or `email` depending on the token scope.
     - We return this payload as the "current user". In a more advanced setup, you might map this to a Pydantic user model or a simpler object, but for our purposes the raw claims dict is sufficient to identify the user.
- We do minimal logging for key events (fetching JWKS, errors, etc.), and primarily use HTTP exceptions to signal auth failures to FastAPI, which will turn them into `401 Unauthorized` responses automatically.

With `get_current_user` in place, we have integrated authentication. As seen in `app/main.py`, we added `dependencies=[Depends(get_current_user)]` to certain routers, meaning any request to those endpoints will go through this function first. If the token is missing or invalid, the request is rejected before hitting the endpoint logic. If the token is valid, the endpoint can optionally get the returned `payload` by declaring a parameter, e.g., `def send_message(user=Depends(get_current_user)):` – but even if it doesn’t, the dependency ensures the user is authenticated.

This approach keeps auth logic separate from business logic (we’re not manually decoding tokens inside endpoints; it’s handled centrally here). It also trusts Auth0 to manage user identities – we’re not checking roles or scopes in this MVP, just that the token is valid. This is in line with our philosophy of _trusting external systems_ for what they do best (Auth0 for auth, we just verify and extract basic info).

**Note on dependencies:** Ensure that `python-jose` (the JOSE library) and `requests` are added to your project’s dependencies, as they are used here. These would have been introduced in the Auth0 integration step (#3), so by now they should be present. The code above uses them directly without additional abstraction.

## Conclusion and Next Steps

By implementing the above components and integrating them in `app/main.py`, we achieve a clean startup and shutdown sequence for Cortex Core:

- When the application starts, it now automatically connects to required services and launches the background processing loop. All routers are registered, and protected endpoints are secured via JWT verification.
- The EventBus is globally accessible for any part of the app that needs to publish or subscribe to events, yet it remains a contained object that we control (attached to the app and easy to swap in tests if needed).
- The background ResponseHandler is running and ready by the time the first user message comes in, ensuring no messages are missed. It will continuously run, handling one event at a time in a simple loop.
- External service interactions (Memory and Cognition) are fully wired up. The core can store and retrieve data from the Memory service and query the Cognition service for context. These happen through the FastMCP clients with minimal overhead in our code.
- Authentication is enforced uniformly, and only valid users can access the chat endpoints. The integration with Auth0 is done in the simplest correct way, without pulling in unnecessary custom logic.
- On shutdown, we cleanly stop asynchronous tasks and close connections, which keeps the system stable over multiple start/stop cycles (important for development reloads and eventual production deployments).

This integrated application is now **functionally complete for the MVP**. All the pieces built in earlier steps operate together:

- A client can obtain a token (e.g., via Auth0) and connect to the system.
- They can send a message to the `/input` API; the message gets saved and an event is published.
- The response loop picks up the event, calls out to Memory/Cognition as needed, and uses the LLM (or stub) to generate a response.
- The answer is published as an event and streamed to the client through the `/output/stream` SSE connection.
- Meanwhile, the user can also use config endpoints to create or list conversations/workspaces (which we implemented in a previous step as simple in-memory operations).
- The health endpoint can be used to check the server status, and the verify endpoint (if implemented in `/auth`) can confirm the token’s validity.

In the final step of the Cortex Core MVP (Testing and Validation), you would rigorously test this end-to-end flow. But at this point, the **application startup and integration** is done. The code we’ve added here should be kept as straightforward as possible, as it is now, so that anyone reading the `main.py` or related files can immediately understand the initialization sequence. We avoided adding extraneous layers or “magic” – the startup reads in a linear fashion and uses plain FastAPI constructs.

You can run the app (for example with `uvicorn app.main:app --reload`) and try a simple manual test: simulate a user message and see if you get a response in the SSE. Ensure the Memory and Cognition services are running (either started by the app with `START_EMBEDDED_SERVICES=true` or launched separately via `python app/backend/memory_service.py` and `python app/backend/cognition_service.py`). With everything in place, Cortex Core MVP will be operational. All further improvements (like more sophisticated LLM prompting, better error handling, etc.) can build on this solid integrated foundation without needing to change the fundamental wiring we established here.
