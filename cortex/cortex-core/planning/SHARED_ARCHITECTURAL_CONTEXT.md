# Cortex Core MVP – Shared Architectural Context

## System Overview

**Cortex Core** is the central orchestrator of the Cortex AI assistant platform. It sits between simple client applications (which send and receive messages) and specialized backend services (which handle memory storage, extra computations, and language model generation). The Core’s design is deliberately minimal and focused: each part does one job, and nothing more. This minimal **MVP** architecture includes only the essential pieces needed for a user to send a message and get a useful AI response. All components communicate through clear, well-defined interfaces. The overall philosophy is **clarity and simplicity** – no clever tricks or premature abstractions. This ensures the system is easy to understand, extend, and maintain.

## Architecture Components and Responsibilities

- **API Layer (FastAPI Web Service)** – Provides the entry point for clients. It exposes HTTP endpoints for input and uses Server-Sent Events (SSE) for output streaming. The API layer’s tasks are simple: authenticate incoming requests, parse the message, and hand it off for processing. It remains **stateless**, meaning it does not store conversation data itself. Each request is independent, with user identity provided by a JWT (JSON Web Token) from Auth0. After forwarding a message inward, the API immediately returns a basic acknowledgment to the client (the actual answer will stream back asynchronously via SSE). The API does not contain any AI logic; it only validates and passes data along.

- **Core Orchestrator (Core Logic)** – The brain of Cortex Core that coordinates the workflow for each message. It is an asynchronous loop or handler that receives incoming messages (via the Event Bus) and orchestrates calls to other components (Memory, Cognition, LLM). The orchestrator’s responsibility is to take a user’s input and produce an answer. It does this by delegating tasks to the appropriate services: storing/retrieving conversation context from memory, requesting any additional context or computations, invoking the Large Language Model, and deciding how to form the final reply. The orchestrator itself contains only minimal logic – just enough to sequence these operations. It does not hard-code knowledge of how memory or cognition work internally; it simply uses their defined interfaces. This keeps the core logic clean and focused on **workflow** rather than on heavy AI processing. If the LLM’s response indicates a need to use a tool (for example, asking for more info), the orchestrator detects that and will call the relevant service (via the MCP clients) to get the needed data, then continue the flow. In summary, the Core Orchestrator handles the **conversation state machine**: it knows what step comes next but relies on other components to do the heavy lifting.

- **Event Bus** – An in-memory publish/subscribe hub inside Cortex Core that decouples components. It is the glue between the API, orchestrator, and output streaming. When the API receives a new message, it publishes an **input event** on the Event Bus. The orchestrator, which is subscribed to these events, will pick it up and begin processing. Similarly, when the orchestrator has a result to send out, it publishes an **output event** to the bus. Any SSE output connections (also subscribers) will receive that and stream it to clients. The Event Bus is extremely simple: it broadcasts every event to all subscribers, and each subscriber filters out what it needs (for example, only processing events for a specific user or of a certain type). There is no complex routing or persistence. Events exist only in memory and only while there are active listeners. This simplicity means there’s no message queue server or database – just a lightweight Python object managing lists of subscribers. The Event Bus ensures **loose coupling**: the API doesn’t need to know who handles the message next, and the orchestrator doesn’t need to know who the client connections are. Each just publishes or subscribes and trusts the bus to fan out the messages.

- **Memory Service (MCP Client & Server)** – An external component responsible for storing conversation history and other memory for each user. This is implemented as a separate service (running its own process or server) that the Core communicates with via the **Model Context Protocol (MCP)**. In practice, within Cortex Core we have a **Memory Client** (using a library called FastMCP) that exposes simple methods like “store_message” or “get_history”. When the orchestrator calls these methods, the MCP client sends a request to the Memory Service (over an SSE-based channel) and waits for the response. The Memory Service then handles actual data storage (for MVP it could be an in-memory store or simple database) and returns the requested data. The key point is that **all conversation state lives in the Memory Service** – Cortex Core itself does not permanently keep any user messages. By isolating memory this way, multiple Core instances or processes can share a single source of truth, and we ensure one user’s history doesn’t leak to another’s. In the MVP, the Memory Service might simply record messages and fetch recent history. Every call includes a user identifier (from the JWT) so that the service can partition data per user. The Memory Service has no knowledge of the overall conversation flow; it only knows how to store and fetch data when asked.

- **Cognition Service (MCP Client & Server)** – Another external service accessed via MCP, meant for any on-demand computations or contextual lookups needed during conversation. The Cognition Service is conceptually where you could implement things like searching a knowledge base, performing calculations, or fetching external info in response to a query. In the MVP, this service is minimal or even a stub. It’s included to demonstrate how the architecture can incorporate additional tools. The Core uses a **Cognition Client** (via FastMCP) to call functions on this service. For example, if the conversation requires some analysis or context beyond raw memory, the orchestrator can invoke a cognition tool like “get_context” or “analyze_input”. As with memory, each call carries the user identity and any parameters. The Cognition Service, running separately, processes the request and returns a result. In MVP, this might just echo some data or provide a placeholder, since heavy cognition might not be needed for a simple Q&A flow. The important idea is **extensibility**: by having this layer, Cortex Core can grow to include more advanced logic without changing the core – just by implementing new MCP-accessible services. (For now, those services don’t talk to anything else – they are self-contained.)

- **LLM Integration (Language Model Adapter)** – The component within the Core that interfaces with a Large Language Model. This could be an API call to an external AI service (like OpenAI GPT-4, etc.) or a call to a locally hosted model. The LLM adapter takes the assembled conversation context (system instructions, recent messages, and the user’s query) and sends it to the model to generate a reply. For the MVP, this integration is kept simple: it might directly call an SDK or HTTP endpoint with the prompt and get a completion. We enable **streaming** here if possible: meaning the model’s partial output can be forwarded piece by piece. The adapter might use a library (like `pydantic-ai`) to enforce that the model’s output follows a certain structure (for example, to detect if the model is asking to use a tool by returning a specific JSON format). But overall, it’s a thin wrapper around the model API. It handles errors or timeouts gracefully (if the model fails to respond, the adapter will catch that and signal an error event). The LLM integration is what produces the actual assistant response content or a request for a tool. It is treated as a **black-box service** from the Core’s perspective: Core just provides input and gets output. By isolating the LLM call here, we could swap models or providers without affecting other parts of the system.

- **Output Streaming (SSE Handler)** – The mechanism for delivering responses back to the client in real time. Cortex Core uses **Server-Sent Events (SSE)** as the one-way channel from server to client. When a client connects to the SSE endpoint (a GET request that stays open), the API layer registers a subscriber on the Event Bus for that connection. As the orchestrator publishes output events (containing the assistant’s response, either in full or in chunks), those events go through the bus to all subscribers. The SSE handler filters events by user (and possibly conversation/session) and formats the ones meant for this client into SSE data packets (`data: ...\n\n`). Each such packet is pushed over the open HTTP response stream to the client. This allows the user to receive the answer gradually (streaming) or as a single chunk once ready, without the client having to poll or re-request. The SSE connection remains open throughout the session, delivering any number of events. If the connection drops, the server notices and cleans up that subscriber. **No other traffic goes over SSE** – it’s a dedicated output channel. By using SSE (which is unidirectional), we keep things simpler than a full bidirectional WebSocket. The client just displays messages as they arrive. The SSE handler ensures that each user only ever receives events tagged for their own ID, maintaining isolation.

- **Authentication & Identity (Auth0 Integration)** – While not a code-heavy component inside Cortex Core, authentication is a critical cross-cutting piece of the architecture. Cortex Core itself does not handle user sign-up or passwords; it relies on an external identity provider (Auth0) to manage users. Clients must obtain a JWT from Auth0 (for example, via logging in through Auth0’s hosted UI or a social login) and include this token with each request to the Cortex Core API. The API layer verifies the token on every call (using Auth0’s public keys) to ensure it’s valid and not expired. From that token, Core extracts the user’s unique ID (`sub` claim) and uses it to **scope all operations**. This means when the Core calls the Memory service or Cognition service, it always passes along the user ID (and any relevant conversation ID) so that those services can use or store data in that user’s partition. Authentication thus provides security (only authorized users can use the API) and identity context (so multiple users’ data never mixes). Importantly, because the token is required each time and no session state is kept in Core, the system remains stateless and each request is independently authenticated. Auth0 also simplifies permission checks — for MVP, we assume any authenticated user can access the basic features, and we don’t have complex roles or ACLs yet.

_(In addition to the above, the design anticipates **Domain Expert Services** in the future – specialized tools or knowledge-base services accessible via the same MCP mechanism as Memory and Cognition. These are not part of the MVP implementation, but the architecture is structured to allow plugging them in without major changes. This illustrates the extensibility of Cortex Core’s approach.)_

## End-to-End Flow of a Message

To see how these components work together, consider a simple scenario of a user sending a message and receiving an answer:

1. **Client Sends Input:** A client (e.g. a web front-end) sends a POST request to the Cortex Core API (for example, to an endpoint `/input`). This request includes the user’s message content and the user’s Auth0 JWT in the Authorization header. The client also likely has already opened a connection to the SSE `/output/stream` endpoint to listen for responses (or it will do so immediately after posting the message).

2. **API Receives and Authenticates:** The FastAPI API layer receives the POST request. It first verifies the JWT – if the token is invalid or missing, the request is rejected. Assuming it’s valid, the API now knows the caller’s user identity (and possibly a conversation ID if included in the request). The message JSON is parsed into a Pydantic model for consistency. For reliability, the API immediately stores the user’s message in the Memory Service (via the Memory client) – this ensures the message is recorded in the conversation history before anything else. Next, the API publishes an **input event** on the Event Bus. This event is a dictionary containing at least the `user_id`, the `conversation_id` (or similar context tag), and the message content. After publishing the event, the API sends a quick HTTP response back to the client, confirming that the message was received and that a response will follow. The client can now wait for SSE updates.

3. **Orchestrator Picks up the Event:** The Core Orchestrator, which has been running in the background and subscribed to the Event Bus, receives the new input event. (It likely runs as an async task started when the app launched, constantly waiting for new events in its queue.) Upon getting the event, the orchestrator logs or notes the incoming message and begins the processing sequence for that user’s query. It already has the user ID from the event, which it will use for all subsequent calls in this flow to keep everything tied to the correct user context.

4. **Retrieve Context (Memory and Cognition):** The first thing the orchestrator might do is gather any relevant context needed to answer the question. It calls the **Memory Service** (via the Memory client library) to fetch recent conversation history for this user. For example, it might request the last N messages in the conversation (including the one that was just stored). The Memory client sends this request over MCP (SSE) to the Memory Service, which returns the data (e.g. a list of past messages with timestamps and roles). Now the orchestrator has the conversation history. Next, the orchestrator may call the **Cognition Service** for any supplemental information. For instance, if the system supports some form of knowledge lookup or analysis, it might invoke a cognition tool like `get_context` or `analyze_input` with the user’s query. In the MVP, this step is optional or might be a no-op (the Cognition service might just return an empty result or some acknowledgement if it’s not truly implemented). Nonetheless, the call is made in the same way via the Cognition client and MCP, illustrating the pattern. The key outcome of this step is that the Core collects all data it needs to construct a prompt for the language model: the conversation memory and any extra context.

5. **Prepare LLM Prompt:** Using the information gathered, Cortex Core formulates the prompt for the Large Language Model. Typically, this will be a sequence of messages that include the conversation history and the new question. For example, it might create a list of messages like:

   - A **system** message that defines the AI’s role or behavior (for instance, “You are Cortex, an AI assistant. Answer concisely and helpfully.”). This primes the model with any global instructions or persona. (This can be configured or kept minimal in MVP.)
   - The recent **conversation history** as alternating user and assistant messages, in order. This gives the model context of what has been discussed so far. (If the history is long, the orchestrator will truncate it to the most recent relevant parts to fit the model’s context window.)
   - The latest **user message** (the one we are responding to), marked clearly as from the user.
     This assembled prompt is now ready to send to the LLM. It captures who the assistant is, what has been said, and what the user is asking now. There’s no complex prompt engineering beyond this – the MVP sticks to a straightforward approach, avoiding any unnecessary complexity in how we talk to the model.

6. **Call the Language Model:** The orchestrator invokes the **LLM adapter** to get an answer. Depending on the implementation, this might be an async call to an external API. For example, using OpenAI’s API, it would pass the message list and ask for a completion, with streaming enabled. As the LLM starts generating text, the Core can handle it in a streaming fashion: receiving partial chunks of the answer as they come. If the model supports it, the adapter will yield tokens or sentences which can be forwarded as they arrive (this provides realtime feedback to the user). Throughout this call, the orchestrator is essentially waiting for the model to either produce a final answer or a special indication that it needs a tool.

7. **(Optional) Tool Use Loop:** If the LLM’s output indicates a request to use a tool instead of giving a final answer, the orchestrator will step in to facilitate that. In practice, we instruct the LLM (via the system prompt or output schema) that if it cannot answer directly and needs more info, it should respond with a structured format (e.g., JSON) specifying which “tool” to use and with what parameters. For example, the model might reply with `{"tool": "calculator", "args": {"expression": "2+2"}}` if it needs to calculate something. The orchestrator detects this (either by parsing the JSON or via a schema validation using pydantic) and pauses the direct answering process. It then calls the appropriate service or function for that tool – possibly another MCP client call or an internal function – to get the result the model asked for. Once the tool’s result is obtained (say the calculator returns `4`), the orchestrator feeds that information back into the LLM (often by appending it to the conversation context as a system or assistant message) and asks the model to continue. This loop can repeat: the model can request multiple tools in sequence, but each time the orchestrator will execute the request and give the model the result, until the model finally provides a normal answer. **In the MVP, tool usage is simplistic** – the only “tools” really are the Memory and Cognition services which we already call. We might not implement any other external tool in the first version, but the architecture is set up to handle this pattern gracefully when needed.

8. **Produce Final Answer:** Eventually, the LLM produces a final answer to the user’s question (a message from the assistant). The orchestrator takes this answer text and performs any final steps before returning it. One important step is to update the **Memory Service** with the assistant’s reply: the orchestrator will send the answer to the Memory Service so that it gets recorded as the latest assistant message in the conversation history. This ensures continuity for future questions. After storing the answer (which is quick, as it’s just an MCP call), the orchestrator creates an **output event** containing the answer content (and relevant metadata like user and conversation ID).

9. **Deliver via SSE:** The orchestrator publishes the output event to the Event Bus. The SSE output handler(s) are subscribed and waiting. The event contains the user ID, so each SSE connection will check if it matches their user. The connection for our user sees the match and proceeds to send the data over the SSE channel. On the client’s side (for example, the web app), an event is received containing the assistant’s answer. If the answer was streamed in parts, multiple SSE events would have been sent (each with a chunk of the answer as it was generated). If it was a single complete answer, it might be one SSE message. The client combines these and displays the final answer to the user. The cycle is now complete: the user asked a question and got a response, with all the behind-the-scenes coordination handled by Cortex Core.

Throughout this flow, **errors or exceptions** (if any) are handled in a simple way. For instance, if the Memory Service is down or the LLM call fails, the orchestrator can catch that and publish an error event (which the client could display as an error message). The design favors always sending _something_ back to the user, even if it’s an error notice, rather than failing silently. However, detailed error recovery (retries, fallbacks) is minimal in MVP, keeping with the simplicity focus.

## Interface Boundaries and Communication

Cortex Core’s components interact only through defined interfaces, ensuring clear boundaries between different parts of the system. Here are the key interface points and how data flows across them:

- **Client ↔ API:** This is a straightforward HTTP interface. Clients send JSON requests (HTTP POST) to the API endpoints and receive immediate JSON acks or token responses. For receiving answers, clients hold a Server-Sent Events (HTTP SSE) connection open to the API. Data over SSE is one-way (server to client) and formatted as text event streams (JSON payload encoded as SSE `data:` lines). The API expects an Auth0 JWT on every call (usually in the `Authorization: Bearer <token>` header) to authenticate the user. This boundary ensures that only properly authenticated HTTP requests enter the system, and all outgoing data to clients is delivered over a standardized SSE stream. There is no direct client access to memory or cognition services – everything funnels through the API endpoint for input and the SSE endpoint for output.

- **API ↔ Core Orchestrator:** Inside the Cortex Core service, the API layer and the core logic communicate via the in-memory Event Bus and simple function calls. The API does not call the orchestrator’s functions directly in a synchronous way waiting for a result (that would couple the request/response tightly). Instead, the API publishes an event to hand off the work and then returns immediately to the client. The orchestrator, running asynchronously, will pick up the event. In some cases (like storing the message to memory), the API might perform a quick call via the Memory client before publishing the event, but that is an isolated operation for durability, not the main processing. The important boundary here is that **the API never needs to know how the orchestrator handles the message**, and the orchestrator doesn’t interact with HTTP details – they are connected only by the event payload. This separation makes the system more modular and resilient (the API thread isn’t tied up doing long LLM processing, for example).

- **Core Orchestrator ↔ Memory/Cognition Services (MCP):** Cortex Core uses the **Model Context Protocol (MCP)** to interact with backend services like Memory and Cognition. The interface on the Core side is a set of client objects (e.g., `memory_client`, `cognition_client`) that provide methods for various actions (often called “tools”). When the orchestrator calls these methods, the MCP client under the hood sends a request to the corresponding service. The communication uses an **SSE-based protocol** (the MCP library manages an SSE connection to each service, acting somewhat like a persistent pipe for requests and responses). For example, a call `memory_client.store_message(data)` will serialize that request, send it over SSE to the Memory Service, and wait for an acknowledgment. Likewise, `memory_client.get_history(user_id, convo_id, limit=10)` will send a request and wait for the service to reply with the data. The interface is **asynchronous and message-based**, even though the orchestrator writes it like a normal function call thanks to the MCP library. Each service defines what “tools” or actions it exposes; the core doesn’t query their database directly or invoke internal functions – it strictly goes through these API calls. The boundary is enforced by the protocol: Memory and Cognition services only accept properly formatted MCP requests and they only respond with results or errors over the same channel. They do not call back into Core on their own; Core always initiates. This design means Memory and Cognition could even be running on separate servers or be replaced with different implementations as long as the MCP interface is consistent. It also sandboxes their functionality – if, say, the Memory Service has a bug, it won’t crash the Core process, it would just fail to respond properly to an MCP call (which Core can handle). All data exchanged includes the user’s ID (and possibly conversation ID) so that these services operate in isolation per user.

- **Core Orchestrator ↔ LLM Provider:** The interface between Cortex Core and the LLM is typically an external API call. For example, if using OpenAI, it might be an HTTPS request to OpenAI’s API endpoint with an authentication key and the prompt data. In our architecture, we wrap this in the LLM adapter component to keep the rest of the code unaware of the exact API details. The boundary here is the contract that given a list of messages (prompt), the LLM will return either a completion (answer) or a tool request. We may use a **structured output schema** to facilitate interpreting the response (for instance, expecting a JSON object from the model if it wants to use a tool). The LLM call can be streaming, meaning the Core will receive incremental parts of the answer. This streaming is handled internally by the adapter and then forwarded as events. The key is that the LLM provider is an external service (could even be a local model server); the Core treats it as a black box. We ensure we don’t embed provider-specific logic beyond the call itself, so we could switch to a different model by adjusting configuration. Error handling is also at this boundary: if the LLM times out or returns an error, the adapter catches it and signals the orchestrator to handle it (perhaps by sending an error message event). **No other component directly interacts with the LLM** – only the orchestrator via this adapter does, keeping the model interaction logic in one place.

- **Event Bus ↔ SSE Output:** Within the Core, the Event Bus and SSE output mechanism work together to deliver results. The boundary here is more conceptual – the SSE handler is essentially a consumer of the Event Bus. Each connected client’s SSE stream corresponds to a subscriber queue on the bus. The SSE handler doesn’t know anything about the orchestrator or how replies are generated; it only knows that when an event shows up with a certain user ID, it should format it for the client. Conversely, the orchestrator doesn’t know about the SSE details; it just publishes events to the bus and assumes someone (if anyone is listening) will forward them to the user. This publish/subscribe boundary ensures that the process of generating answers is decoupled from the process of delivering them. It also allows flexibility: multiple clients could listen (subscribe) for the same user’s events (for example, if a user is logged in from two devices, both could get the answer). Or if no client is currently connected, the event simply goes nowhere (and that’s fine for real-time messaging). The SSE interface to the client is one-way, so we treat the Event Bus → SSE → Client path as a strictly outbound pipeline carrying only the data the core pushes out.

Overall, these interface boundaries create a clear separation: the **client only talks to the API**, the **core only talks to external services via defined protocols**, and all internal communication is mediated by simple events. No component reaches around another or breaks these abstractions. This makes reasoning about the system easier since data flow is predictable and linear through these boundaries.

## Cross-Cutting Concerns and Principles

Finally, there are several important design considerations that apply across all parts of Cortex Core:

- **Authentication and Security:** Every request into the system is authenticated. By using industry-standard JWTs via Auth0, Cortex Core avoids inventing its own auth scheme and ensures a robust security baseline. Each component trusts the `user_id` derived from the JWT to enforce isolation. There is no notion of an unauthenticated request doing anything in the core. Additionally, because external services (Memory, Cognition) only accept calls from Core (which includes the user context), we can later add service-to-service authentication or tokens if needed, but in the MVP we assume a trusted environment for those internal calls. Sensitive operations are all guarded by checking identity, and no user data leaves the system except to that same authenticated user’s SSE stream.

- **User Data Isolation:** Cortex Core is built to be multi-user from the start. A user’s conversation and data are kept completely separate from another’s. This is achieved by scoping every piece of data with a user identifier and often a conversation/session identifier. The Memory Service partitions stored messages by user (and would use different storage buckets or database records per user). The Event Bus events carry the user ID so that subscribers only pick up their own events. Even in memory (RAM), when the orchestrator works on a message, it holds that user’s context only for the duration of processing. There is no global state that mixes user data. This isolation is critical for both security and correctness (one user’s history should not pollute another’s answer). It also means the system could be extended to serve many users in parallel – their data paths will simply not intersect. Developers must ensure any new component or service follows this rule: always tag and segregate data by user (and where relevant by conversation).

- **Stateless Core & Scalability:** Cortex Core itself does not maintain persistent state between requests – it’s fundamentally stateless aside from in-memory caches or the ephemeral event bus. All long-lived state (like conversation history, extended knowledge, etc.) is offloaded to external services (Memory, etc.). This stateless design means that we can run multiple instances of the Core behind a load balancer without worrying that one instance “knows” something another doesn’t. As long as they share the same backend services, any instance can handle any request for a given user. It also means if the Core process restarts, no conversation data is lost – the Memory Service still has it. Statelessness simplifies a lot of the development (no complex session management) and aligns with the simplicity philosophy. It also improves resilience: a crash in the orchestrator only affects the in-flight tasks, not the stored data, and a new instance can pick up the work. We do ensure that some operations, like writing a user message to memory, are done synchronously at input to avoid losing data if a crash occurs mid-processing. In summary, **Cortex Core can be thought of as a pure function or pipeline – input comes in, output goes out, and all state goes elsewhere**.

- **Clear Separation of Concerns:** Every component in this architecture has a single, clear purpose, and the interfaces between them are narrow and well-defined. This reduces complexity dramatically. For example, the API layer doesn’t attempt to process or understand the conversation – it only handles HTTP and auth. The orchestrator doesn’t store data permanently – it asks Memory to do it. The Memory Service doesn’t know why it’s storing data – it just does it when asked. The LLM just generates text given a prompt – it doesn’t need to know who the user is or where the data came from. This separation follows a microservice-like philosophy but on a very small, MVP scale. It prevents unintended couplings; changing one component (like swapping out the LLM or modifying memory storage details) has minimal impact on others. For the AI assistants and developers working on the system, this clarity means you can work on one part without worrying about breaking others, as long as you honor the interface.

- **Minimalism and Evolution:** The Cortex Core MVP includes only what is necessary for the current functionality, but the way it’s structured allows for growth. By following the implementation philosophy of _“build the simplest thing that works now”_, we avoid over-engineering. There are no superfluous layers or abstractions – if something can be done with a direct method call or a straightforward data structure, we do it that way. This minimalism is intentional. It makes the system easier to reason about and reduces bugs. Whenever a new requirement comes (say we need a new tool service, or a more complex conversation flow), we can add it following the same patterns (MCP for new services, new event types if needed, etc.) without having to refactor core assumptions. In other words, the architecture is **extensible by addition**, not by modification of what’s already working. Logging, error handling, and other cross-cutting features are kept very simple in MVP (just enough to debug issues) and can be expanded as necessary once the basic end-to-end pipeline is solid.

- **Cross-System Consistency:** We use common libraries and conventions to keep the system consistent. For example, Pydantic models ensure that data structures (like message formats, tool request schemas, etc.) are well-defined and validated across the board. The same Auth0 JWT that the API uses is also the source of truth for user identity in all services (they rely on the user ID passed in, which comes from that token). By not duplicating functionality (no separate user accounts in Memory service, for instance), we reduce complexity. All services follow the same pattern of being a **MCP server** with defined tools, so the Core interacts with each in a uniform way. This consistency reinforces reliability: every integration works similarly, and developers/assistants have a predictable model to follow when adding new parts.

By adhering to these cross-cutting concerns, Cortex Core MVP achieves a balance of simplicity, clarity, and functionality. The system is easy to understand at a glance: a message comes in, goes through a clear series of steps, and an answer goes out. Each step is isolated, secure, and minimal. This shared understanding of the architecture should enable any AI assistant or engineer working on one part of the system to do so in alignment with the whole, without stepping on each other’s toes. The architecture’s simplicity is a feature: it makes collaborative development by multiple isolated assistants feasible, since the interactions are unambiguous and the context (this document) covers everything essential about how the pieces fit together.
