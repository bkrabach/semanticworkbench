# Whiteboard Memory Service – FastMCP SSE Implementation Guide

## Overview and Goals

The Whiteboard Memory Service is a standalone microservice that provides a persistent, summarized “whiteboard” memory for conversations. It runs as an independent MCP server (Model Context Protocol server) using Server-Sent Events (SSE) for communication. In the MCP architecture, hosts (like chat applications or IDEs with an MCP client) connect to multiple specialized servers over a standard protocol. This memory service is one such server focused on contextual memory: it listens for context update events and maintains a shared memory artifact that can be retrieved as needed. The “whiteboard memory” is a concise, running summary of recent events – a **shared memory** that agents use to track tasks and decisions. The service’s core goal is to continuously **update and serve this persistent summary** so that AI assistants have an efficient, up-to-date context without needing the entire history.

**Key Features and Requirements:**

- **Independent MCP Server:** The service is implemented with FastMCP (a Python framework for MCP) and runs separately from the main AI application. It communicates via SSE, following the MCP protocol for event streaming and bi-directional calls (the FastMCP framework handles the SSE details for us). This means the memory service can be started on its own and the core AI assistant (MCP host) can connect to it and call its tools/resources securely over a persistent SSE channel.
- **Whiteboard Memory Pattern:** It implements the “whiteboard memory” design – maintaining a persistent textual summary of important context. On each relevant context change (e.g. new user message, agent action, or domain event), the service uses an LLM to integrate that information into the whiteboard memory. The memory is kept concise (approximately **two pages of text** maximum) by merging and compressing new information with existing memory. This prevents unlimited growth and ensures the summary remains quick to read and inject into prompts.
- **Context Update via LLM:** The service offers a **tool** (function) that the core can call, e.g. `context_update`. When invoked with new context items (such as recent chat messages or events), the service will fetch recent context (to provide continuity), then call an LLM (OpenAI/Anthropic) to produce an updated memory summary. It uses **Pydantic-AI** to structure this LLM interaction, benefiting from typed inputs/outputs and message history management. The prompt to the LLM is carefully designed to incorporate the current memory and new context and instruct the model to output a revised summary under the size limit.
- **Rolling Recent Context Window:** If not all context is provided on each update call, the service maintains a short **rolling window** of recent context items in memory (not persisted to disk). This window might be the last N messages or events relevant to the conversation. On each `context_update`, the service appends the new items to this in-memory list (trimming older entries beyond N) so that the LLM sees a bit of recent history for continuity. This helps the model update the summary in a coherent way even if it only receives incremental changes. (For example, if the core only sends the most recent user message on each call, the memory service can include a few preceding messages from its window to give the model context).
- **Persistent Summary Storage:** Only the **whiteboard summary itself is persisted** between sessions – **not** the raw input context. This persistent memory artifact (a string blob of the summary) is stored in a minimal data store (for the MVP, a simple file or a SQLite database can be used). Storing only the distilled summary protects privacy (no full chat logs stored in this service) and keeps storage requirements low. When the service restarts or a memory needs to be retrieved, it loads this summary from storage. New updates overwrite the stored summary with the latest version.
- **MCP Resource for Memory Retrieval:** The service exposes the latest whiteboard memory via an MCP **resource** endpoint, allowing the assistant (or other agents) to fetch it by a URI. Each distinct conversation’s memory is identified by a URI scheme such as `memory://<user_id>/<conversation_id>`. For example, the memory for user `alice` in conversation `123` might be available at `memory://alice/123`. The MCP client/assistant can load this resource to get the summary text to include in prompts or analysis. Under the hood, this is implemented as a FastMCP resource function that looks up the memory by user and conversation and returns the stored summary.
- **FastMCP Protocol Compliance:** The server is built to be fully MCP-compliant, meaning it registers its tools and resources with FastMCP and communicates using MCP message formats. Tools are like RPC calls (with input parameters and a return value) and resources are like REST GET endpoints that provide data. The memory service will define its tool(s) and resource(s) accordingly so that any MCP client (the AI core or even a test client) can interact with it through the standard protocol. FastMCP’s SSE transport ensures real-time, event-driven updates and easy integration with LLM hosts.

## High-Level Architecture

**Components:** To implement these features cleanly, we break the service into focused modules. Following the principles from the project’s architecture guidelines (separation of concerns, clarity, and testability), each component of the memory service will have a dedicated module or class. The main components include:

- **MCP Server Interface:** The entry point that initializes the FastMCP server and defines the MCP **tools** and **resources**. This component ties everything together – it registers the `context_update` tool and the `memory://...` resource with the FastMCP instance, and it starts the SSE server loop. We will use FastAPI/Starlette via FastMCP to run the SSE endpoint, but FastMCP abstracts most details. Essentially, this is the glue between the external MCP calls and the internal logic (it calls into the Memory Store and Update Logic when requests come in).
- **Memory Update Logic (Whiteboard Updater):** This is the core LLM-driven logic that updates the summary. It encapsulates how we prompt the LLM and process its result. Given the current memory state and new context events, it constructs the prompt and invokes the LLM (via PydanticAI) to get an updated summary. This component should be isolated so it can be unit-tested by simulating the LLM. We’ll likely implement this as a function or class method (e.g. `update_memory(current_memory, recent_events) -> str`) that uses a PydanticAI **Agent** to run the prompt and returns the new summary text. It will also handle ensuring the summary doesn’t exceed desired length (we can instruct the model accordingly and possibly post-truncate if needed).
- **Memory Store:** The storage layer responsible for retrieving and saving the memory artifact. It provides methods like `get_memory(user, conv) -> str` and `save_memory(user, conv, text)` and also manages the in-memory recent context cache for each conversation. For simplicity, we can implement the persistent store as either file-based (each memory artifact stored in a file named by user+conv) or a lightweight database (SQLite table mapping (user, conv) to memory text). This module abstracts those details so the rest of the service just calls the store. It also keeps a dictionary of recent context events per conversation in memory, which is updated on each context_update call and trimmed to the configured window size.
- **LLM Handler (Pydantic-AI Integration):** While the update logic uses PydanticAI to run a prompt, we may encapsulate the configuration of the LLM model and any structured output models separately. This could be part of the update logic or a helper module. Essentially, we need to configure which model to use (OpenAI GPT-4, Anthropic Claude, etc.), handle API keys and any system prompts. Pydantic-AI allows defining the model by a string like `"openai:gpt-4"` or `"anthropic:claude-2"` and will route calls appropriately. We will leverage **structured outputs** if needed (in our case the output is just a string summary, so a simple `str` return type is fine), and possibly define a Pydantic `BaseModel` for the context items input. The LLM handler part ensures we don’t scatter model-specific details around – it can read configuration (model name, max tokens, etc.) and create the global `Agent` instance used for all updates.

By splitting into these pieces, we make the system easier to implement and test. Each part can be developed and verified in isolation (for instance, we can test MemoryStore with fake data, test the prompt logic with a dummy model, etc.) in line with our **implementation philosophy** of modular, maintainable design.

## Project Structure and Module Layout

We will organize the code into a small package (e.g., `whiteboard_memory_service/`). Here’s a suggested layout of files and their responsibilities:

- **`server.py`:** The main entry point of the service. This will create the FastMCP server instance and register all tools and resources. It also contains the `if __name__ == "__main__": server.run()` to launch the server. In FastMCP, you typically instantiate a `FastMCP` object, decorate functions with `@server.tool()` or `@server.resource()`, then call `server.run()`. For example, `server.py` might look like:

  ```python
  from mcp.server.fastmcp import FastMCP
  from whiteboard_memory_service.memory_store import MemoryStore
  from whiteboard_memory_service.memory_updater import MemoryUpdater, ContextEntry

  server = FastMCP("WhiteboardMemoryService")  # Initialize MCP server

  store = MemoryStore()        # instantiate the memory storage
  updater = MemoryUpdater()    # instantiate LLM updater (sets up Agent internally)

  @server.tool()
  async def context_update(user_id: str, conversation_id: str, entries: list[ContextEntry]) -> bool:
      """Tool to incorporate new context entries into the whiteboard memory."""
      # Retrieve current memory and recent context for this convo
      current_mem = store.get_memory(user_id, conversation_id)
      recent_events = store.get_recent_context(user_id, conversation_id, entries)
      # Use LLM to get updated memory summary
      updated_mem = await updater.update_memory(current_mem, recent_events)
      # Save the new memory and update recent context window
      store.save_memory(user_id, conversation_id, updated_mem)
      return True  # indicate success (or could return nothing)

  @server.resource("memory://{user_id}/{conversation_id}")
  def get_memory(user_id: str, conversation_id: str) -> str:
      """Resource to retrieve the latest whiteboard memory for a user conversation."""
      return store.get_memory(user_id, conversation_id)

  if __name__ == "__main__":
      server.run()
  ```

  This structure shows a dynamic resource (with placeholders in the URI) to fetch memory and a tool function to handle updates. The tool uses our `MemoryStore` and `MemoryUpdater` to do the heavy lifting. FastMCP will automatically handle client requests to these endpoints and convert data to/from JSON as needed.

- **`memory_store.py`:** Contains the `MemoryStore` class. This class manages storage and retrieval of memory and context. Key methods: `get_memory(user, conv)` (load from an internal dict or file if not in memory), `save_memory(user, conv, text)` (store to dict and persist to file/db), and `get_recent_context(user, conv, new_entries)` which updates the in-memory recent events list for the conversation (e.g., appending `new_entries`, then returning a list of recent entries to use for the LLM prompt). Internally, `MemoryStore` might have a dict like `self.memory_data = { (user,conv): "summary text" }` and `self.recent_events = { (user,conv): [ContextEntry, ...] }`. On initialization, it can load any existing memory files or initialize an empty state. For persistence, a simple approach is to use a directory (like `./data/`) and store each memory as a file named `<user>_<conv>.txt`. On `save_memory`, write/update that file; on `get_memory` (if not in the dict), read from file if exists. This way the file system is our persistent store. (Alternatively, use SQLite: on init, connect to `memory.db` and create a table if not exists; use SQL for get/save. For the guide, file-based is easier to implement and explain.)

- **`memory_updater.py`:** Houses the logic for merging context and calling the LLM. This could define a class `MemoryUpdater` that encapsulates the LLM **Agent**. In its `__init__`, it would configure the Pydantic-AI Agent, e.g., selecting the model and setting a system prompt template. For example:

  ```python
  from pydantic_ai import Agent, Message
  from pydantic_ai.models import openai, anthropic  # ensure model providers available

  class MemoryUpdater:
      def __init__(self, model_spec: str = None):
          model_spec = model_spec or "openai:gpt-4"  # default model (could be set via env)
          # Create an agent with a system role prompting style instructions
          system_prompt = ("You are a helpful assistant that maintains a shared memory.\n"
                           "You will receive a current memory summary and new events. Merge them into an updated summary, concise (~2 pages), preserving important details and context. Omit trivial details and keep the summary coherent and comprehensive.")
          self.agent = Agent(model_spec, system_prompt=system_prompt)

      async def update_memory(self, current_memory: str, recent_events: list[ContextEntry]) -> str:
          # Compose the prompt for the LLM
          if current_memory:
              intro = "Current memory:\n" + current_memory + "\n"
          else:
              intro = "No existing memory (start fresh).\n"
          events_text = "\nNew events:\n" + "\n".join(f"- {e.role}: {e.content}" for e in recent_events)
          user_prompt = intro + events_text + "\n\nUpdate the memory accordingly:"
          # Run the agent (LLM call) with the composed prompt
          result = await self.agent.run(user_prompt)
          return result.data if result.data else ""
  ```

  The above pseudocode shows how we might prompt the model. We include the current memory and the new events, then ask the model to produce an updated memory. The system prompt (set once in the Agent) instructs the model on style and length constraints (to always keep it concise, etc.). We treat all new context entries as text lines (potentially denoting who said/did what). The `ContextEntry` dataclass (or Pydantic model) might have attributes like `role` (e.g., “user” or “assistant” or a system event type) and `content`. We then call `agent.run()` with the assembled user prompt. PydanticAI handles the API call to the configured model and returns a result object; `.data` holds the main content (it’s a string here because we expect a textual answer). Using an async call (`await self.agent.run(...)`) allows the tool function to be async as shown. We may also consider streaming or partial updates, but for MVP a single call/response is fine.
  _Note:_ For conversation continuity, Pydantic-AI supports passing a `message_history` to include prior messages. In this case, however, we are manually providing the needed context (current summary and new events) in the prompt, so we don’t maintain a multi-turn dialogue with the model – each update is a fresh prompt. This simplifies the interaction.

- **`models.py`:** Defines Pydantic models for structured data used by the service. For instance, a `ContextEntry` model can subclass `BaseModel` to validate context items passed into `context_update`. Example:

  ```python
  from pydantic import BaseModel
  class ContextEntry(BaseModel):
      role: str   # e.g. "user", "assistant", "system"
      content: str
      timestamp: float | None = None  # optional, Unix time or similar
  ```

  By using a Pydantic model, FastMCP will automatically parse incoming JSON into `ContextEntry` instances when the tool is called, ensuring the data format is correct. We can extend this model as needed (for example, a type field if we want to distinguish message vs file artifact, etc., but role+content suffice for now). We might also define a model for the memory artifact itself (e.g. `class Memory(BaseModel): content: str`), although since our resource just returns a string, that may not be necessary. Pydantic models also come in handy for structured outputs, but here the output of the update is a simple string summary (so we can return `str` directly from the tool).

- **`config.py`:** (Optional) Contains configuration handling, such as reading environment variables or a config file for settings like model choice, API keys, or persistence options. For example, we might have `MODEL_SPEC = os.getenv("MEMORY_MODEL", "openai:gpt-4")` and use that in `MemoryUpdater`. Similarly, if using file storage, `DATA_DIR = os.getenv("MEMORY_DATA_DIR", "./data")`. Having a config module centralizes such values. Ensure secrets like API keys are not hardcoded – PydanticAI will pick up OpenAI or Anthropic keys from their respective environment variables (e.g. `OPENAI_API_KEY`, `ANTHROPIC_API_KEY`), so we rely on those being set in the environment where the service runs. Document how to configure these in deployment.

- **`tests/`**: A directory for unit and integration tests (see **Testing** section below for details). We’ll likely have test modules for the `MemoryStore` (testing that saving and loading works and that recent context window behaves correctly), for the `MemoryUpdater` (using a dummy model to test prompt assembly and result post-processing), and for the MCP interface (possibly using FastMCP’s testing tools or a test client to simulate calling the tool and resource).

This module breakdown adheres to the single-responsibility principle: each file has a clear purpose, making it easier to maintain. For example, the logic of how to prompt the LLM is in one place, separate from how data is stored. This also means an AI assistant (or a developer) can work on one part (say, tweaking the prompt strategy) without touching the networking or storage code, which aligns with our design philosophy.

## MCP Interface: Tools and Resource Definitions

The contract between the memory service and the core assistant is defined by an MCP **tool** for updates and an MCP **resource** for retrieval:

- **`context_update` Tool:** The `context_update` tool is an MCP endpoint (similar to a POST API call) that the core system will invoke whenever new context is available that should be reflected in memory. For example, after each user message or assistant response, the orchestrator can call `context_update` with that content (and possibly some recent messages if available). In our implementation, `context_update` is defined as an async function decorated with `@server.tool()`. It takes structured input (user_id, conversation_id, and the list of new context entries). Upon execution, it uses the MemoryStore and MemoryUpdater as described above: load current memory, update it with LLM, save new memory. We defined it to return a boolean (True) indicating success. This could be extended to return the updated memory text or some metadata, but since the assistant can always retrieve the memory via the resource, a simple acknowledgement keeps the tool lightweight. Also note: we might want to throttle or debounce updates – if many messages arrive in quick succession, the core might call this too frequently. For MVP, assume it’s called in a controlled manner (one call at a time per conversation).

- **`memory://{user_id}/{conversation_id}` Resource:** This resource allows read-only access to the memory summary. We register it with `@server.resource("memory://{user_id}/{conversation_id}")` and implement a function that fetches from MemoryStore. FastMCP will handle incoming GET requests from the client (which might be triggered when the LLM tries to access the memory, or preemptively by the orchestrator to supply context). The URI uses a custom scheme `memory://` to clearly indicate this is the memory artifact (this is analogous to e.g. `users://{id}/profile` examples in FastMCP. The MCP specification allows defining such custom resource schemas to organize data sources. Our resource function simply returns a string (the summary), and FastMCP wraps that into an MCP resource response. If the memory does not exist yet for the given IDs, it can return an empty string or a message like “(Memory is empty)” – the design choice here could be to return empty and rely on the assistant to handle it.

**Internal Workflow:** When the core assistant needs context memory, it will perform an MCP **load** of the resource URI, which under the hood calls our `get_memory` function. This could be done automatically by the AI if given the URI, or explicitly by the orchestrating code. Similarly, the `context_update` tool will be called by name. In testing or manual use, one could use an MCP client library to call these endpoints directly to verify they work (see Testing section).

**SSE and Session Handling:** FastMCP manages SSE connections and session IDs for MCP under the hood. In our server, we don’t need to manually write SSE logic; calling `server.run()` will start an event loop and expose an `/sse/` endpoint that clients connect to. The FastMCP library sets up routes for SSE and message exchange as needed. We just ensure our FastMCP instance (`server`) is created and run. By default, FastMCP likely uses uvicorn to run an ASGI app. We could also integrate it into a larger FastAPI app if needed (mounting the routes), but since this is a dedicated service, running it standalone (e.g., `uvicorn server:server_app`) is fine. The main point is that once running, any MCP-compliant client can connect, initialize a session, and call our tool/resource as defined. The Core AI (host) will manage the session and call these as needed.

## LLM Prompt Strategy for Memory Updates

One of the most important pieces is how we prompt the LLM to update the whiteboard memory. The prompt must be crafted to merge the old summary with new information effectively, without exceeding the target length (~2 pages of text). Here’s our plan for the prompt and LLM usage:

- **System Prompt:** When initializing the `Agent` in PydanticAI, we supply a system prompt (the role instructions). This can instruct the model about its task and any constraints. For example: _“You are an AI memory manager. Your job is to maintain a running summary (‘whiteboard memory’) of a conversation or session. When given new events, update the summary to include them. The summary should be concise (no more than ~2 pages), capturing key points and decisions. Avoid verbatim repetition; integrate information logically. Only output the updated summary text, nothing else.”_ This ensures the model knows the context of its role and the desired output format (just the summary text, no explanations). We included a similar system message in `MemoryUpdater.__init__` above. With Anthropic models like Claude, this might go as an `INSTIGATOR` or system message; with OpenAI, as the system role content. PydanticAI handles placing it appropriately based on the model provider.

- **User Prompt Composition:** For each update, we provide the current state and new info. We decided to prepend a short description of the current memory (if any) and then list the new events. The example in code assembled something like:

  > **Current memory:** > _... (existing summary text) ..._ > **New events:**
  > – user: _"Hello, I need help with X"_
  > – assistant: _"Sure, let me check..."_

  > **Update the memory accordingly:**

  This format clearly delineates what was known (current memory) and what just happened (new events). The model is then asked to produce the “updated memory”. By showing the current summary, we allow the model to choose whether to modify, expand, or rewrite parts of it. By listing new events, we ensure it has the raw info to incorporate. We prefix each event with who or what it is (user, assistant, system, or other agent) so the model has context of perspective. We also could include timestamps or sequence if relevant, but likely not needed in summary beyond ordering as given.

- **Length Control:** We explicitly mention in the system prompt to keep it around two pages. We can’t hard enforce token count easily, but we trust the model to follow the instruction. If using Anthropic’s Claude, which has a very large context window, we might also include something like “(The final summary should be around 1000 words or fewer.)”. For OpenAI GPT-4, 2 pages (~800-1200 words) is well within its capabilities too. If the model for some reason returns an excessively long summary, we might implement a safeguard: e.g., after getting `result.data`, if its length in characters or tokens is way above a threshold, we truncate it (preferably at a sensible boundary). In practice, a well-crafted prompt should prevent that.

- **Merging Strategy:** We rely on the LLM to integrate new info intelligently. For example, if the current memory says “We have discussed A and decided B,” and a new event is the user changing their mind on B, the model should update that part of the summary. This is complex, but LLMs are quite capable of summarizing with incremental updates when instructed. We emphasize not to just append – instead to produce a cohesive new summary. If needed, we could prompt with something like “Here is the updated summary (revise where necessary):” to reinforce rewriting.

- **Structured Output Considerations:** We want just plain text back. We do **not** want the model to return JSON or any structured format in this case (we’re not extracting specific fields – just maintaining a narrative summary). Therefore, we do not define a Pydantic result model for the LLM output; we simply let it output text. (PydanticAI by default will capture the output as `result.data` if the return type is `str` as in our agent run usage). If we did want a structured return, PydanticAI supports a `result_type` on the Agent or even function-based “function calling” to enforce output schema. For example, we could have required the model to output JSON `{"memory": "<summary>"}`, but that adds overhead of parsing and might confuse the model’s brevity. Simplicity is preferred here.

- **Model Selection:** The choice of model can affect the quality of summarization. Claude (Anthropic) is known for long context and might be very good at summarizing large transcripts concisely. GPT-4 is also very capable. We allow configuration of `model_spec` (like `"anthropic:claude-2"` vs `"openai:gpt-4"`). This can be done via environment variable or config (e.g., set `MEMORY_MODEL="anthropic:claude-2"` to use Claude; otherwise default to GPT-4). PydanticAI uses the prefix (`openai:` or `anthropic:`) to route the request appropriately. Ensure the necessary API keys are available. For OpenAI, `OPENAI_API_KEY` must be set; for Anthropic, `ANTHROPIC_API_KEY` must be set. The service should be flexible to use either. In code, as shown, we pick a default but allow override.

- **Pydantic-AI Agent Reuse:** We create one Agent and reuse it for all calls (the `MemoryUpdater.agent` instance). This is efficient and also allows the Agent to maintain some state if needed. PydanticAI’s Agent is lightweight and can be called multiple times. If we needed per-conversation separate system prompts or different models, we could manage multiple Agents, but it’s not necessary here. All memory updates follow the same instructions format.

By carefully designing the prompt and leveraging PydanticAI for model calls, our memory updates are handled in a consistent, structured way. The **principle** followed is to offload as much of the summarization logic to the LLM (since it’s good at that), while we provide clear guidance (prompts) and guardrails (length instruction, separate fields for old vs new info). This approach aligns with the AI Assistant Guide’s recommendation to let the AI handle content generation within well-defined boundaries, and keeps our code focused on preparing and post-processing that content.

## Persistence and Data Management

Ensuring that the memory persists across restarts and that we don’t accumulate too much data are important considerations:

- **Persistent Storage Implementation:** As discussed, we can choose between file-based or SQLite. For a minimum viable product, a file approach is straightforward: Create a folder (say, `data/`) and use a naming scheme for memory files. One scheme is `<user_id>__<conversation_id>.txt` (using a delimiter like double underscore in case either ID contains a slash or special char). The MemoryStore will map a (user_id, conv_id) to a file path. On `save_memory`, open the file in write mode and write the summary text. On `get_memory`, if the entry is not loaded in memory, open the file in read mode. We should also handle the case where the file doesn’t exist (meaning no memory yet) – then return empty string or None. File I/O errors should be caught and logged.

  If using SQLite, we would define a table like `memory(user TEXT, conversation TEXT, content TEXT, PRIMARY KEY(user, conversation))`. On save, upsert (INSERT OR REPLACE) the row; on get, SELECT the content. SQLite file (e.g., `memory.db`) would be stored in the service directory. Since the data is small (just summary text per conversation), performance is not an issue. SQLite adds a dependency and slightly more complex setup (creating tables), so for now, files are fine. The guide can note that swapping to a DB later is feasible if needed (especially if we want to store additional metadata or handle concurrent writes more robustly).

- **Recent Context (Non-Persistent):** The recent events cache should **not** be written to disk; it’s only to aid incremental updates while the service is running. If the service restarts, we assume that either the core will send a larger batch of context on the next update or that minor loss of that ephemeral context is acceptable. The MemoryStore’s `recent_events` dict is reset on restart. We limit this list to a certain number of items or cumulative size. A reasonable default might be the last 5-10 events, or last N characters. This prevents unbounded growth in RAM. We can make this limit configurable (e.g., keep last 5 messages by default). The logic in `get_recent_context(user, conv, new_entries)` would do: append new_entries to `recent_events[(user,conv)]` list, then if length exceeds N, drop the oldest. It then returns either the full list or perhaps the tail of the list that will actually be used for prompting. We might actually decide to use the entire recent list for the prompt every time along with current memory. Or we could decide only the new entries are needed since we always have the up-to-date memory (which already contains older info). Including some recent prior context can help the model with the nuances of how to integrate, but including too much might lead to redundancy. A balanced approach: maybe include at most e.g. 3 most recent events before the new ones, just to provide immediate lead-in context. We can experiment with what yields the best summary updates.

- **No Raw History Stored:** It’s worth emphasizing that we do **not** store raw conversation history long-term in this service. The role of this service is a distilled memory. If full history logging is needed, that would be another component (or the core itself). This keeps our storage small. Each memory file essentially contains the running summary (which might be, say, a few kilobytes at most). This is very lightweight – even if there are 1000 conversations, the stored data might be a few MB total in worst case. File naming should handle safe characters (if user or conv IDs are GUIDs or such, that’s fine; if they could contain slashes or spaces, we should sanitize or base64-encode them to form file names). These are engineering details the implementer should be aware of.

- **Resource Access to Storage:** The `get_memory` resource will call `store.get_memory`. If performance becomes a concern (say, reading from disk every time), we can rely on the MemoryStore caching it in memory after the first load. So `get_memory` could effectively return from `store.memory_data` dict. We just need to ensure that when we `save_memory`, we update the dict as well as the file. In our design, `save_memory` does both. Therefore, subsequent `get_memory` calls are fast (in-memory). If a memory was never saved (no file), `get_memory` returns empty. If it was saved in a previous run but not yet loaded in the dict since restart, `get_memory` will open the file and then store it in dict for next time. This way, we don’t repeatedly hit the disk for the same memory on each call.

- **Multiple Users/Conversations:** The keys (user_id, conversation_id) uniquely identify a memory. It’s possible that the service might be used in contexts without a user (maybe just conversation or session IDs). Adjust accordingly – we primarily expect an AI assistant with a notion of user (or organization) and a conversation thread. If user_id is not relevant, the conversation_id alone could index memory. The code can treat the composite as a single key as well (like combine into one string). But using a tuple (user, conv) is straightforward in Python dictionaries and for file naming.

By designing the persistence in this minimal way, we fulfill the requirement of **persisting the memory artifact** reliably, without over-engineering. This also means the service could be deployed in a serverless environment or restarted frequently and not lose the conversation memory (as long as the file/DB persists). It also simplifies compliance, since the only stored data is the summary (which presumably is less sensitive than raw logs and can be deleted easily if needed).

## Configuration and Security

We want our implementation to be flexible and secure out-of-the-box:

- **Model and API Key Config:** Avoid hardcoding any API keys or model names. Use environment variables for keys (which is standard for OpenAI/Anthropic SDKs). The PydanticAI library will look for these; for instance, using `anthropic:` model strings requires `ANTHROPIC_API_KEY` to be set, and `openai:` requires `OPENAI_API_KEY`. Document that the user of this service should export those env vars before running the server. For model selection, an environment variable (like `MEMORY_MODEL`) can be used. Alternatively, one could use a simple config file (YAML/JSON) read at startup to choose the model and possibly the max tokens or other parameters. We can illustrate using env: in `MemoryUpdater.__init__`, `model_spec = os.getenv("MEMORY_MODEL", "openai:gpt-4")`. If one wants to switch to Claude, they just set that variable to `"anthropic:claude-2"` when running the service.

- **Port and Host:** FastMCP likely runs on a port (default maybe 8000). Make sure to allow configuring host/port if needed, especially if deploying containerized. Possibly, FastMCP’s `server.run()` allows passing host/port arguments. If not, it might read from env like `PORT`. We should check FastMCP docs or just mention that by default it listens on localhost. If the Core is on the same machine, that’s fine. If not, ensure network access and maybe run with `--host 0.0.0.0` if needed.

- **Authentication:** At MVP, MCP does not inherently authenticate tool calls – it’s assumed the environment is controlled (e.g., an LLM like Claude Desktop connects locally). If needed, one could implement an auth token check in the server (FastMCP might support requiring a token for connection). The guide can note this as an advanced consideration but not implement it initially.

- **Logging and Debugging:** For development, enable logging to see when context updates happen and how the prompt looks (to debug the summarization). We can add simple `print` or use Python’s `logging` library. PydanticAI might also have verbose logging or a debug mode to show the prompts and raw model responses (which helps during testing to ensure the model is following instructions). Especially as we refine the prompt, seeing the model’s output is useful.

- **Adherence to Implementation Principles:** The guide itself is written to allow an AI assistant developer to implement the service step by step. We ensure that each piece is documented and justified according to principles in **IMPLEMENTATION_PHILOSOPHY.md** (like modular design, minimal persistence, clarity of purpose for each module) and the **AI_ASSISTANT_GUIDE.md** (likely suggesting the assistant to think in terms of responsibilities, to test often, and not to inject personal data etc.). Security-wise, since we only store summaries, the risk is lower, but still treat the summary as possibly sensitive (it may contain personal info summarized). So handle file permissions accordingly (only accessible to the service).

## Testing Strategy

Testing is crucial to ensure the Memory Service works correctly and follows the intended behavior. We will write tests for each logic layer, using PydanticAI’s tools to avoid actual API calls where possible:

- **1. MemoryStore Tests:** We can test `MemoryStore` independently with normal Python unit tests (no LLM needed). For example, using temporary directories or an in-memory approach:

  - Test that saving a memory and retrieving it returns the same content. This covers both the in-memory dict and the file persistence. Use a unique temp file path to not collide with real data. After `save_memory("user1", "convA", "Hello")`, ensure `get_memory("user1","convA")` returns `"Hello"`. Also restart the store (simulate by creating a new MemoryStore instance, which should load existing files) and check it still retrieves the same.
  - Test that recent context window truncates properly. e.g., set max events = 3 (perhaps allow MemoryStore to be initialized with a custom max for testing). Then call `get_recent_context` sequentially with multiple sets of new entries and ensure the length of stored events doesn’t exceed 3 and that the oldest are dropped. Ensure order is maintained (we likely want it in chronological order).
  - Test edge cases: requesting memory for an unknown key (should return empty string), saving an empty summary (store should handle writing an empty file or deleting the file), etc.

- **2. MemoryUpdater (Prompt Logic) Tests:** We want to test that given certain inputs (current memory + new events), our prompt formatting and result handling produce the expected output. This is tricky to test fully with a live LLM (since output will vary). Instead, we use **PydanticAI’s `TestModel`** to simulate the LLM. `TestModel` is a special dummy model that generates a predictable output without calling external APIs. It basically tries to produce output conforming to the expected schema or just echoes back structured data. We can leverage this by injecting `TestModel` into our Agent. PydanticAI allows overriding the model on an Agent for testing. For example:

  ```python
  from pydantic_ai.models.test import TestModel

  updater = MemoryUpdater()
  updater.agent.override(model=TestModel(custom_result_text="UPDATED_SUMMARY"))  # replace real model with test
  test_current = "User greeted the assistant."
  test_events = [ContextEntry(role="user", content="How’s the weather?")]
  updated = asyncio.run(updater.update_memory(test_current, test_events))
  assert "UPDATED_SUMMARY" in updated
  ```

  In this snippet, we override the `Agent` inside our MemoryUpdater with a `TestModel`. We use `custom_result_text` to specify what the dummy model should output. By doing this, when we call `update_memory`, it will not call the real API but will immediately return a placeholder output (which we can define). The `TestModel` generates a dummy output that often includes the tool calls it would have made or reflects inputs. For simplicity, we might just check that our function returns _something_ without error. Or if using `custom_result_text`, that exact text is returned. This verifies that our code up to the model call is working (prompt constructed, etc.), and that we handle the result correctly. We should also test scenarios like empty current memory (the prompt should still work), multiple new events at once, etc. Each test doesn’t verify the semantic quality (since the dummy model isn’t summarizing), but it ensures the flow works.

  Additionally, if we want a more logic-driven test, we could subclass `FunctionModel` to mimic a summarization (for example, have it concatenate the current memory with a simplified version of new events). But that might be overkill – manual inspection using a real model in a dev setting might suffice for qualitative validation. The key is that our tests don’t depend on external calls (for reliability and speed), hence using `TestModel` as recommended.

- **3. Tool/Resource Integration Tests:** We should test that the FastMCP server correctly calls our functions. One approach is to use the MCP client in Python to simulate calls. FastMCP provides a way to run the server in-process and call tools directly (for example, using `mcp.ClientSession` as in FastMCP examples. Another simpler approach: since our tool and resource are just Python functions (decorated), we can call them directly in tests by instantiating the store/updater. But to truly simulate MCP, using an MCP client ensures our function signatures and decorators are correct. For instance:

  ```python
  import asyncio
  from mcp import ClientSession, StdioServerParameters
  from mcp.client.stdio import stdio_client

  async def run_memory_service_test():
      # Launch the server in a subprocess using stdio (FastMCP supports uvicorn invocation)
      server_params = StdioServerParameters(command='uvicorn', args=['server:server', '--port=0'])
      async with stdio_client(server_params) as (read, write):
          async with ClientSession(read, write) as session:
              await session.initialize()  # initialize MCP session
              # Call context_update tool
              payload = {
                  "user_id": "testuser",
                  "conversation_id": "testconv",
                  "entries": [ {"role": "user", "content": "Hello"} ]
              }
              result = await session.call_tool('context_update', payload)
              assert result.status == 0  # 0 indicates success
              # Then call the resource
              mem_content = await session.get_resource(f'memory://testuser/testconv')
              assert "Hello" in mem_content.content[0].text or mem_content.content[0].text == ""
  asyncio.run(run_memory_service_test())
  ```

  The above pseudo-test would start the server (using `uvicorn` to run `server:server` – which assumes in `server.py` we expose a FastAPI app or similar; FastMCP might handle differently. If `server.run()` starts its own loop, we might not use uvicorn manually. But FastMCP’s docs show a pattern with `stdio_client` or using `fastmcp install` in dev. This is a bit involved, and in many cases, simply testing the internal logic is enough. However, if we want to ensure the FastMCP integration is correct, a small integration test like this can be used. Alternatively, since FastMCP is a well-tested library, we might trust that if our functions do the right thing when called, the server will wire them up fine, as long as the decorator usage is correct.

  We should also test the resource function independently: call `get_memory("userX","convY")` when no memory is set – expect `""`. After setting a memory via store, call it again – expect that memory string. Because the resource function is simple (just calls store), unit testing it is trivial once store is tested.

- **4. Memory Artifact Integrity Tests:** Over a sequence of updates, ensure that the memory summary doesn’t grow unbounded. This can be done by simulating several context_update calls in a row and checking the length of the stored memory. For example, simulate a conversation: user says something, assistant responds, do this 5 times (call context_update each time with that turn’s messages). If our dummy model just appends, the summary would grow; ideally, the real model would compress. But with TestModel, by default it may not mimic compression. However, we can artificially limit summary length in our MemoryUpdater for test. Or simply assert that after each update, the summary length is not drastically more than before (maybe <= previous_length + new_input_length). This is hard to enforce without real summarization. Instead, we could measure that our prompt always includes the instruction to keep it short. That’s more of a prompt correctness test. Alternatively, integrate a real model in a staging environment and run a scenario to see if it behaves (this would be more of a manual or integration test, not unit test).

  Another aspect: test that no raw context is persisted. We can inspect the data directory or DB after some operations and ensure only the summary text is present, not the full transcripts. This can be part of a teardown check in tests. It’s essentially verifying our design (if we accidentally wrote context entries to disk, that would be a bug).

- **5. Using PydanticAI’s Test Utilities:** We have already mentioned `TestModel`. PydanticAI also provides `ALLOW_MODEL_REQUESTS` flag to ensure no real calls happen during tests (we can set `pydantic_ai.models.ALLOW_MODEL_REQUESTS = False` to be safe, so if any test accidentally tries to call a real model, it will error out rather than incur cost). We should set that at the top of our test suite. They also have `capture_run_messages` which can capture the message history of an agent run. We might use that to verify that our agent was prompted with the expected content. For example, wrap `update_memory` call in a capture, then inspect that the messages include the system prompt and the user prompt containing certain key phrases (like it should include "Current memory:" if current_memory was given). This directly tests that our prompt assembly logic is correct. It’s a bit advanced, but since PydanticAI makes it available, it’s a good way to ensure our instructions are actually being sent as we expect.

- **6. Manual Testing:** Finally, once unit tests pass, run the service and test manually. One can use a tool like `mcp-cli` if available, or even connect via a development interface (if using Cursor IDE or another MCP host). Given this is for an AI assistant use-case, ideally integrate it with the core AI to see it in action: e.g., have a conversation and see if the memory resource gets populated with a summary. This manual phase will validate the real LLM interaction and quality of summaries, which is hard to fully automate.

By following this testing strategy, we ensure each part of the service works in isolation and together. The use of PydanticAI’s test models allows fast, deterministic tests without needing actual LLM calls during CI or development. We also ensure that the design requirements are met (persisting only summary, maintaining size limits, etc.) by explicitly checking those conditions in tests. The Implementation Philosophy likely emphasizes testing with minimal external dependencies, and we’ve achieved that via the TestModel. The AI Assistant Guide probably also encourages verifying logic with both typical and edge inputs – our tests cover empty inputs, multiple events, etc.

## Conclusion

By following this guide, an AI assistant developer can implement the Whiteboard Memory Service in isolation with confidence. We defined a clear module structure, separating concerns of MCP interfacing, LLM interactions, and data management. We leveraged FastMCP to quickly expose functionality over SSE without dealing with low-level networking, and used Pydantic-AI to keep our LLM calls robust and testable. Key principles applied include: **modularity**, **clarity**, **minimal persistence**, and **structured AI integration**. The result is a mini-project that provides significant value – enabling an AI system to recall and summarize context – while remaining maintainable and aligned with the overall architecture. With proper testing in place and configuration handled via environment, the service can be deployed as a standalone component and integrated into the AI assistant’s workflow.

**Next Steps:** Once implemented, one can refine the summarization prompt based on real outputs, add support for additional context types (if needed, e.g., handling image descriptions or other modalities by adjusting how context entries are formatted), and possibly extend the memory concept (for example, introducing different memory “channels” or long-term vs short-term memories as future improvements). But as an MVP, this Whiteboard Memory Service meets the specified requirements and provides a solid foundation for contextual memory in an AI assistant system.
