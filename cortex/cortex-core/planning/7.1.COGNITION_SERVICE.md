# Cognition Service (FastMCP SSE Server) – Implementation Guide

## Overview and Goals

The **Cognition Service** is a standalone microservice responsible for the AI assistant’s reasoning and response generation. It runs as an independent **MCP server** using Server-Sent Events (SSE) for communication, separate from the core orchestrator. This service listens for **context change events** (e.g. a new user input, a memory insight, or a domain expert result) and decides how to react – whether to produce an assistant reply, defer action, or trigger other events in the future. In the MVP implementation, the Cognition Service will **generate a reply for every new user input**. This offloads the heavy LLM processing from the core, keeping the core simple and delegating all language model interactions to this service. The design follows Cortex’s architectural principles of **ruthless simplicity** and **separation of concerns**: the cognition logic is isolated here, with minimal coupling to other components. This guide outlines a clean, self-contained implementation, ensuring the service can be developed and tested in isolation (no external state or legacy dependencies required).

**Key Objectives for Cognition Service:**

- **Independent SSE Server:** Runs as its own FastMCP server process, exposing an SSE endpoint for the Core to connect and a matching endpoint for incoming commands (no stdio or in-process integration). This ensures cloud-native decoupling: the Core communicates with Cognition over HTTP SSE, exactly as it would with any external service.
- **Context-Driven Logic:** Replaces the Core’s old “response handler” loop with internal logic that reacts to events. On receiving a new user message event, the service will evaluate the conversation context (for now, primarily the user’s query, and optionally recent history) using a Large Language Model (LLM) and **decide an action**. For MVP, the action is always “generate an assistant reply and send it back.” In future iterations, this logic can become smarter – e.g. sometimes doing nothing (if waiting for more info) or dispatching a follow-up event (if the LLM requests a tool or external data).
- **LLM Interaction Layer:** Owns all communication with the LLM. Uses **Pydantic-AI** to interface with LLM providers (OpenAI, Anthropic, etc.) in a structured, type-safe way. The service will leverage Pydantic-AI’s best practices – defining models for the LLM output, maintaining message history as needed, and possibly using the agent/tool mechanism – without adding any custom abstraction around it. The goal is to call the LLM directly but with structured prompts and results, trusting the library to handle complexity.
- **Stateless and Testable:** The service should not require any external database or persistent state to function. It doesn’t maintain long-lived conversation data internally (that’s the Memory Service’s job). Each event is processed with the context provided (or fetched) at runtime. This makes the component easy to test in isolation – we can simulate input events and assert on outputs without needing a running Core or Memory service. Pydantic-AI’s `TestModel` can be used to avoid real API calls during unit tests, ensuring no external API keys or network calls are needed for testing.
- **Minimal and Direct Implementation:** Adhere to the project philosophy of simplicity. We avoid complex frameworks or indirection – using FastAPI/FastMCP directly for the server and Pydantic-AI directly for LLM calls. There are no unnecessary layers: events come in, we process them in one pass, and emit results. Error handling is basic but robust (no crashes on LLM errors; instead log and return a safe message). No legacy code is carried over – this is a fresh implementation focused only on current requirements, aligning with the “present-moment focus” principle.

## High-Level Design

**Event-Driven Flow:** The Cognition Service operates in an event-driven manner, reacting to incoming context events from the Core:

- The Core (Cortex orchestrator) will send a **“new input” event** to Cognition whenever a user submits a message. In practice, this is done via an MCP **tool call** over SSE (for example, calling a `generate_reply` method on the Cognition server). Other event types (like a “memory_insight” or “domain_result”) could be sent similarly if those features are enabled, but for the MVP we focus on user inputs.
- The service receives the event (through its `/messages` endpoint), immediately spawns an asynchronous task to handle it (so the HTTP request can return quickly), and decides what to do. In MVP, it will always run the LLM to produce an answer. (In the future, it might conditionally wait for more context or combine multiple events before responding.)
- **LLM Prompt Construction:** On a new input, the service prepares a prompt for the LLM. This typically involves collecting recent conversation context (prior messages) and the latest user query. For simplicity, our MVP implementation may use just the latest user message (and an optional static system instruction) as context. The architecture allows fetching more context from the Memory Service if needed – either by having the Core supply it or by the Cognition Service querying Memory via MCP. To keep concerns separated, the preferred approach is that the Cognition Service calls the Memory Service for history when needed, rather than the Core bundling context. However, to maintain **ruthless simplicity**, the initial version might skip this cross-service call and just work with minimal context. (We assume the conversation is short or that the core already ensured the memory service is updated with the new message.)
- **LLM Invocation:** The service then calls the configured LLM (OpenAI, Anthropic, etc.) via Pydantic-AI. This call is made asynchronously and can optionally stream partial results. In MVP, we can request a single-shot completion (the full reply) for simplicity. The Cognition Service is essentially acting as an **agent**: it formulates the prompt, sends it to the model, and will interpret the model’s output. No other component needs to know the details of this interaction.
- **Result Emission:** Once an assistant reply is generated, the service sends it back to the Core through the SSE connection. The FastMCP framework takes care of formatting this as a JSON-RPC response event to the Core’s client. The Core, upon receiving this, will publish an **output event** on its internal bus (allowing the frontend SSE stream to push the answer to the user). From the Core’s perspective, it invoked a tool on Cognition and got a result – it doesn’t know or care that an LLM was involved. The Cognition Service might also log the interaction and any usage metrics (tokens, latency) for debugging, but it does not store conversation state. After producing the reply, it awaits the next event.

**Future Interaction Patterns:** The above describes a straightforward request-response cycle. In future iterations, the Cognition Service may handle more complex patterns:

- **Deferred or Multi-step responses:** The service might receive a user question and decide to _not_ answer immediately, e.g. if it expects a “domain expert” service to provide additional info. It could hold off responding until another event arrives (like a `domain_result`). This would require the service to temporarily stash the context (or use the LLM’s function calling to automatically wait). Our design anticipates this by allowing events to be of different types and by not automatically responding to non-input events. For MVP, we implement a single-step response, but the event loop could later accommodate waiting or multiple events before producing output.
- **Tool use via LLM:** We plan to use Pydantic-AI to support structured outputs so the LLM can request tools. For example, we might instruct the LLM: _“If you need more information, respond with `{"tool": "...", "args": {...}}`.”_ The Cognition Service could parse the LLM output into a Pydantic model (e.g. `ToolRequest` vs `FinalAnswer`). If a tool is requested (say the LLM output indicates it wants to call `get_context`), the service can invoke the appropriate backend (Memory or a Domain Expert) via MCP, then feed the result back into the LLM for a refined answer. This essentially creates an internal loop of LLM -> tool -> LLM until a final answer is produced. While this is beyond the MVP scope, our implementation will be structured to allow adding this loop without major changes. Notably, Pydantic-AI’s agent system and message history support will be helpful here – we can maintain the conversation and tool usage context across multiple LLM calls in one session.
- **No Omission of Responses:** In some cases, Cognition might decide “do nothing” (e.g., if an input is just an acknowledgment or if another service will handle it). For MVP, we’ll **always respond** to demonstrate end-to-end functionality. The architecture, however, doesn’t require a response for every event – the service could simply not emit an event for certain inputs, and the core would handle that gracefully (perhaps timing out or sending no output). We document this behavior for clarity but do not implement it yet.

Throughout these flows, we stick to the principle of **clarity over cleverness**. The code will follow the logic in a linear, easy-to-follow sequence: receive event → gather context → call LLM → return answer. There will be no complicated state machines or multi-threaded trickery – just straightforward async code. Each piece of functionality is well-contained, making it easier to extend or modify independently. Next, we break down the components and their responsibilities in the implementation.

## Project Structure and Components

We implement the Cognition Service as a small Python package (or module) with a clear separation of concerns. The directory structure could be as follows:

```plaintext
cognition_service/
├── __init__.py
├── main.py              # Entry point: creates the FastMCP server and runs it
├── models.py            # Pydantic models for events and LLM outputs (if structured)
├── logic.py             # Core logic for handling events and interacting with the LLM
├── config.py            # Configuration (e.g., model selection, API keys via env)
└── memory_client.py     # (Optional) MCP client to Memory Service for fetching history
```

**Component Roles:**

- **`main.py` – Service Startup and Server Definition:** This module initializes the FastMCP server and registers the tools (RPC methods) that the service offers. We use the Model Context Protocol SDK’s `FastMCP` class to create an SSE-capable server instance. In `main.py`, we’ll create `mcp = FastMCP("CognitionService")` and decorate our handler function(s) with `@mcp.tool(...)`. Finally, it will call `mcp.run(...)` (when executed as **main**) to start the server. This sets up the HTTP routes (`/sse` for the event stream and `/messages` for incoming calls) under the hood. The startup is kept minimal – no special initialization is needed besides loading config (like setting the LLM model name from env). If needed, we could also include a simple health-check route or log a startup message, but the SSE endpoint itself can serve as a connectivity check.
- **`models.py` – Data Models and Schema:** Defines Pydantic models for the input and output data structures handled by the service. This includes an `InputEvent` model representing context-change events the service handles. For example, `InputEvent` might have fields like `type: Literal["user_input", "memory_insight", ...]`, `user_id: str`, `conversation_id: str`, and `content: str` (for user messages). In practice, the MCP tool call parameters serve a similar role – we may not explicitly instantiate an `InputEvent` object if FastMCP maps JSON to function arguments, but having a model helps for validation and documentation. We also define any LLM output schemas here if using structured output. For MVP, the output is just a plain answer string (no complex schema). But we anticipate models like:
  - `ToolRequest` (with fields `tool: str` and `args: dict`) to represent an LLM asking to use a tool.
  - `FinalAnswer` (with field `answer: str`) to represent a final answer.
    These can be used with pydantic-ai to validate and parse the LLM’s response. Initially, we might not actively use these models in code, but they document the expected format and allow easy extension of logic later (for example, after getting the LLM reply, do `try: ToolRequest.parse_raw(output)`).
- **`logic.py` – Cognition Logic Handler:** Contains the core function that implements the decision-making for events. For instance, a function `async def handle_event(event: InputEvent) -> str` (or similar) that encapsulates: checking the event type, gathering necessary context, and generating a response if appropriate. In MVP, this will essentially always call the LLM for `"user_input"` events and return the model’s answer. We separate this logic from the FastMCP interface so it can be unit-tested easily. `handle_event` can be called directly in tests with a constructed `InputEvent` (or just parameters), bypassing the network layer. This function will use the LLM agent from `pydantic_ai` (configured via `config.py`) to get a completion. If the event type is not one the service handles (e.g. an unexpected type), it can simply log and return None or an empty string (causing no output event). This module could also contain helper functions, like `get_memory_context(user_id, conv_id)` if we integrate with the Memory Service, and error handling utilities for the LLM call.
- **`config.py` – Configuration Management:** Defines configuration constants and possibly a Pydantic `BaseSettings` class for environment variables. This is where we specify the LLM model/provider to use and load API keys securely. For example, `LLM_MODEL = os.getenv("LLM_MODEL", "openai:gpt-3.5-turbo")` and `SYSTEM_PROMPT = os.getenv("SYSTEM_PROMPT", "You are Cortex, a helpful AI assistant.")`. We rely on standard env vars like `OPENAI_API_KEY` and `ANTHROPIC_API_KEY` being set in the environment for the respective providers – the Pydantic-AI library will pick those up automatically (we do not hardcode any credentials in code). The config might also include toggles like `ENABLE_MEMORY_FETCH = False` for MVP (to easily switch on/off cross-service context retrieval).
- **`memory_client.py` – Memory Service Integration (Optional):** In MVP, we can stub this or omit it. It would be used if the Cognition Service needs to fetch conversation history or other info from the Memory Service via MCP. For example, it could use the official MCP client to call Memory’s `get_history` tool. If implemented, it might use the same `mcp.client.sse` utilities the Core uses. However, to reduce complexity, our initial version might not call out at all. We include this module as a placeholder with a clear interface (e.g., `async def fetch_recent_history(user_id, conversation_id) -> List[Message]`) so that future development can fill it in. This keeps the Cognition logic **decoupled** – it will call `memory_client.fetch_recent_history` if available, and that function can decide whether to actually call the memory service or return dummy data for now.

Additionally, we will have a `tests/` directory (if outside the package) or test modules to validate the service. For example, `tests/test_cognition_logic.py` to unit test `logic.handle_event` behavior, and maybe `tests/test_server_integration.py` to test that the FastMCP server responds correctly given a sample input (this could be done with an MCP client or by simulating a JSON-RPC call via HTTP).

## Internal APIs and Interfaces

**MCP Tool Interface:** The Cognition Service exposes its functionality to the Core via MCP **tools**. In FastMCP, each tool is essentially an RPC method. We will define one primary tool for MVP, e.g. `generate_reply`. This will handle new user messages. Its signature could be:

```python
@mcp.tool()
async def generate_reply(user_id: str, conversation_id: str, content: str) -> str:
    """Generate an assistant reply for a new user message."""
    ... (calls into logic and returns reply text) ...
```

When the Core’s MCP client calls this tool (with JSON params `{"user_id": ..., "conversation_id": ..., "content": ...}`), the service will execute the function and stream back the result. Under the hood, FastMCP will send a JSON-RPC response over SSE containing either a `"result"` with the returned string or an `"error"` if an exception was raised. We don’t need to manually format the SSE messages – the framework handles that. We just return the reply string (or raise if something went wrong).

We may also register other tools if needed. For example, if we wanted the Core to be able to explicitly request a context summary, we could offer a `get_context(user_id, conversation_id) -> str` tool (similar to what was envisioned in earlier design. For now, we keep it simple: one tool to process inputs. We name it clearly (`generate_reply`) to reflect its purpose. (The name will appear in logs and in any tool listings on the client side.)

**Event Data Model:** The input to `generate_reply` is effectively our `InputEvent`. FastMCP will automatically validate basic types (matching the function annotations). We still document the expected structure:

- `user_id` (str): The user’s unique ID (e.g. Auth0 sub) – used for multi-user context isolation.
- `conversation_id` (str, optional): The conversation/thread ID. In MVP, the system might use a default if not provided. We include it to route to the correct context if needed. The service can simply pass this along to memory fetch calls or include it in logs.
- `content` (str): The text of the user’s message. This is the primary input the LLM sees. In some cases, if this event were from a different source (like a memory insight), `content` might hold a summary or info instead – but then the tool likely would be different. For `generate_reply`, we expect this to be a user message.

We do not explicitly pass a full message history or other context in this call. If the LLM needs more context, the service itself will retrieve it (or the LLM will ask via tool invocation in future designs). This keeps the interface lightweight. The **separation of concerns** is such that the Core doesn’t assemble prompts – it just forwards the trigger and relevant IDs.

**Output and Response Model:** The service’s output is a string (the assistant’s reply) for now. In the future, if we implement the structured output checking, the logic might return a `FinalAnswer` model instance or raise a special exception to indicate no reply. But at the MCP layer, everything is translated to JSON. A successful call to `generate_reply` will result in the Core receiving a JSON-RPC result, e.g.:

```json
{ "id": 42, "result": "Sure, I can help with that." }
```

The Core’s MCP client will translate this into an internal event (like an `output` event with that content) that eventually gets streamed out to the user via SSE. We don’t include metadata like the user_id in the result because the core already knows which conversation this corresponds to (it tracks the `id` and routing). If we wanted to be explicit, we could return a structured dict like `{"reply": "...", "conversation_id": ...}`, but that would duplicate what the core already manages. So we keep it direct.

For error cases, if our `generate_reply` function raises an exception (or returns an `MCPError`), the MCP framework will send an `"error"` response. We will handle errors inside the function and prefer to return a friendly message rather than propagate an exception. For example, if the LLM API fails or times out, `generate_reply` can catch that and return a fallback string like `"[Sorry, I couldn’t generate a response]"`. That way, the user still gets a message via SSE, and the conversation doesn’t break. We also log the error for debugging. This approach aligns with the idea that errors in downstream processing are delivered asynchronously and not as HTTP 500s to the initial request.

**LLM Agent Interface:** Inside our logic, we use Pydantic-AI’s **Agent** to interact with the LLM. We configure the agent with:

- A model name (including provider prefix), e.g. `"openai:gpt-3.5-turbo"` or `"anthropic:claude-v1"`. This could come from `config.py`. The agent will pick up the appropriate API key from the environment (e.g., `OPENAI_API_KEY` or `ANTHROPIC_API_KEY`). We do not hardcode any keys – the deployment must set them.
- A result type, likely `str` for a plain answer. Pydantic-AI will wrap the result in an `AgentRunResult[str]` object when we run the agent.
- (Optionally) a system prompt. We can provide a default system instruction like “You are a helpful assistant...” from config. This helps steer the model’s behavior but remains simple. If not provided, we can run without a system prompt for now.
- (Optionally) tools/functions – in a more advanced scenario, we could register functions (like memory lookup) as tools that the agent’s LLM can call. For MVP, we will not register any function tools directly in the agent; the orchestration of tools is handled outside the LLM (the LLM just gives an answer). This keeps the initial run simple and avoids needing to intercept function calls. We do, however, design the code such that adding tool functions is straightforward later.

Using the agent looks like:

```python
from pydantic_ai import Agent
from pydantic_ai.models.test import TestModel  # for testing

# Initialize agent (at module load or in handler)
LLM_MODEL = os.getenv("LLM_MODEL", "openai:gpt-3.5-turbo")
agent = Agent(LLM_MODEL, result_type=str, system_prompt=SYSTEM_PROMPT)

async def generate_reply_via_llm(messages: list[dict]) -> str:
    """
    Call the LLM agent with a list of messages (chat history) and return the assistant's reply text.
    """
    result = await agent.arun(message_history=messages)  # asynchronous run, returns AgentRunResult[str]
    reply_text: str = result.data  # Extract the assistant's reply from result
    return reply_text
```

In our case, `messages` will typically consist of just one user message and maybe a system message. For example, we might construct:

```python
messages = []
if SYSTEM_PROMPT:
    messages.append({"role": "system", "content": SYSTEM_PROMPT})
messages.append({"role": "user", "content": user_input})
```

This is a minimal chat history given to the model. If in future we retrieve past conversation turns from memory, we would prepend those as additional messages (in order). Pydantic-AI expects a `message_history` as input for conversation context; it will handle formatting for the provider’s API. After `agent.arun`, we get an `AgentRunResult` object which contains the final answer in `result.data`. It also holds the `message_history` (all messages including the assistant’s answer) and usage info (tokens, etc.) if we need them. We could log `result.usage` or inspect `result.message_history` for debugging. If the model’s response was supposed to be structured (e.g., match `FinalAnswer`), we could do `result.value` which would be a Pydantic model instance. But since we used `result_type=str`, `result.data` is just the string.

We **do not maintain a long-lived message history in memory** inside this service for each conversation – that remains the responsibility of the Memory Service. If continuity is needed, the core or cognition can fetch history each time. Alternatively, we could keep an `Agent` per conversation and supply `result.new_messages()` from the last run to the next run, effectively accumulating context. However, that introduces state in the service (we’d need to store these across events). To align with stateless design and avoid memory buildup, we opt to reconstruct context on each call (using memory service when necessary). This makes each `generate_reply` call independent and easier to scale (the service could handle many conversations without storing each one’s history in RAM). It also simplifies testing – we can test one call in isolation.

**Error Handling and Edge Cases:** Within the logic, we’ll add basic error handling:

- If the LLM call raises an exception (network error, API error, etc.), we catch it. We can log the error (with context like user*id or a snippet of the query) for debugging. Then we return a safe fallback string, e.g. *“Sorry, I’m having trouble responding right now.”\_ as the result. This ensures the Core still receives a `"result"` (so it won’t propagate an error to the user SSE). The user will see an apology or error message as the assistant’s reply, rather than nothing. This keeps the UX acceptable even if the AI fails, and it prevents one bad event from crashing the service’s loop.
- If the input content is empty or not meaningful, the LLM may return an empty or generic answer. That’s fine – we don’t add special rules for that in MVP. All content is treated uniformly.
- We guard against extremely long inputs or too many tokens by relying on the LLM provider’s limitations and the possibility to truncate history. Since we aren’t pulling a lengthy history yet, we likely won’t hit token limits for single input. In future, if memory returns a lot of text, we would truncate to last N messages as needed. We document this potential but it’s not implemented now.
- Concurrency: FastAPI/FastMCP will handle each incoming event in its own task. We must ensure our code is thread-safe. Our usage of the Agent (which uses async under the hood) is safe to call concurrently for different events. We don’t use global mutable state except the agent object, which manages its own state per `.run` call. Pydantic-AI allows reusing an Agent for multiple runs; that should be fine as long as we await each properly. If needed, we could instantiate a new Agent inside the handler to avoid any shared state – the overhead is low. But reusing is convenient for re-using the model config.
- The service does not implement authentication or authorization checks on incoming events – we assume the Core only sends legitimate events (it will have already verified the user’s identity). The Cognition Service trusts the core, which is consistent with our zero-legacy, minimal security model in MVP (in a real system, one might secure the MCP channel with tokens or restrict allowed user_ids, but that’s beyond our current scope).

In summary, the internal interface is a clean async function call: an input comes in, we return an output string. Everything is strongly typed with Pydantic models and function signatures, making the code self-documenting and less error-prone. Next, we’ll outline how the service runs and how it ties into the MCP framework at startup.

## Service Startup and Execution

The Cognition Service runs as a standalone process (for example, `python cognition_service/main.py`). The `main.py` sets up the FastAPI/Starlette app via FastMCP. We use the **FastMCP SDK** to simplify SSE setup:

1. **Initialize FastMCP:** We create an instance with a service name. e.g. `mcp = FastMCP("CognitionService")`. This name is mostly for identification/logging. FastMCP will create a Starlette app under the hood.
2. **Register Tools:** We import our handler function from `logic.py` and decorate it. For example:
   ```python
   from cognition_service.logic import handle_event
   @mcp.tool()
   async def generate_reply(user_id: str, conversation_id: str, content: str) -> str:
       return await handle_event(user_id=user_id, conversation_id=conversation_id, content=content)
   ```
   We use `handle_event` inside to keep this function thin. (Or we could call the LLM logic directly here; either way is fine. The key is that the heavy logic is not mixed with this interface code.) We add a docstring to the tool function to describe it; FastMCP might expose that to clients. We might also register a second tool for a health check or debugging (not strictly needed since FastAPI’s root can serve that). For example, an `echo` tool could be added during development to test connectivity.
3. **Mount SSE App (if needed):** If we weren’t using FastMCP’s CLI, we could mount the Starlette SSE routes into a FastAPI app manually. But FastMCP’s `mcp.run()` likely handles this for us. According to the FastMCP documentation, simply calling `mcp.run()` when the script is executed will start an ASGI server serving the MCP endpoints. We can optionally specify a host/port. For clarity in MVP, we’ll pick a fixed port for the Cognition Service (e.g., 9100). This must match what the Core is configured to connect to. So in `mcp.run()` we might do `mcp.run(host="0.0.0.0", port=9100)`. If `FastMCP.run` doesn’t accept parameters (some versions might not), an alternative is to create a FastAPI `app`, mount `mcp` into it, and run Uvicorn. But that’s more steps. We choose the simplest path: use the FastMCP built-in runner or CLI. (Our docs will clearly state how to run it.)
4. **Run Indefinitely:** Once started, the service will wait for incoming connections. The Core (or a developer with an MCP client) will connect to the `/sse` endpoint to establish the event stream. FastMCP will output a log like “endpoint: /messages/?session_id=... ” to confirm the SSE channel is ready. When Core wants to send a message, it will POST to that `/messages` endpoint. FastMCP then invokes our `generate_reply` function with the JSON payload. Our code processes and returns the result, which FastMCP then sends back over the SSE stream to the Core. This loop continues for each input. We don’t need an explicit while-loop in our code – the server’s event loop and FastAPI’s request handling take care of continuously listening and dispatching calls.
5. **Shutdown:** On service shutdown (SIGINT or process exit), FastMCP will close the SSE connections. There isn’t much state to clean up in our case. If we had open connections to Memory or other resources, we’d close them here. We could use `@app.on_event("shutdown")` if we went the FastAPI route, or just rely on process termination. Because the service is stateless and uses mostly external async calls, shutting down is straightforward.

**MCP SDK Integration:** We ensure that we use the **official MCP Python SDK** as intended, without writing custom networking code. This aligns with our “no reinventing the wheel” policy. The FastMCP library handles SSE details – such as sending periodic pings, managing session IDs, etc., as evidenced by the example that prints `: ping` comments in the SSE stream. By using `FastMCP("CognitionService")` and its `tool` decorators, we get a fully functional MCP server that the Core can interface with uniformly. The Core’s side will use the MCP client to connect to `http://cognition-service-host:9100/sse` and invoke `generate_reply` via JSON-RPC. Because we stick to the expected patterns (one transport, SSE, and standard JSON-RPC messages), there’s no special-case code needed in the core for this service – it will treat it like any other MCP tool provider.

**Tool and Event Routing:** The Cognition Service doesn’t subscribe to the core’s in-memory event bus directly (that bus exists only inside core). Instead, the core acts as a bridge: it translates its internal events into MCP calls. For example, when the core’s Input endpoint publishes a `user_input` event, the orchestrator (which could now be just a lightweight task) uses the Cognition client to call `generate_reply`. We thus maintain **loose coupling** – the Cognition Service doesn’t know about the event bus or any core internals, it only knows about the MCP call it receives. This design makes it possible to run the service separately (even on another machine or container) and to test it by mimicking MCP calls easily.

One aspect to note is that after sending the reply, the core should store the assistant’s answer in the Memory Service (so the conversation history is complete). Our Cognition Service will not do that itself in MVP (it doesn’t automatically call Memory to save the answer). We assume the Core, upon receiving the reply, handles persistence (the Core’s orchestrator could call `memory_client.store_message` with the assistant’s response). We mention this to clarify responsibilities: Cognition generates content; Memory stores all messages. This separation reinforces domain isolation (Cognition isn’t modifying long-term state, just computing outputs).

## Testing the Cognition Service

To ensure the Cognition Service works correctly and adheres to the contract, we will create a few tests. Because the service is meant to run independently, we can test its logic without running the entire Core. Key testing strategies:

1. **Unit Test the Logic Handler:** We can call the core logic function directly with a simulated input. For example, instantiate an `InputEvent(user_id="U123", conversation_id="C1", content="Hello")` and call `await handle_event(event)`. We expect a string response. Since this will invoke the LLM agent, we don’t want our unit tests making actual API calls. Pydantic-AI provides a `TestModel` for this purpose. We can override the agent’s model to use `TestModel`, which will short-circuit the LLM call and return a dummy result. For instance:

   ```python
   from cognition_service.logic import agent
   from pydantic_ai.models.test import TestModel

   @pytest.mark.asyncio
   async def test_handle_event_basic():
       # Override the agent's model to TestModel for a deterministic response
       test_model = TestModel()
       with agent.override(model=test_model):
           reply = await handle_event(user_id="test", conversation_id="test", content="Hello")
       assert isinstance(reply, str)
       # The TestModel by default returns 'success (no tool calls)' if no tools were invoked
       assert "success" in reply.lower() or reply != ""
   ```

   In this test, we check that we got a string reply (not an error or None). With `TestModel`, the `reply` might equal `"success (no tool calls)"` (which is a placeholder text indicating the LLM “succeeded” without needing tools). We just verify that some reply is produced. We can also verify that no exceptions were raised and the logic path was executed. This test ensures that our handler can run end-to-end in isolation.

2. **Simulate an LLM Tool Request (if implemented):** If we added logic to parse tool requests, we would test that separately. For example, craft a fake LLM output like `'{"tool": "get_context", "args": {"query": "weather"}}'` and see that our logic detects it and does not return a final answer immediately. In MVP, since we’re not doing that loop yet, we skip this. But we can include a placeholder test that ensures our Pydantic models parse correctly:
   ```python
   def test_tool_request_model():
       output = '{"tool": "get_context", "args": {"query": "Paris"}}'
       req = ToolRequest.parse_raw(output)
       assert req.tool == "get_context"
       assert "query" in req.args
   ```
   This just validates the model setup.
3. **Integration Test via MCP Client:** For a higher-level test, we can actually run the service (maybe in a thread or test server) and use an MCP client to call it. The MCP SDK could be used in tests to mimic the core. For example, using `mcp.client.sse.sse_client` to connect to the local service’s SSE endpoint, then sending a JSON-RPC message. However, this is relatively heavy to do in an automated test. As an alternative, we could use HTTPX to POST directly to the `/messages` endpoint. The tricky part is obtaining a valid `session_id` or ensuring the server accepts the message. In dev, one could manually test:

   ```
   uvicorn cognition_service.main: mcp --port 9100
   ```

   Then use a tool like **Cursor** or a cURL command to send a request. For example, after connecting to `/sse` (which Cursor does automatically), do:

   ```bash
   curl -X POST http://localhost:9100/messages -d '{"id":1,"method":"generate_reply","params":{"user_id":"U1","conversation_id":"C1","content":"Hi"}}' -H "Content-Type: application/json"
   ```

   One should see an SSE event from the server with the answer. In our tests, instead of full integration, we rely on the fact that FastMCP’s internal workings are tested by its library. We focus on our logic’s correctness and assume that if our `generate_reply` returns a string, the MCP framework will deliver it properly.

   We can assert some integration behavior indirectly: for instance, if we run the app in test mode and call the `generate_reply` function via FastAPI’s TestClient (mounting the app as described in docs), but since FastMCP might not easily expose a sync route, this might not be trivial. For MVP, a manual test or integration test outside automated suite might suffice.

4. **Concurrent Requests Test:** We could simulate two `handle_event` calls concurrently with different inputs (using `asyncio.gather`) to ensure no shared state issues. If our implementation is stateless except for the `agent` (which might accumulate history if reused improperly), this test can catch if replies bleed over. In practice:

   ```python
   async def test_concurrent_requests():
       inputs = [("U1","C1","Hello"), ("U2","C2","Hi there")]
       test_model = TestModel()
       with agent.override(model=test_model):
           results = await asyncio.gather(*[handle_event(user, conv, text) for user, conv, text in inputs])
       assert all(isinstance(r, str) for r in results)
       # Each should succeed independently (TestModel returns same success message for both)
       assert results[0] == results[1] == "success (no tool calls)"
   ```

   This would ensure our agent usage can handle parallel runs (which it should, because each `handle_event` awaits its own `agent.run`). If there were any internal race conditions or global data issues, this test might reveal it (in our simple case, likely fine).

5. **Edge Case Tests:** Test how the service responds to an extremely short input (empty string or just whitespace) and an extremely long input (beyond typical context length). For an empty input, perhaps our LLM will return an empty or generic answer. We just ensure no error. For a long input, we might simulate splitting or truncation if we implement it. In MVP we rely on the model’s handling. We can send a large dummy text to `handle_event` (with `TestModel` which might just return success anyway, ignoring content length).

By covering these test cases, we gain confidence that:

- The service logic is correct and yields expected results for normal inputs.
- The structured output parsing (if any) works for known patterns.
- The system can handle multiple calls and error scenarios gracefully.
- The integration points (MCP and LLM agent) are configured correctly (since using `TestModel` bypasses actual LLM, we assume Pydantic-AI’s integration is fine – we might do one real call test with a live key as a smoke test if possible).

Finally, from a **design resilience** perspective, this implementation is **clean and minimal**. There are very few moving parts: essentially one function doing the work, one external library managing the protocol, and one for the AI. This simplicity means fewer places for bugs to hide. The strict separation (Core doesn’t need to know how Cognition generates answers, and Cognition doesn’t handle anything but AI logic) follows our **single responsibility** principle. If we need to change the LLM provider or add tool logic later, we can do so in this service without touching the Core. If we want to scale out cognition (run multiple instances, or move it to a bigger machine), it’s easy because it’s a self-contained service. We avoided any premature optimization or abstraction – e.g., we didn’t introduce a generic “EventProcessor” class or a complex state machine, since the straightforward approach was sufficient (and easier to read/maintain).

## Conclusion and Next Steps

By following this guide, you will have a working **Cognition Service** that fits neatly into the Cortex architecture. It will accept new user inputs via MCP/SSE and respond with AI-generated answers, all while remaining decoupled from the core logic and other services. The design emphasizes **directness** (using libraries as-is, with minimal glue code) and **clarity** (each part of the code has a clear purpose, and we’ve documented the internal interfaces thoroughly). This aligns with Cortex’s philosophy that simpler is better – we implemented the essential features needed for the assistant to converse, and nothing more.

Going forward, this service can be extended with additional capabilities:

- Integrating actual conversation history from the Memory Service to provide better answers.
- Implementing the tool-use loop to allow the LLM to query Memory or a Domain Expert mid-response.
- Adding streaming token-by-token replies (since SSE would allow us to send partial data) for a more responsive UI.
- Enhancing error handling and logging (e.g., log timing and token counts for each response for monitoring).

Each of those can be added without breaking the existing structure – the service is built to be **easily evolvable**. But as per our guiding principles, we have started with the simplest possible implementation that is still functional. This ensures a solid, easy-to-understand foundation on which future improvements can be made with confidence.

With the Cognition Service in place, the Cortex Core can now delegate all heavy LLM work to it. This keeps Core lean and focused on orchestration, and allows the LLM integration to be developed and tuned in isolation. Developers can run the Cognition Service independently (for example, replacing the LLM with a stub or a different model for experiments) without affecting the rest of the system. This modularity and clarity of design will help Cortex scale its capabilities without accumulating unnecessary complexity or technical debt.
