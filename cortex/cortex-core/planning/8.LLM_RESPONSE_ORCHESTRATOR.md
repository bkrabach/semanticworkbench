# LLM Response Orchestrator – Implementation & Guide

## Overview and Philosophy

The **LLM Response Orchestrator** (sometimes called the Response Handler) is the core background component in Cortex Core that listens for user input events and produces the AI assistant’s responses. It runs as an asynchronous task, decoupled from request/response threads, and coordinates between the Event Bus, the Large Language Model (LLM), and any tool services (Memory, Cognition) needed to answer user queries. This component is implemented with a _ruthless focus on simplicity_ and direct integration with external libraries, following the project’s **Implementation Philosophy** (KISS, minimal abstractions) and **AI Assistant Guide** best practices (clarity, isolation of concerns). The orchestrator’s logic is linear and easy to follow: it subscribes to incoming messages, decides how to handle them (possibly calling a tool or directly answering), and then publishes the result. There are no unnecessary layers of abstraction – it uses the Event Bus and LLM SDKs directly, and each step is clearly separated to reduce complexity.

## Responsibilities and Design Principles

- **Event-Driven Processing:** The orchestrator subscribes to the EventBus for `"input"` events. When a new user message event arrives, it triggers the orchestrator to generate a response. This decoupling via an event system keeps the API layer simple (just publishing an event) and the response generation asynchronous.
- **Background Async Task:** Runs in a continuous loop as a FastAPI startup task. It must run independently of HTTP requests, using `asyncio` to avoid blocking. The orchestrator will fetch events from the EventBus one at a time and handle them sequentially, ensuring thread-safety and simplicity.
- **LLM Integration (OpenAI/Anthropic):** Uses the `pydantic-ai` framework to call an LLM (OpenAI GPT or Anthropic Claude) directly, without extra wrappers. The model provider and model name are configurable via settings or environment variables, enabling easy switching between OpenAI and Anthropic. The orchestrator defines a clean coroutine interface, e.g. `async def call_llm(messages: List[dict]) -> AsyncGenerator[str, None]`, to send chat messages to the LLM and yield the response (potentially streaming partial chunks). Internally, this function will choose the appropriate model client based on config.
- **Structured Output Parsing:** The LLM is prompted to produce structured output that can be validated using Pydantic models. The orchestrator defines two output schemas – for example, a `ToolRequest` model (with fields like `tool: str` and `args: dict`) and a `FinalAnswer` model (with field `answer: str`). Every LLM response is attempted to be parsed into these models. This allows the orchestrator to **detect when the LLM is requesting a tool** versus when it has provided a final answer. Using `pydantic-ai`, we can enforce these structures and catch invalid outputs.
- **Tool Invocation Loop:** If the LLM response indicates a tool should be used (e.g. it returns `{"tool": "memory", "args": {...}}`), the orchestrator will call the corresponding Cortex microservice client (Memory or Cognition) to fetch the needed data. It then feeds that data back into the LLM in a follow-up prompt to get a refined answer. This loop is kept extremely simple – at most one or two iterations (to avoid an endless agent loop). We cap the tool usage depth to **2 cycles max** (initial query + one tool invocation) for predictability. If no tool is requested, or after fulfilling one tool request, the loop ends with a final answer.
- **Event Publishing:** Once a final answer is ready (or an error has occurred), the orchestrator publishes an `"output"` event on the EventBus with the result. This event will be picked up by any SSE streaming endpoints and delivered to the client. The output event contains minimal data: identifiers (user and conversation IDs) and the answer content (or error info). If streaming, the orchestrator can publish intermediate partial answer events as well, to allow realtime token streaming in the client.
- **Lifecycle Management:** The orchestrator is started automatically when the FastAPI app starts (registered via `@app.on_event("startup")` or similar). It should also respond to shutdown signals – for example, by breaking out of its loop or unsubscribing from the EventBus to allow a clean exit. Proper cleanup ensures no orphan tasks keep running when the application stops.

## Implementation Step-by-Step

### 1. Launching the Orchestrator Task (EventBus Subscription)

First, set up the orchestrator as a background task in the app’s startup. In your FastAPI `main.py` (or app factory), create an async task that runs the response handler loop. For example:

```python
@app.on_event("startup")
async def startup_event():
    # Launch the orchestrator loop as a background task
    asyncio.create_task(run_response_orchestrator())
```

The `run_response_orchestrator()` coroutine will subscribe to the EventBus and continuously listen for new input events. We assume there is a global `event_bus` instance providing a `subscribe` method that returns an asyncio queue or channel for events. Here’s a simplified structure of the orchestrator loop:

```python
async def run_response_orchestrator():
    # Subscribe to all input events
    event_queue = event_bus.subscribe(event_type="input")
    while True:
        event = await event_queue.get()       # Wait for next input event
        try:
            await handle_input_event(event)   # Process the event and generate output
        except Exception as e:
            logger.error(f"Error processing event: {e}", exc_info=True)
            # Optionally publish an error output event so the user is informed
            error_event = {"type": "output", "error": True, "message": "Failed to process input."}
            event_bus.publish(error_event)
            # Continue loop even if one event fails
            continue
```

In this loop, we fetch each event and pass it to a handler. Wrapping the processing in a try/except ensures that if one event handling raises an exception, it’s caught and logged, and the loop continues to the next event instead of crashing. If an error occurs, you might also publish an `"output"` event with an error flag or message, so the client isn’t left waiting indefinitely. The loop runs forever, so ensure it is started as a background task (so it doesn’t block startup) and consider using an asyncio Event or cancellation to break out on shutdown.

**Shutdown Handling:** To cleanly stop the orchestrator, you can listen for FastAPI’s shutdown event and cancel the task. For instance, keep a reference to the `asyncio.Task` returned by `create_task` and call `.cancel()` on it during shutdown. Alternatively, set a global flag that the loop checks (e.g., break if `event is None` or if `stopped` flag is set via an event). The EventBus itself could provide a way to unsubscribe or close the queue to unblock the `get()`. Ensuring the loop ends will allow the app to shut down gracefully.

### 2. Configuring LLM Provider and Model

To integrate with an LLM, we need to configure which provider (OpenAI or Anthropic) and model to use. This should be made configurable via environment variables or settings. For example, you might have:

- `LLM_PROVIDER` (e.g., `"openai"` or `"anthropic"`)
- `OPENAI_API_KEY` and model name (e.g., `"gpt-3.5-turbo"`)
- `ANTHROPIC_API_KEY` and model (e.g., `"claude-v1"`)

Using Pydantic’s BaseSettings is an elegant way to manage these. Define a settings class:

```python
from pydantic import BaseSettings

class LLMSettings(BaseSettings):
    llm_provider: str = "openai"
    openai_model: str = "gpt-3.5-turbo"
    anthropic_model: str = "claude-v1"
    openai_api_key: str | None = None
    anthropic_api_key: str | None = None

settings = LLMSettings()
```

Make sure to populate the API keys in your environment for the chosen provider. The orchestrator will read these settings on startup to decide which client to use. This design allows switching LLM backends without code changes – just change the env vars or config.

### 3. LLM Call Interface (`call_llm`)

We define a clean interface for sending messages to the LLM and receiving its response. The signature `async def call_llm(messages: List[dict]) -> AsyncGenerator[str, None]` is recommended. Here, `messages` is a list of chat messages (dictionaries with at least `"role"` and `"content"`, similar to OpenAI’s ChatCompletion format). The coroutine returns an async generator yielding strings – which could be chunks of a streaming response or a final full response.

Inside `call_llm`, use the appropriate SDK or `pydantic-ai` model to send the request:

- **Using PydanticAI:** This library provides a unified interface for multiple LLM providers. For example, it has model classes for OpenAI and Anthropic. You can initialize the model at startup based on the provider, then reuse it for calls.

  ```python
  from pydantic_ai.models.openai import OpenAIModel
  from pydantic_ai.models.anthropic import AnthropicModel

  if settings.llm_provider == "openai":
      llm_model = OpenAIModel(model_name=settings.openai_model, api_key=settings.openai_api_key)
  elif settings.llm_provider == "anthropic":
      llm_model = AnthropicModel(model_name=settings.anthropic_model, api_key=settings.anthropic_api_key)
  else:
      raise ValueError("Unsupported LLM provider")
  ```

  Now, to call the model, format the `messages` as needed (OpenAI expects roles like `"system"`, `"user"`, `"assistant"`, whereas Anthropic expects prompt strings with certain prefixes). PydanticAI’s model classes likely accept a list of messages or a combined prompt. For simplicity, you can convert a list of `{role, content}` messages into a single prompt string if needed. Use the model’s `run` or `agenerate` method. For example:

  ```python
  async def call_llm(messages: List[dict]):
      # The pydantic-ai model might handle the chat format internally
      async for chunk in llm_model.agenerate(messages):
          yield chunk
  ```

  The `.agenerate` (or similar) method yields chunks of the response as they arrive (for streaming). If streaming is not needed, you could use a synchronous call that returns the full result and then yield it once. Ensure to catch exceptions from the LLM call (network errors, API errors) and handle them (possibly propagate up as errors to be caught in the main loop).

- **Direct SDK Alternative:** If not using PydanticAI, directly integrate the provider’s SDK. For OpenAI, you’d use `openai.ChatCompletion.create` with `stream=True` for streaming. For Anthropic, use their SDK for Claude (Anthropic provides a similar client where you send a prompt and get a response or stream). Direct usage might look like:

  ```python
  import openai

  async def call_llm(messages: List[dict]):
      response = await openai.ChatCompletion.acreate(
          model=settings.openai_model,
          messages=messages,
          stream=True
      )
      # response is an async generator of OpenAI chunks
      async for chunk in response:
          if chunk.choices[0].delta.get("content"):
              yield chunk.choices[0].delta["content"]
  ```

  This example streams partial `content` as they come. Anthropic’s API (via their `anthropic` Python SDK) has a similar interface with a streaming option. No matter which method, keep this function thin – it should primarily pass through the call to the external API and yield results.

By isolating all LLM API interaction in `call_llm()`, we keep the orchestrator logic clean. This function can be easily swapped or mocked for testing (e.g., replaced with a fake that yields a preset answer). It aligns with **direct integration** philosophy: no extra abstraction layer on top of the SDK or PydanticAI – just use it as intended.

### 4. Structured Response Models (Pydantic Validation)

To enable the orchestrator to interpret the LLM’s intent (whether it’s giving an answer or requesting a tool), we define structured output schemas. Using Pydantic, create models for each possible response type. For example:

```python
from pydantic import BaseModel

class ToolRequest(BaseModel):
    tool: str
    args: dict

class FinalAnswer(BaseModel):
    answer: str
```

These models represent mutually exclusive outcomes from the LLM. A `ToolRequest` might look like `{"tool": "memory", "args": {"query": "latest messages"}}`, indicating the LLM wants to use the Memory service (perhaps to fetch conversation history). A `FinalAnswer` simply has the answer text.

**Instructing the LLM:** To leverage these models, we guide the LLM to output in a controlled JSON format. Typically, this is done by including a system message or prompt that says: _“If you need additional data, respond with a JSON like `{"tool": "...", "args": {...}}`. If you can answer directly, respond with `{"answer": "text here"}` only.”_ By clearly specifying the format, we increase the chance the LLM complies. We then parse the LLM’s raw response string using the Pydantic models: first attempt to parse `ToolRequest`, and if that fails (validation error), attempt `FinalAnswer`. One of them should succeed if the LLM followed the format.

With `pydantic-ai`, you might not need to manually parse – the library can integrate the model into the LLM call. For example, some frameworks allow you to specify an output schema and will automatically parse the result into that model. If using such a feature, the orchestrator can directly get a `ToolRequest` or `FinalAnswer` object from the LLM call. However, be prepared to handle cases where the LLM returns something unexpected (e.g., not valid JSON). In those cases, our orchestrator can default to treating the response as a final answer text (or as an error). The guiding principle is robustness: use the structured model when possible, but don’t let parsing failures break the conversation flow entirely.

### 5. Handling an Input Event (Processing Logic)

Now we tie it all together in the `handle_input_event(event)` function (as used in the loop above). This is the core logic executed for each incoming message event. We outline it in steps:

1. **Extract Event Data:** The event is likely a dict or object containing details like `user_id`, `conversation_id`, and the user’s message content. (Before publishing the input event, the API route or client probably already stored the message in the Memory service, so the event may not need the full text if we plan to fetch from memory. But for simplicity, assume the event includes the raw message too). Validate that the necessary fields are present. Because we control the publisher (the `/input` endpoint), we ensure it always sends a consistent payload – e.g., `{"type": "input", "user_id": "...", "conversation_id": "...", "content": "Hello"}`. No heavy validation is needed beyond maybe confirming types.

2. **Optional: Retrieve Context Proactively:** There are two strategies to provide conversation context to the LLM:

   - **Proactive context fetching:** Immediately call the Memory service to get recent conversation history, and perhaps call Cognition service for a summary or other derived context. This gives the LLM all needed info up front. For example:
     ```python
     history = await memory_client.get_history(user_id, conversation_id)
     summary = await cognition_client.get_summary(user_id, conversation_id)
     ```
     Then construct the prompt messages: a system message like “You are an assistant. Here is the conversation so far: [history]”, and a user message with the latest question. This approach ensures the LLM has context, but it means we call the external services every time, even if not needed.
   - **LLM-driven tool use (structured approach):** Start by only giving the LLM the user’s query (and maybe minimal instructions) and let it decide if it needs more information. Using the structured output schema, the LLM might respond with a `ToolRequest` asking for conversation history or other info. This approach can save unnecessary calls – the Memory is only queried if the AI specifically asks. It’s a bit more complex but aligns with an **agent-like** pattern.

   For the MVP and clarity, it’s fine to choose one approach. The structured tool loop is more novel, so we’ll illustrate that path further below. In practice, you could also combine them: e.g., always include the last N messages as context, but allow tool requests for anything additional.

3. **Initial LLM Call:** Formulate the initial prompt for the LLM. Using the structured approach, you might send:

   - System role message: explaining the format and that tools are available. Example: `"You are an assistant. Answer the user. If you need extra info, respond with a JSON ToolRequest of the form {\"tool\": ..., \"args\": ...}. Otherwise, answer with {\"answer\": ...}."`
   - User role message: containing the actual user question (and possibly some brief context if available or necessary).
     Then call `await call_llm(prompt_messages)`. This will yield the LLM’s response. If streaming chunk by chunk, you might want to buffer the chunks until you detect the end of a JSON structure (or you can accumulate the whole response first for easier parsing).

4. **Interpret LLM Response:** Take the output from the LLM and attempt to parse it into the models. For example:

   ```python
   raw_response = "".join([chunk async for chunk in call_llm(messages)])
   tool_request = None
   final_answer = None
   try:
       tool_request = ToolRequest.parse_raw(raw_response)
   except ValidationError:
       try:
           final_answer = FinalAnswer.parse_raw(raw_response)
       except ValidationError:
           # The response didn't match any expected schema
           final_answer = FinalAnswer(answer=raw_response.strip())
   ```

   In this pseudocode, we collect the full response (assuming it’s not huge) for parsing. If it matches `ToolRequest`, we know the LLM wants to use a tool. If it matches `FinalAnswer`, we have the answer. If neither parsed (say the LLM gave a normal sentence or some format error), we treat the whole output as the answer content. This way, the conversation still continues even if the model slightly deviated from the expected format.

5. **Tool Invocation (if requested):** If `tool_request` is not `None`, proceed to fulfill it:

   - Determine which tool/service is needed. For example, `tool_request.tool` might be `"memory"` or `"cognition"`. You should have clients for these. A simple implementation can map tool names to functions: e.g., `"memory"` -> `memory_client.query(...)` and `"cognition"` -> `cognition_client.analyze(...)`. Use the `args` provided by the LLM in the request to call the tool appropriately.
   - Await the result from the service call. Handle any errors (e.g., the Memory service is down or returns nothing) by returning a safe message or empty result.
   - Incorporate the tool result into a follow-up prompt for the LLM. Usually, you’d provide the tool output in a new system message or as the assistant’s message, then ask the LLM to continue. For example:

     ```python
     tool_name = tool_request.tool
     data = await fetch_tool_result(tool_name, tool_request.args)
     # Construct new prompt messages
     followup_messages = []
     followup_messages += prompt_messages  # start with original prompt for context
     followup_messages.append({
         "role": "assistant",
         "content": f"Tool result for {tool_name}: {data}"
     })
     followup_messages.append({
         "role": "user",
         "content": "Please continue and provide the final answer."
     })
     answer_chunks = [chunk async for chunk in call_llm(followup_messages)]
     final_text = "".join(answer_chunks)
     final_answer = FinalAnswer.parse_obj({"answer": final_text})
     ```

     Essentially, we tell the LLM: “Here’s the info you asked for, now proceed.” We append an assistant message with the tool output (so the model knows that it was the assistant itself that got that data), and then a user prompt or system instruction to produce the final answer. Simpler approach: just include the tool result and ask “Now answer the user’s question.” The LLM should then produce a final answer. We parse that into `FinalAnswer` (or just take the text).

   - **Loop Limiting:** We deliberately only allow one tool request in this flow. You could structure this with a `for` loop or `while` that runs a limited number of cycles. For instance: `for attempt in range(2): ...` and break once a final answer is obtained. If the LLM tries to request another tool after the first, you might either refuse (treat it as error or ignore and ask for final answer) or handle it if it’s trivial. But in our design, we assume one tool usage is enough for the vast majority of queries. This prevents complicated multi-step planning which is out of scope for a simple orchestrator.

6. **Prepare the Output:** By this point, we should have a `final_answer` (either from the first LLM call if no tool was needed, or after the follow-up call). Extract the answer text. You may also have some metadata (perhaps which tool was used, or any flags). Construct an output event payload, for example:

   ```python
   output_event = {
       "type": "output",
       "user_id": event["user_id"],
       "conversation_id": event["conversation_id"],
       "data": {"answer": final_answer.answer}
   }
   ```

   If you want to support streaming, you could instead send multiple output events: e.g., while receiving chunks from `call_llm`, publish each partial chunk as it arrives (`{"type": "output", "user_id": ..., "content": "partial text", "partial": True}`), and then send a final event with a `partial: False` or some indicator to mark completion. A simpler approach is to accumulate internally and publish one event at the end for now (it will still reach the client near-instantly after generation).

7. **Publish the Output Event:** Use the EventBus to publish the result so that any subscribers (like the SSE manager) get it. For example:

   ```python
   event_bus.publish(output_event)
   ```

   Since our orchestrator may be handling one event at a time, it can immediately publish once done, before moving to the next input. The EventBus will typically broadcast this to all listening queues; the SSE subsystem will filter events by user or conversation and push to the correct client connection.

8. **Post-Processing:** (Optional) You might store the assistant’s answer in the Memory service, so the conversation history is complete. This can be done by calling `memory_client.store_message(user_id, conversation_id, role="assistant", content=answer_text)`. This way, the memory has a log of all messages. While not strictly necessary for the orchestrator’s own functioning, it is useful to persist the conversation. If implemented, ensure this call is also minimal and non-blocking (perhaps fire-and-forget, or not critical if it fails).

Throughout `handle_input_event`, maintain clarity and linearity. Use helper functions to break out sub-tasks if it becomes too long – e.g., `fetch_tool_result(tool, args)` as a separate coroutine for tool use, or `format_initial_prompt(event)` as a helper to construct the LLM prompt. Each part (getting context, calling LLM, parsing output, invoking tool, publishing result) should be understandable in isolation. We avoid any complex class hierarchy or state machine; the straightforward procedural flow is easier to trace and less error-prone.

([Publisher-Subscriber pattern - Azure Architecture Center | Microsoft Learn](https://learn.microsoft.com/en-us/azure/architecture/patterns/publisher-subscriber)) _Example of a simple publish-subscribe flow:_ The API publishes an **input** event to the Event Bus, the orchestrator (as a subscriber) processes it and then publishes an **output** event which is delivered to clients. This decoupling via an event bus allows the input and output to be handled asynchronously.

### 6. Starting with FastAPI and Clean Shutdown

As mentioned, the orchestrator is started on app startup. In practice, you might incorporate it in the main application module like so:

```python
# app/main.py
app = FastAPI()

# ... include routers, etc. ...

@app.on_event("startup")
async def startup_event():
    # maybe initialize event_bus here or ensure clients ready
    asyncio.create_task(run_response_orchestrator())

@app.on_event("shutdown")
async def shutdown_event():
    # if needed, signal the orchestrator to stop
    if orchestrator_task:
        orchestrator_task.cancel()
    await event_bus.close()  # hypothetical cleanup if event_bus needs it
```

Make sure that any resources the orchestrator uses are ready by startup. For example, if the MemoryService client needs to connect or the EventBus needs initialization, do that before launching the task. Conversely, on shutdown, close connections (the LLM might not need explicit closing if it’s stateless HTTP calls, but if using a persistent connection or session, close it). The event bus might not need cleanup if it’s just in-memory queues, but if it has background tasks or threads, join/close them too.

One subtle point: if using `asyncio.create_task` inside an async startup handler (as shown), the task will run concurrently. If you prefer, you can also start a background task via FastAPI’s built-in mechanism with `BackgroundTasks`, but since we need an indefinite loop, `create_task` in startup is fine. Document this behavior clearly so that other developers know the orchestrator is running as soon as the app is up.

### 7. Running the Orchestrator in Isolation

During development or debugging, it’s useful to run the orchestrator logic outside the full app to ensure it works. You can simulate an input event easily. For example, write a simple test script or unit test that does:

```python
# Prepare a dummy EventBus and clients
event_bus = EventBus()
memory_client = FakeMemory()    # stub that returns preset history
cognition_client = FakeCog()    # stub that returns a preset summary
# Configure orchestrator dependencies (could be global or passed in)

# Publish a test input event
event_bus.publish({
    "type": "input",
    "user_id": "test-user-1",
    "conversation_id": "conv-123",
    "content": "Hello, what’s my last message?"
})
# Run the orchestrator loop for one iteration
asyncio.run(asyncio.wait_for(run_response_orchestrator(), timeout=1.0))
# Check the EventBus for an "output" event
output_event = event_bus.get_last_event("output")
print(output_event)
```

This pseudo-code demonstrates triggering the orchestrator without an HTTP server. In practice, you might need to adapt how you break out of the loop (for instance, modify `run_response_orchestrator` to stop after one cycle when running in test mode). Alternatively, expose a method `process_one_event(event)` that does the handling for a single event without the infinite loop, and test that method directly by passing a crafted event object.

You can also run the full app and use the HTTP endpoints to drive the flow in a controlled way. For example, using `uvicorn` to start the app, then sending a POST to `/input` via a REST client, and observing the SSE output on `/output/stream`. For manual testing, implement a simple client script or use `curl`/browser to ensure the end-to-end path works.

### 8. Testing Strategy

**Unit Tests:** For the orchestrator component, you should isolate and test the logic in various scenarios:

- **Direct Answer Path:** Simulate a case where the LLM responds with a final answer right away. Use a fake `call_llm` that returns a known answer (e.g., `"{"answer": "42"}"` or just `"42"`). Verify that `handle_input_event` publishes an output event with that answer and does not call any tool clients.
- **Tool Request Path:** Simulate the LLM returning a `ToolRequest`. For example, have the fake LLM return `{"tool": "memory", "args": {"query": "recent"}}`. Also fake the `memory_client.get_history` to return some dummy history. Then ensure that the orchestrator calls the memory client, then calls the LLM again, and finally publishes the expected answer. You might need to fake the second LLM call too (so that when provided the tool output, it returns a final answer). This can be done by configuring the fake `call_llm` to return different outputs depending on input (for example, inspect the prompt and if it contains `"Tool result"`, return a final answer).
- **Validation Handling:** Test that if the LLM returns a malformed output (neither valid JSON nor matching the schema), the orchestrator still wraps it into a FinalAnswer. For instance, fake an LLM response like `"I couldn't find anything."` (not JSON). The orchestrator should interpret that as answer text and publish it, rather than throwing an error.
- **Error Handling:** Force exceptions in different places: e.g., memory service raising an exception or timing out, or the LLM call raising an `APIError`. Verify that the orchestrator catches these and publishes an error event (with `error: true` or similar) and does not stop the loop. This ensures resilience.

Use dependency injection or monkeypatching to replace actual network calls with test doubles. Because our implementation avoided singletons and heavy static logic, it’s straightforward to inject fakes: e.g., pass a `memory_client` object to the orchestrator or use global variables that tests can override. The code should not directly call external APIs without a way to override (for instance, ensure the OpenAI API key is not required in tests by using a fake LLM provider or PydanticAI’s test model to simulate responses).

**Integration Tests:** Set up the full stack using FastAPI’s `TestClient` (for synchronous tests) or `AsyncClient` (for async tests) to simulate actual HTTP requests and SSE streaming:

- Start the FastAPI app in the test (or use `TestClient(app)`), and have an endpoint or mechanism to retrieve events from the EventBus (or more realistically, connect to the SSE and read events).
- Post to `/input` with a sample message, then poll or wait on an output. Since SSE is tricky to test synchronously, one approach is to have the test subscribe directly to the event bus. If the EventBus is accessible, the test can subscribe to output events and verify it receives the correct one in response to the input.
- Alternatively, run the orchestrator in the test without the web layer: publish to event bus, and ensure output event comes. This is more a unit test of the orchestrator as described, but doing it through FastAPI ensures that wiring (publishing in the endpoint, etc.) is correct.
- Test different configurations: set `LLM_PROVIDER` to `"anthropic"` (assuming keys available) and verify the orchestrator still works. Possibly use a dry-run mode for LLM calls in integration tests (to not actually call external APIs).

**Coverage considerations:** Aim to cover the branching logic:

- Tool vs no-tool.
- Valid JSON vs invalid.
- Error paths for each external call.
- The loop continuation after errors (you can enqueue two events, one that causes an error, followed by a normal one, and ensure the second still gets processed).
- Multi-user or multi-conversation scenario if applicable: e.g., ensure that two input events interleaved (with different conv IDs) still each produce correct outputs. (Our simple single-loop design will handle them sequentially, so order of arrival dictates order of responses – which is fine for MVP.)

By designing the orchestrator with simple, injectable components and avoiding hidden state, we make it much easier to test thoroughly. The focus is on deterministic, predictable behavior for each input event.

### 9. Keeping it Simple and Extensible

This implementation intentionally avoids over-engineering:

- We did **not** create an elaborate class hierarchy or abstractions for orchestrating tools. A straightforward set of `if/else` and function calls suffices. This adheres to the “present-moment focus” – solving the problem at hand in the simplest way.
- The orchestrator is isolated in its own module (or a small set of functions). It communicates with the rest of the system only via well-defined interfaces: the EventBus and the service clients. This isolation means we could modify its internal logic (or swap out the LLM provider) without affecting other parts, as long as it consumes and produces the same events.
- We trust the external systems (LLM API, memory service) to do their jobs. We call them directly and handle errors, but we don’t add layers around them. This builds on the **“direct over indirect”** principle from our guides – simpler to integrate directly than to wrap everything.
- All configuration (like model choice, API keys, tool endpoints) is done via environment/settings. No hard-coding of model IDs or magic strings in the logic. This makes the component flexible for different deployments and easier to configure.
- We placed a hard limit on tool loop depth (one tool use). This keeps the interaction understandable. In the future, if we want more complex agent behavior, we can expand the loop or integrate an agent framework – but for now, one level deep covers the main use case (e.g., fetch needed info then answer).

Following these guidelines, the LLM Response Orchestrator remains **clear, testable, and maintainable**. Another developer (or AI assistant) should be able to read through the code and map it almost one-to-one to the steps described in this guide.

## Example Walk-Through

To illustrate the flow, imagine a user asks: “What did I tell you earlier about my appointment?” and that information is stored in the memory service:

1. The `/input` API receives the request and immediately publishes an input event: `{"type": "input", "user_id": "U1", "conversation_id": "C1", "content": "What did I tell you earlier about my appointment?"}`.
2. The orchestrator’s event loop picks up this event and begins processing.
3. It sends the user’s query to the LLM with instructions for tool use. The LLM returns a `ToolRequest(tool="memory", args={"query": "appointment details"})` – effectively asking for relevant memory.
4. The orchestrator calls the Memory service (via its client) with the query. Suppose the memory returns: “You said your doctor appointment is on Monday at 10 AM.”
5. The orchestrator feeds that info back into the LLM prompt, asking it to answer the question.
6. The LLM now responds with a `FinalAnswer(answer="You mentioned that your appointment is on Monday at 10 AM.")`.
7. The orchestrator publishes an output event with this answer. The SSE subsystem sends it to the user’s browser, and the user sees the assistant’s reply.

This whole sequence happens asynchronously and relatively quickly. The user only had to wait for the final answer event (which our system streams as soon as it’s ready). Under the hood, the orchestrator did exactly one tool call and two LLM calls, respecting the design limit.

## Testing and Validation

During development, use unit tests to validate each part of this logic, and integration tests to cover the end-to-end behavior. Before deploying, run through a few manual scenarios (with and without tools needed) to ensure the assistant behaves as expected. Logging at each step (with sensible log levels) can help trace the processing in a running system without stepping through a debugger. For instance, log when a ToolRequest is detected, what tool was called, and when an output event is published.

By adhering to this guide, you’ll implement the LLM Response Orchestrator as a focused, reliable module of Cortex Core. Its simplicity is a strength: it’s easier to extend or tweak once the basic version is running, and it’s less prone to bugs due to unnecessary complexity. Happy coding!
