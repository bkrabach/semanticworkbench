# In-Memory Event Bus – Implementation & Documentation Guide

## Overview

The **In-Memory Event Bus** is a core component of the Cortex Core MVP that provides a simple publish/subscribe mechanism for internal events. It decouples producers and consumers of events (such as input handlers, background processors, and output streams) without introducing external messaging systems. This component is designed with **ruthless simplicity** and **separation of concerns** in mind: it uses Python’s built-in async capabilities directly (no unnecessary wrappers) and focuses only on current needs. The result is a minimal, in-memory pub/sub system that fits seamlessly into the FastAPI & `asyncio` application context, enabling real-time server-sent events (SSE) and internal task orchestration with minimal overhead.

**Key characteristics of this Event Bus:**

- **Minimal API**: Just enough functionality to subscribe to events, publish events, and unsubscribe. No premature abstractions or features that aren’t needed in the MVP (e.g. persistent queues, complex routing).
- **Direct Library Usage**: Leverages native constructs like `asyncio.Queue` for event delivery, instead of building custom queue mechanisms. This adheres to the project’s philosophy of using libraries as intended and avoiding needless indirection.
- **Topic-Based Simplicity**: Events are categorized by a **type** (and optionally by a target context like conversation ID), enabling subscribers to get only the events they care about. We avoid any elaborate filtering or pattern matching beyond these basics.
- **Asynchronous & Non-Threaded**: Designed to run within the FastAPI event loop. Producers and consumers use `await` to send/receive events. No threads or background polling are introduced, keeping concurrency simple and implicitly managed by `asyncio`.
- **Separation of Concerns**: The Event Bus does not perform business logic. It simply passes messages. Producers (e.g. the user input endpoint or a response generator) publish events, and consumers (e.g. the response handler orchestrator or SSE streamer) subscribe and react to those events. This keeps each piece of the system focused on its own task, following the single-responsibility mindset.

By adhering to these principles (consistent with the **Cortex Platform Implementation Philosophy** of keeping things simple and focusing on present requirements), the Event Bus remains easy to understand, maintain, and integrate into the broader architecture.

## Implementation Plan

To implement the In-Memory Event Bus, we will proceed with a focused, step-by-step approach. Each step corresponds to a small, self-contained task that an AI assistant (or developer) can implement in isolation:

1. **Define the Event Schema** – Establish a minimal, consistent structure for event messages that will be used by all producers and consumers. This schema will include core fields like event type and context identifiers, ensuring everyone speaks the same “event language.” (Prefer simple dictionaries for events to avoid over-engineering; use Pydantic models only if strict validation is absolutely required, which for MVP it is not.)
2. **Create the EventBus Class** – Implement a lightweight `EventBus` class with in-memory subscription management. Provide methods to subscribe to events (with optional filters like event type or conversation ID), publish events, and unsubscribe. Use **direct and simple data structures** (like lists and dicts) to track subscribers. Keep the logic straightforward: no advanced routing or thread management, just basic pub/sub on the current event loop.
3. **Use Asyncio Queues for Delivery** – For each subscription, use an `asyncio.Queue` to hold events for that subscriber. When events are published, put them into the appropriate queues. This allows subscribers to asynchronously iterate over new events at their own pace (ideal for streaming responses via SSE or processing events in background tasks). It also avoids complex callback chains or additional threads, aligning with the **direct paths** principle.
4. **Ensure Safe Publish & Subscribe Operations** – Implement minimal necessary error handling and cleanup. For example, avoid crashes if a subscriber is not present, and provide a way to remove subscribers (so we don’t leak queues when clients disconnect). Given the simplicity, we won’t add heavy locking or threading, but we will document that subscribers should unsubscribe when no longer needed (especially in long-lived connections like SSE). The system will trust components to manage their lifecycle (reflecting “pragmatic trust” and focusing on common cases).
5. **Instantiate a Singleton Event Bus** – To make the Event Bus easily accessible, create a single instance (e.g. at application startup) that other modules can import and use. This ensures there is one central bus coordinating events. (For example, we might instantiate it in the `app.services` or `app.core` package so that `event_bus` can be imported wherever needed.)
6. **Integrate with Other Components** – Provide integration points and usage examples for components that produce or consume events. This includes:
   - The **Input handler** (API endpoint) publishing a `"user_message"` event when a new user message is received.
   - The **Response Handler** (background orchestrator) subscribing to `"user_message"` events, triggering the AI response generation, and then publishing an `"assistant_response"` event.
   - The **SSE Stream (Output)** logic subscribing to `"assistant_response"` events for a specific conversation and streaming those out to the client.
     We will illustrate how these interactions happen using the Event Bus interface, referencing stub or interface definitions if they exist (or providing simple scaffolding if not). This ensures the Event Bus fits into the end-to-end flow without ambiguity.
7. **Document Clearly and Concisely** – Alongside the code, include concise documentation for each part of the component. Use docstrings and comments to explain the intent of methods and any non-obvious decisions. Keep the language simple and the explanations focused, in line with writing **code as communication**. Also, avoid any extraneous information – document what’s needed to use and maintain the event bus, but nothing more (no hypothetical extensions or unused features in this guide).

Following this plan will yield a self-contained Event Bus implementation that an assistant can build directly. Next, we detail each part, including the event schema and the actual code for the EventBus class, along with usage examples demonstrating integration points.

## Internal Event Schema

Consistency in event structure is important so that all parts of the system know how to interpret the messages. We define a minimal internal event schema as a Python dictionary with a few standard keys. **All events** published on the bus should include at least:

- **`type`** (`str`): The event type or name, e.g. `"user_message"`, `"assistant_response"`, `"error"`, etc. This indicates what kind of event it is and typically determines who should handle it.
- **`conversation_id`** (`str` or UUID): Identifier for the conversation context this event is associated with (if applicable). In a chat application, most events relate to a specific conversation. This allows consumers (like SSE streams) to filter events to the correct client conversation.
- **`user_id`** (`str` or UUID): Identifier for the user associated with the event (if applicable). This can be used for additional filtering or context, ensuring events are routed only to authorized/related consumers. In a single-tenant context, this might be less critical, but it’s included for completeness and future multi-user scenarios.
- **Additional payload fields**: Any other data relevant to the event. This will vary by event type. For example, a `"user_message"` event might include a `"content"` field for the text of the message. An `"assistant_response"` event might include a `"response"` or `"content"` field with the AI’s reply, and possibly a `"message_id"` or other metadata. We keep these fields as flat as possible (no deeply nested structures unless necessary) to maintain clarity.

**Example Event Payloads:**

```python
# Example of a user message event published when a user sends a new chat message
{
    "type": "user_message",
    "user_id": "u-12345",
    "conversation_id": "c-abcde",
    "content": "Hello, world!"
}

# Example of an assistant response event, published when the AI responds
{
    "type": "assistant_response",
    "user_id": "u-12345",
    "conversation_id": "c-abcde",
    "content": "Hi there! How can I assist you today?",
    "request_id": "r-98765"   # example of an optional field, like linking to a request
}
```

All producers of events (e.g., the input endpoint or the response generation logic) should construct their event dictionaries in this form. All consumers (e.g., the SSE streamer) can then rely on fields like `conversation_id` and `type` to decide what to do with each event. By using plain dictionaries, we avoid unnecessary complexity – this is sufficient for internal messaging. (If at some point strict validation is needed, we could introduce Pydantic models for events, but in the MVP it’s not required as we control the event creation points and keep them consistent.)

**Note:** We are deliberately not adding timestamps or sequence IDs at this stage, since ordering is implicitly handled by the sequence of `publish` calls (and our current single-process, in-memory design). If needed, those could be added later, but per the **“no future-proofing”** principle, we include only what we need right now.

## EventBus Component Implementation

With the schema defined, we now implement the Event Bus itself. The `EventBus` will be a simple class providing `subscribe()`, `publish()`, and `unsubscribe()` methods. Internally, it will track subscribers in memory and route events to them. Each subscriber is associated with an `asyncio.Queue` that will receive events. This design allows different parts of the system to consume events by simply awaiting items from a queue, which fits naturally with async workflows (for example, an SSE response can iterate over a queue of events to stream).

Key considerations in this implementation:

- We allow optional filtering by event type and conversation. A subscriber can listen for only a certain type of event (e.g., only `"assistant_response"` events) and/or only events for a certain conversation (e.g., only events for conversation `"c-abcde"`). This prevents subscribers from having to sift through irrelevant events. In many cases for MVP, we will use one filter or the other (e.g., response handler cares about event type, SSE cares about conversation and event type both). Both filters are optional for flexibility.
- We use a simple data structure (a list of subscriber records) to store subscriptions. Each record can hold the filter criteria and the queue. For simplicity and to avoid premature optimization, we will linearly check this list on each publish. Given the expected small number of subscribers (likely on the order of active SSE connections plus one or two internal consumers), this is absolutely fine and keeps the code straightforward. We avoid introducing complex indexing or topics mapping until a real need arises.
- `publish` will distribute events **synchronously** on the event loop by enqueuing messages to subscribers. We won’t spawn threads or background tasks for this; the overhead is minimal (just `queue.put_nowait` calls). If any subscriber’s queue is full or becomes unavailable, we’ll catch exceptions to prevent one failing consumer from breaking the bus. (In this MVP context, we typically use unbounded queues or handle overflow gracefully, because back-pressure and flow control are not critical for the initial use cases.)
- `subscribe` returns the `asyncio.Queue` to the caller. The caller can then `await queue.get()` to receive the next event. This approach aligns with an **iterative async flow** (especially for SSE streaming where we continuously read events). We’ll include an example of turning this queue into an SSE stream below.
- `unsubscribe` allows removing a subscriber when it’s no longer needed (for example, when an SSE client disconnects or a background task ends). This prevents memory leaks by clearing out the queue reference. The method will also allow the calling component to optionally do any additional cleanup (like closing the queue or indicating completion).

Below is the implementation of the `EventBus` class with these points in mind:

```python
import asyncio
from typing import Any, Dict, Optional

class EventBus:
    """In-memory publish/subscribe event bus for internal Cortex events."""
    def __init__(self):
        # List of subscriber subscriptions. Each item is a dict with:
        # {"queue": asyncio.Queue, "event_type": Optional[str], "conversation_id": Optional[str]}
        # Subscribers will receive events that match on both filters (if provided).
        self._subscriptions = []

    def subscribe(self, event_type: Optional[str] = None, conversation_id: Optional[str] = None) -> asyncio.Queue:
        """
        Subscribe to events on the bus, optionally filtering by event_type and/or conversation_id.
        Returns an asyncio.Queue that will receive matching events.
        The caller is responsible for reading from the queue and eventually unsubscribing.
        """
        # Create a new queue for this subscriber
        queue: asyncio.Queue = asyncio.Queue()
        # Register the subscription with the specified filters
        sub_record = {
            "queue": queue,
            "event_type": event_type,
            "conversation_id": conversation_id
        }
        self._subscriptions.append(sub_record)
        return queue

    def publish(self, event_type: str, event_data: Dict[str, Any]) -> None:
        """
        Publish a new event to the bus.
        event_type: The type/name of the event (e.g., "user_message", "assistant_response").
        event_data: A dictionary containing the event payload. Must include at least the event type and context.
        """
        # Ensure the event_data itself knows its type (for consistency)
        # – typically event_data['type'] should equal event_type.
        event_data.setdefault("type", event_type)
        # Iterate over a *copy* of subscriptions to avoid issues if the list is modified during iteration
        for sub in list(self._subscriptions):
            # Check filters: if subscriber specified an event_type or conversation_id, filter by those
            if sub["event_type"] is not None and sub["event_type"] != event_type:
                continue  # event type doesn't match this subscriber's filter
            if sub["conversation_id"] is not None:
                # If the event has a conversation_id, check it against the filter.
                # If event lacks conversation_id (None), we skip it if subscriber expects one.
                if "conversation_id" not in event_data or event_data.get("conversation_id") != sub["conversation_id"]:
                    continue
            # If we reach here, the event matches the subscriber's criteria; deliver it.
            try:
                # Use put_nowait to enqueue the event without blocking. In normal cases, the queue is empty or small.
                sub["queue"].put_nowait(event_data)
            except asyncio.QueueFull:
                # If a queue has a size limit and is full, we could drop the event or log a warning.
                # For MVP, we assume queues are unbounded (default) or large enough, so this is unlikely.
                continue
            except Exception as e:
                # Catch-all for any unexpected errors to ensure one bad subscriber doesn't break publishing.
                # We remove the subscriber to avoid repeated errors.
                print(f"[EventBus] Error delivering event {event_type} to subscriber, unsubscribing it. Error: {e}")
                self._subscriptions.remove(sub)

    def unsubscribe(self, queue: asyncio.Queue) -> None:
        """
        Unsubscribe a previously subscribed queue from the bus.
        This will stop delivering events to that queue and allow it to be garbage-collected.
        The caller should ensure no further reads are done on the queue after unsubscribing.
        """
        # Find the subscription with this queue and remove it
        self._subscriptions = [sub for sub in self._subscriptions if sub["queue"] is not queue]
        # Optionally, you might want to notify or close the queue. We leave it to caller to handle queue closure if needed.
        # For example: queue.put_nowait(None) could be used as a sentinel to indicate no more events.
```

**About this implementation:**

- The `EventBus` stores each subscription as a dictionary containing the subscriber’s queue and filter criteria. We choose a list of dicts for simplicity. (This could have been a custom `Subscription` object or tuple, but a dict keeps it clear and is easy to work with.) We also avoid any global state outside the class – all subscriptions live in the `_subscriptions` list on the instance.
- `subscribe(...)` creates a new `asyncio.Queue()` and records the subscription. We do not impose a max size on the queue by default (so it’s unbounded), under the assumption that events will be consumed reasonably quickly. If needed, a maxsize could be set to avoid unbounded memory growth, but in an MVP scenario with controlled usage, that’s likely unnecessary. The function returns the queue so the caller can start consuming from it. We rely on the caller to call `unsubscribe` when done; otherwise, the reference in `_subscriptions` would keep it alive.
- `publish(event_type, event_data)` goes through each subscriber and delivers the event if it matches their criteria. We use `event_data.setdefault("type", event_type)` to ensure the payload has a `type` field that matches – this is just a sanity measure so that subscribers can always read `event["type"]` if they want to double-check or use the payload standalone. The filtering logic checks:
  - If the subscriber specified an `event_type` filter and it doesn’t match the event’s type, skip this subscriber.
  - If the subscriber specified a `conversation_id` filter, then we ensure the event has a `conversation_id` and it matches; otherwise skip. (If an event doesn’t naturally have a `conversation_id`, it simply won’t be delivered to subscribers that asked for a particular conversation.)
  - If both filters pass (or if the subscriber had no filter on one or both), we enqueue the event on that subscriber’s queue.
- We convert the subscription list to a copy (`list(self._subscriptions)`) before iterating. This prevents issues if a subscriber unsubscribes or a new one subscribes while we’re in the middle of a publish. It’s a simple way to avoid iteration invalidation without needing complex locking. Given our usage patterns (subscribe/unsubscribe will not be extremely frequent relative to publishes), this is acceptable. This approach also inherently handles the case where an unsubscribe might happen from another task concurrently: even if `_subscriptions` is modified, we are iterating over a snapshot.
- We catch exceptions from the `queue.put_nowait` call. In normal operation, using an unbounded queue, we don’t expect `QueueFull`. If a bounded queue is used and fills up, the current strategy is to drop the event for that subscriber (just `continue`), under the assumption that if a subscriber falls behind drastically, we don’t want to block the whole system. This is a simple error handling approach aligning with **“handle common cases and trust components”** – we trust that subscribers normally keep up. We also have a broad exception catch that logs an error and removes the subscriber if something unexpected happens during delivery (for example, if the subscriber’s queue has been closed elsewhere, causing an exception). This ensures one misbehaving subscriber doesn’t continually throw errors on every publish.
- The `unsubscribe(queue)` method filters out the subscription with the matching queue. We do a list comprehension to rebuild `_subscriptions` without that entry. If the queue isn’t found, the method silently does nothing (which is fine – it means it was already removed or never registered). After unsubscribing, new events will no longer be put into that queue. We leave it to the subscriber’s code to decide what to do with the queue (they might simply let it be garbage-collected, or they might put a termination token like `None` into it to break any consumer loops). We note that an improvement could be to set a flag or put a final event, but for MVP this is left simple: unsubscribe means “you won’t get further events.”

This implementation focuses on clarity and current needs. It avoids any premature generalization (for instance, we are not implementing hierarchical topics, wildcards, priority, etc.). It’s essentially a few dozen lines of straightforward code fulfilling the role of an event mediator in-memory, which matches the project’s goal of **scrappy but structured** solutions. We also keep it framework-agnostic (pure Python, no FastAPI-specific code here), so it could be used in any async context within the application.

### Singleton Instance and Usage

In a typical application setup, we will create a single instance of this `EventBus` and use it throughout. For example, in the application’s startup, or in a module that is imported by others, we could do:

```python
# app/core/event_bus.py (or app/services/event_bus.py)
from app.core.event_bus import EventBus  # assuming EventBus class is defined in this module
event_bus = EventBus()
```

and in other parts of the application, import `event_bus` to publish or subscribe to events. This avoids passing around the EventBus instance everywhere and is acceptable here because the EventBus is a fundamental piece of infrastructure (in line with how one might use a global logger or a database session factory in a simple scenario). This usage is seen in our API layer where an endpoint simply calls `event_bus.publish(...)` – the endpoint doesn’t need to know how EventBus is implemented, just that it’s available as a shared service.

_(In our project structure, we might locate `EventBus` in a `components` or `core` module, and then import an instance into `app.services` for convenience. The exact file location isn’t critical as long as it’s accessible; what matters is treating it as a singular resource.)_

## Integration with Other Components

With the Event Bus in place, we outline how it connects to producers and consumers of events in the system. The major interactions in the Cortex Core MVP involve:

- **User input events**: When the user sends a message (via the POST `/input` endpoint), an event should be published to notify the system of the new message.
- **Response handling**: A background task (the response handler orchestrator) listens for input events, invokes the necessary logic (memory lookup, LLM call, etc.), and then publishes an output event.
- **Output streaming**: The GET `/output/stream` SSE endpoint subscribes to output events for the relevant conversation and streams them to the client in real-time.

We ensure the Event Bus API is suitable for these uses. Below, we provide stubbed examples demonstrating these integration points.

### 1. Publishing User Input Events (Producer Example)

The input endpoint itself should remain a thin wrapper (per our design principles, no heavy logic in the endpoint). When a message comes in, after basic processing, the endpoint will publish an event to the Event Bus and return a quick acknowledgment to the client. For instance:

```python
# In the Input endpoint (e.g., app/api/input.py)
@router.post("/input")
async def receive_user_message(input_msg: InputMessage, current_user=Depends(get_current_user)):
    user_id = current_user.id
    conv_id = input_msg.conversation_id or conversation_service.get_or_create_default_conversation(user_id)
    # Prepare the event payload for the new message
    event = {
        "type": "user_message",
        "user_id": user_id,
        "conversation_id": conv_id,
        "content": input_msg.content
    }
    # Publish the user_message event to the EventBus (non-blocking fire-and-forget)
    event_bus.publish("user_message", event)
    # (Also save message to memory or database, handled by another service)
    return {"status": "received", "conversation_id": conv_id}
```

In this example, the event bus decouples the API from whatever processes the message. The endpoint doesn’t wait for a response to be generated; it simply emits an event and returns. The choice of event type `"user_message"` and the payload structure aligns with our schema. Note that we include `conversation_id` and `user_id` so any consumer knows the context of this message.

This demonstrates **separation of concerns**: the API layer doesn’t directly invoke the AI or any long-running task. It relies on the Event Bus to hand off the work to whoever is subscribed for `"user_message"` events. Also, the use of `event_bus.publish` is a direct call with a dictionary – no additional abstraction layer, just using our event bus instance as a global mediator.

### 2. Response Handler Orchestrator (Consumer & Producer Example)

On the other side, we have a **Response Handler** component that runs in the background (likely started at application startup as an `asyncio` task). Its job is to wait for new input events and produce output events. We have a stub of this in the project (often in `app/core/response_handler.py`). Using the Event Bus, the logic would look roughly like:

```python
# app/core/response_handler.py

async def run_response_handler():
    """
    Background task that subscribes to user_message events and produces assistant_response events.
    """
    # Subscribe to all user_message events (across all conversations)
    queue = event_bus.subscribe(event_type="user_message")
    print("[ResponseHandler] Subscribed to user_message events, waiting for messages...")
    while True:
        event = await queue.get()  # wait for the next user_message event
        if event is None:
            # If we ever use None as a sentinel for shutdown, break out.
            break
        try:
            # 1. Retrieve conversation context (e.g., recent messages) via a memory service or client
            conv_id = event["conversation_id"]
            user_id = event["user_id"]
            message_content = event.get("content", "")
            conversation_history = memory_service.get_conversation_history(user_id, conv_id)
            # 2. Call cognition/LLM service to generate a response (could be async call)
            ai_response_text = await llm_service.generate_response(user_id, conv_id, message_content, conversation_history)
            # 3. Publish the assistant_response event with the generated content
            response_event = {
                "type": "assistant_response",
                "user_id": user_id,
                "conversation_id": conv_id,
                "content": ai_response_text
            }
            event_bus.publish("assistant_response", response_event)
            print(f"[ResponseHandler] Published assistant_response for conversation {conv_id}")
        except Exception as e:
            # Basic error handling: log and continue (we don't want the loop to stop on errors)
            print(f"[ResponseHandler] Error processing message event: {e}")
            error_event = {
                "type": "assistant_response",  # we can still use assistant_response type for errors, or define separate type
                "user_id": event.get("user_id"),
                "conversation_id": event.get("conversation_id"),
                "error": str(e)
            }
            event_bus.publish("assistant_response", error_event)
```

In this pseudocode:

- The response handler subscribes to `"user_message"` events (with no specific conversation filter, because it wants to handle all incoming messages). It then enters a loop waiting for events on the returned queue.
- When a new event is received, it extracts the information (conversation ID, user ID, and message content). It uses other services (`memory_service` and `llm_service` in this example, which would be simple interfaces to a memory store and the language model respectively) to get the necessary data and produce a response. These details are beyond the scope of the Event Bus, but it shows where the bus fits: it triggers the sequence, and it carries the result.
- After computing the AI’s response, the handler publishes an `"assistant_response"` event back to the Event Bus. Note that it uses the same `conversation_id` and `user_id` so that any listener (like the SSE for that conversation) can identify where this response belongs. We keep the event schema uniform – it’s still a dictionary with type and context and content.
- If an exception occurs during processing (for instance, if the LLM service fails), we catch it, log it, and publish an event indicating an error. In this simplified case, we reuse the `"assistant_response"` type with an `"error"` field. Alternatively, we could define a separate `"error"` event type. For MVP, consolidating under one type might be okay for simplicity, as long as consumers handle the presence of an `"error"` key. In any case, the Event Bus is used to notify downstream components of the failure as well (which could be shown to the user via SSE).

This background task approach embodies the **“visualize end-to-end flow”** principle – from input event to output event, with the Event Bus connecting the pieces. The code remains straightforward and linear within the async loop, and importantly, **the Event Bus hides the complexity of multi-component communication**: the handler doesn’t need to know about SSE or any other subscriber, it just publishes the result and moves on.

### 3. SSE Stream Manager / Output Endpoint (Consumer Example)

Finally, the output side: when a client connects to receive events via Server-Sent Events, we use the Event Bus to get those events. FastAPI (Starlette) allows us to return an `asyncio` stream (like an `AsyncGenerator`) as a response, which can be used to push events to the client as they arrive. The typical pattern is:

- Client calls GET `/output/stream?conversation_id=...`
- We authenticate the user, determine which conversation’s events to stream.
- We subscribe to the Event Bus for `"assistant_response"` events for that conversation.
- We then enter a loop, awaiting events from the queue and yielding them in SSE format (text/event-stream).
- If the client disconnects or the conversation ends, we break the loop and unsubscribe.

Here’s a skeletal integration of that in code form:

```python
# In the Output SSE endpoint (e.g., app/api/output.py)
from fastapi import Depends
from starlette.responses import StreamingResponse

@router.get("/output/stream")
async def stream_response(conversation_id: Optional[str] = None, current_user=Depends(get_current_user)):
    user_id = current_user.id
    # Determine the conversation to stream (use provided or default)
    conv_id = conversation_id or conversation_service.get_default_conversation(user_id)
    # Subscribe to assistant_response events for this conversation only
    queue = event_bus.subscribe(event_type="assistant_response", conversation_id=conv_id)

    async def event_stream():
        try:
            while True:
                # Wait for the next event for this conversation
                event = await queue.get()
                if event is None:
                    break  # End of stream signal if used
                # Format the event as an SSE data frame (text/event-stream format)
                # We will send the whole event as JSON string, or just the content, depending on client expectations
                import json
                data_str = json.dumps(event)
                # Yield in SSE format: "data: <json>\n\n"
                yield f"data: {data_str}\n\n"
        finally:
            # Ensure we unsubscribe to avoid leaking the queue if the loop exits
            event_bus.unsubscribe(queue)

    # Return a streaming response that will iterate over the event_stream generator
    return StreamingResponse(event_stream(), media_type="text/event-stream")
```

What’s happening here:

- We figure out which conversation’s events to send. (`conversation_service.get_default_conversation` is a placeholder for whatever logic picks the right conversation if one isn’t explicitly provided.)
- We call `event_bus.subscribe(event_type="assistant_response", conversation_id=conv_id)`. This filters events to only those that are assistant responses for the given conversation. The returned queue will receive exactly those events (as published by the response handler, per above).
- We define an async generator function `event_stream()` that pulls from the queue and yields events in SSE format. Each SSE message is prefixed with `data: ` and followed by two newlines. We choose to send the entire event JSON in one go (`json.dumps(event)`), which gives the client flexibility to parse it. (Alternatively, we might send only `event["content"]` if we wanted to simplify the client side, but including the full event (type, etc.) is more general.)
- The loop runs indefinitely, waiting for new events. If the queue provides a `None` (which could be used as a sentinel for stream closure), we break out. The loop also will break if the client disconnects; FastAPI/Starlette will cancel the generator, causing us to enter the `finally` block.
- In the `finally` block, we make sure to call `event_bus.unsubscribe(queue)`. This is crucial cleanup: once the client is gone, we no longer need this subscription. Not doing so would leave an idle queue in the Event Bus’s list. By unsubscribing, we allow that entry to be removed and garbage-collected, preventing memory leaks over time.
- We return a `StreamingResponse` that uses our generator. FastAPI will handle sending the response headers and then streaming our yielded chunks to the client. The function itself returns immediately after setting up the streaming, and the actual sending happens asynchronously as events come in.

This pattern shows how **directly** the Event Bus is used in the SSE context. There’s no extra layer – the endpoint code itself subscribes to the bus. This direct usage aligns with our ethos of **“follow direct paths”** and avoids duplicating event distribution logic (we don’t need a separate SSE manager because the Event Bus already does the job of delivering events to the correct subscriber queue). We do note, however, that if more complex SSE connection management were needed (for example, tracking many connections, broadcasting to multiple users, etc.), we might introduce an SSE manager. For now, the combination of `EventBus` + this simple generator is sufficient and clear.

### 4. Other Potential Integrations

While the main use cases for the Event Bus in the MVP are covered above, it’s worth noting that this in-memory pub/sub system can be leveraged by any internal component that needs to react to events without tight coupling. For example, if we had a logging or analytics component, it could subscribe to certain event types to record metrics. Or if we implement a mechanism to push configuration changes, we could use an event to notify parts of the system.

In keeping with the **“present-moment focus”**, we are not implementing any of these hypothetical cases now, but the Event Bus is flexible enough to extend in those directions when needed. Components would follow the same subscribe/publish pattern shown.

## Summary of Responsibilities

To clarify the separation of concerns in the architecture with respect to the Event Bus:

- **Event Bus (this component)**: _Responsible for routing events between producers and consumers in-memory._ It does not alter events, does not persist them, and does not make decisions based on content. It’s essentially a conduit with simple filtering capability. This component should remain stateless beyond holding subscriptions – it doesn’t maintain complex state about conversations or users (that’s for other components).
- **Event Producers**: These include API endpoints (producing events like new user messages) and internal processes (like the response handler producing assistant responses). Their responsibility is to create well-formed event data and publish it. They should not need to know who will handle the event.
- **Event Consumers**: These include the response handler (consuming user messages) and the SSE streamer (consuming assistant responses). Their job is to take appropriate action when an event is received. They use the Event Bus to get events but are otherwise unaware of the producers. For example, the SSE streamer doesn’t care how the assistant response was generated, only that it shows up as an event to send to the client.
- **Integration Points**: The glue code that connects these (like the FastAPI endpoints and background task startup) ensures that subscriptions are created and torn down at the right times. For instance, on app startup we might kick off `run_response_handler()` (perhaps via `asyncio.create_task`), and on each SSE request we manage a subscription. These integration pieces ensure the Event Bus is used correctly but they are not part of the Event Bus itself.

By maintaining these clear roles, the system stays **modular and testable**. We could independently test the Event Bus’s pub/sub behavior, the response handler logic (by feeding it events directly), or the SSE endpoint (by simulating events being published). This modular design is a direct consequence of our event-driven separation.

## Conclusion

The In-Memory Event Bus for Cortex Core MVP is a **lightweight, straightforward implementation** of a pub/sub mechanism tailored to our immediate needs: routing chat messages and responses within a single-process application. It exemplifies the project’s design philosophy by doing **just enough** and no more:

- It uses simple Python constructs (`asyncio.Queue`, lists, dicts) to achieve its goals, avoiding unnecessary complexity.
- It cleanly integrates with FastAPI’s async nature, enabling streaming and background processing without additional threads or external services.
- It leaves room for future growth (like more event types or more sophisticated routing) but does not include those speculatively.

With this guide and implementation, an AI assistant or developer should be able to build the Event Bus component confidently and understand how it fits into the bigger picture of Cortex Core. The component can be developed and reviewed in isolation, then plugged into the system where it will interact with other parts as described. This standalone guide ensures that even with limited context, the implementer can follow the plan and end up with a functioning piece of the MVP that aligns with all the outlined principles.
