# MCP Client Integration for Services

## Overview and Context

The Cortex Core MVP integrates multiple AI **services** using the Model Context Protocol (MCP) to maintain a clear separation of concerns. Each service (e.g. **Memory**, **Domain Expert** modules, and the **Cognition** engine) runs as an independent component, and the core connects to them via an MCP client interface. This approach ensures each domain remains isolated, with minimal coupling between components. The integration is designed with **ruthless simplicity** – there are no unnecessary abstraction layers. Every service is accessed through an explicit interface using MCP over Server-Sent Events (SSE).

By using MCP for all service communications, Cortex Core treats external AI services as standardized **MCP servers**. The core acts as an MCP client that communicates with these servers. This standardized protocol simplifies adding or replacing services without affecting the core logic. Each service focuses on its domain responsibility (for example, the Memory service handles long-term context storage, a Domain Expert might handle factual Q&A or calculations, and the Cognition service handles overall reasoning). The core’s job is simply to route context and commands to the right service and handle responses – **nothing more**. This strict role delineation follows our architectural philosophy of minimalism and domain isolation.

## Using SSE for All Service Connections (No stdio Support)

All MCP connections in Cortex Core use **Server-Sent Events (SSE)** as the transport mechanism. We **drop all support for stdio-based transport** in this integration. SSE offers a robust, network-friendly way to maintain persistent connections with services. It allows services to stream results or updates back to the core as they become available. This means the core can listen for real-time events from each service without constantly polling.

**Why SSE only?** SSE aligns with a cloud-native, decoupled architecture. Each service can run remotely or as a separate process, and the core connects over HTTP. This avoids tight process coupling that stdio (standard input/output pipes) would require. If a particular MCP server was originally designed for stdio, we assume it will be run behind an **MCP proxy** that exposes an SSE endpoint. In practice, this means every service the core talks to will have an HTTP endpoint for SSE (typically an `/sse` URL) and a corresponding endpoint for sending commands (often `/sse/messages`). The core does not spawn subprocesses or use local I/O pipes for services – instead, it treats all services as remote endpoints. This simplifies the deployment and makes the system more flexible (services could be on different machines or containers, for example).

**Important:** Ensure that any references to stdio in configuration or documentation are removed or replaced. The Cortex Core MVP client will not attempt stdio connections. All service integrations assume an HTTP SSE connection. If you have a local service that only supports stdio, run it with an adapter like **mcp-proxy** to convert it to SSE. This uniform approach keeps our code paths simple and avoids conditional logic for different transports. We implement **one transport protocol only**, reducing complexity and potential bugs.

## MCP Client Setup for Service Integration

To integrate a service, the core uses an MCP client instance configured for that service’s SSE endpoint. We will set up a lightweight client for each service with just enough logic to connect, send requests, and handle responses. We avoid any heavy frameworks beyond the official MCP Python SDK (which includes the FastMCP utilities) to keep things simple and transparent.

**General integration flow:**

1. **Initialize an MCP client** for the service’s SSE URL.
2. **Establish the SSE connection** and perform any required handshake or initialization.
3. **Send requests** to the service via the MCP client (using JSON-RPC calls defined by that service).
4. **Receive responses or events** from the service via the SSE stream, and translate them into internal events for the Cortex Core (usually published on the in-memory event bus for other components to consume).
5. **Handle termination** gracefully by closing the SSE connection when the service or core shuts down.

Each service integration will follow this pattern. The code will be organized per service to maintain clarity. We will not create a monolithic “MCP manager” that handles all services together; instead, each service’s client will be isolated (e.g. a `MemoryClient`, `DomainExpertClient`, and `CognitionClient`). This ensures changes to one service’s API or behavior don’t inadvertently affect others.

### Connecting to an SSE Endpoint

The MCP SDK provides convenient tools for connecting to SSE endpoints. For example, using the **FastMCP** (in the official MCP Python SDK), connecting to a service might look like this (in an async context):

```python
from mcp.client.sse import sse_client
from mcp.client import ClientSession

service_url = "http://<service-host>/sse"  # The SSE endpoint of the service
# Create SSE client context (this yields low-level read/write streams)
streams_context = sse_client(url=service_url)
# Open the SSE connection and get the streams (asynchronous context manager)
read_stream, write_stream = await streams_context.__aenter__()

# Create a client session using the streams
session_context = ClientSession(read_stream, write_stream)
service_session = await session_context.__aenter__()

# Perform MCP initialization handshake
await service_session.initialize()

# (Optional) verify connection by listing available tools or resources
tools_response = await service_session.list_tools()
available_tools = [t.name for t in tools_response.tools]
print(f"Connected to {service_url}, tools: {available_tools}")
```

In the above snippet, we use the SDK’s `sse_client` to connect, then wrap it in a `ClientSession` which provides high-level methods like `initialize()` and `list_tools()`. The `initialize` call performs the standard MCP handshake with the server (establishing a session, exchanging any required info like protocol version). The `list_tools` call is part of MCP’s standard API for servers that expose tools – this can confirm the connection and fetch the capabilities of the service.

**Note:** For services that do not conceptually provide “tools” (for instance, a pure memory store might not list tools), the `list_tools` step can be omitted or replaced with another sanity-check call. The key is that after initialization, the client is ready to send whatever requests are needed for that service’s functionality.

### Sending Requests and Receiving Events

Once connected, the core will send requests to the service whenever needed. These requests follow the JSON-RPC 2.0 format defined by MCP. Under the hood, the `service_session` object’s methods (like `call_tool` or any custom method call) will send an HTTP POST to the service’s message endpoint (e.g. `/sse/messages`). Responses or push notifications from the service come back over the SSE stream and are handled by the client library, typically delivered as return values or exceptions from the method call, or via async generator events.

For example, if we want to call a tool on a Domain Expert service (say the service provides a tool named `"lookup_facts"`), we could do:

```python
# Using the same service_session from above, call a specific method or tool
result = await service_session.call_tool("lookup_facts", {"query": "What is the capital of France?"})
if result:
    # Process the result (which might be a list of content pieces or a structured object)
    print("Domain expert response:", result)
```

In this pseudo-code, `call_tool` sends a JSON-RPC request with method `tools/call` and appropriate params. The SDK then waits for the result over SSE and returns it. If the service instead defines a custom method (not using the generic tools interface), the SDK might allow calling it via a generic request function or a generated method. For instance, if the Memory service expects a method named `"storeMemory"` or `"fetchMemory"`, we could send a JSON-RPC notification or request like:

```python
memory_data = {"text": "User mentioned Paris", "timestamp": 1682451200}
await service_session.request("storeMemory", params=memory_data)
```

We favor explicit and specific calls like this in the core code. Each client will have clearly defined methods for the operations it needs from its service. Avoid overly generic call wrappers when a direct method name makes the intention clear. This explicit interface design means if you read the `MemoryClient` code, you immediately see what operations are invoked (storeMemory, fetchMemory, etc.) and the expected data.

**Receiving events:** Some services might asynchronously push events (e.g., a continuous update or a notification). The SSE connection allows the core to handle those in real time. The MCP SDK’s `ClientSession` can be iterated or awaited for such events. For example, if the Memory service broadcasts a `"memoryAdded"` event when new data is stored by someone else, the core’s Memory client could listen:

```python
async for event in service_session.events():
    if event.method == "memoryAdded":
        details = event.params
        handle_memory_added(details)
```

Here, `events()` is a hypothetical async generator yielding JSON-RPC notifications. The core would translate these into its internal event bus (for instance, publishing a `MemoryUpdated` event into the system for any interested component to consume). In our MVP, not all services will necessarily send unsolicited notifications, but the plumbing is in place to support it if needed. The design remains **pragmatic**: implement what we need now (synchronous request/response for known use cases) while supporting extension to more advanced patterns (like streaming results or push notifications) without redesign.

## Service-Specific Integration Details

We now outline specifics for each major service integrated via MCP. The goal is to illustrate any unique considerations and to provide usage examples tailored to each service’s role. Each sub-section will show how the core uses the MCP client to interact with that service, and how it fits into the overall context evaluation cycle.

### Memory Service Client Integration

**Role:** The Memory service retains and retrieves conversational context or other long-term knowledge. It might store summaries of past interactions, key facts, or user preferences that persist across sessions. By isolating this in a separate service, we keep memory management logic out of the core and allow it to scale or evolve independently (for example, swapping out a simple in-memory store for a vector database in the future, without changing core code).

**Interface:** Let’s assume the Memory service exposes two primary RPC methods:

- `store_memory` – to save new information or context.
- `retrieve_memory` – to fetch relevant stored information given the current context or a query.

The core’s Memory client will provide corresponding methods that wrap these RPC calls. For instance, when a new user input arrives, the core might call `store_memory` to log that input or any extracted facts. And before the Cognition service processes a user query, the core might call `retrieve_memory` to gather past relevant info to include in context.

**Example usage:**

```python
# On new user message event:
user_message = {"text": message_text, "timestamp": current_time}
await memory_client.session.request("store_memory", params=user_message)
# (No result expected for a store operation, could be a notification)

# Before processing user message:
query = {"topic": current_topic_or_entities}
memory_results = await memory_client.session.request("retrieve_memory", params=query)
if memory_results:
    # memory_results could contain an array of memory items or text
    context_extension = memory_results.get("memories", [])
    incorporate_into_context(context_extension)
```

In the above pseudo-code, `memory_client.session` is our connected `ClientSession` to the memory service. We directly call `.request("store_memory", params)` with the appropriate data. We use `.request` (which expects a response) or `.notify` (if we fire-and-forget). The retrieved data is then used to enrich the context for cognition. Note how straightforward this is – the Memory client doesn’t do any processing itself, it just shuttles data to/from the Memory service. All business logic about what to store or retrieve (and how) lives in the Memory service or the higher-level orchestrator logic, not in the transport layer.

By keeping the Memory client as a thin wrapper, we adhere to minimal abstraction. There’s no caching or complex logic in the client; it simply forwards requests and returns results. Caching or throttling, if needed, would be a conscious decision and likely handled at a higher layer or within the Memory service itself, to avoid making the client stateful beyond what MCP requires (MCP itself handles session state and context ids).

### Domain Expert Services Client Integration

**Role:** Domain Expert services provide specialized knowledge or perform specific tasks (e.g., a Calculator service, a Factual QA service, a Code Execution service, etc.). In Cortex Core MVP, these are external modules that the Cognition service (or core) can call upon to augment the AI’s capabilities. Each runs as an MCP server exposing one or more **tools** or API methods.

**Interface:** Domain expert services often align naturally with the MCP _tools_ concept. Each distinct capability is a tool. For example, a “WikiSearch” service might have a `search` tool that takes a query and returns summaries. A Calculator service might have an `evaluate_expression` tool. These tools are defined in the service and are discoverable via `list_tools`. The core’s DomainExpert client will typically:

- Connect and call `initialize` (already handled in the general setup).
- Possibly call `list_tools()` at startup to know what capabilities are available (though in a static system, we might know what the service offers in advance).
- When the system (likely the Cognition service) needs to use a tool, the core client will call `call_tool(name, args)` via the MCP session.

**Example usage:**

Suppose we have a Domain Expert service that provides a single tool named `"wiki_search"` for looking up information, and our Cognition logic decides it needs information on “Paris population”. The core (or Cognition service) would do:

```python
tool_name = "wiki_search"
tool_args = {"query": "Paris population"}
result = await domain_expert_client.session.call_tool(tool_name, tool_args)
if result:
    # Assume result is a list of content pieces (as per MCP spec, tools return content arrays)
    content_items = result  # could be TextContent or structured data
    for item in content_items:
        if hasattr(item, "text"):
            print("WikiSearch result snippet:", item.text)
```

This call sends a JSON-RPC request to the Domain Expert service’s `/sse/messages` endpoint. The service performs the search and returns one or more content objects (perhaps text snippets with the information). The MCP SDK likely wraps these in convenient Python classes (e.g., `TextContent` objects). Our client code can handle them appropriately – maybe by forwarding them to the Cognition service or placing them on the event bus for any subscriber (the design will depend on how the Cognition service consumes external info, which we cover shortly).

**Multiple domain services:** If the system has several domain expert services, we will have one client per service. For example, `wiki_search_client`, `calculator_client`, etc. Each will run in parallel as needed. The event-driven nature of the core (with an in-memory event bus) means we might, for instance, publish a `DomainQueryRequest` event that one or more domain services subscribe to. But within this implementation guide, we focus on the direct client calls for clarity. The key is to not hard-code coordination logic in the transport layer; each client just does its job (execute a request and return result). Coordination (like deciding which service to query first or how to merge results) should be handled by the Cognition service or a controller in the core.

By isolating domain expert interactions in their own clients, we fulfill the **explicit interface** principle. Each DomainExpert client class has methods corresponding to that service’s domain actions. This avoids confusion and makes maintenance easier – if the Calculator service API changes, we only update `CalculatorClient` and the rest of the system remains unaffected.

### Cognition Service Client Integration

**Role:** The Cognition service is the central intelligence of Cortex. It observes the overall context (user inputs, outputs from Memory and Domain Experts, etc.) and decides how to respond or act. Unlike other services that simply execute commands, the Cognition service is expected to contain an advanced AI (likely a large language model) that can reason about the inputs and generate an appropriate output or decision.

**Key change in framing:** We no longer treat the Cognition service as a simple request-response engine that must answer every input blindly. Instead, we treat it as a context evaluator. The core will inform the Cognition service of relevant context changes (new user message, new memory retrieved, domain expert findings, etc.) and the Cognition service will internally decide whether to produce a user-facing response, ask a follow-up, or do nothing. For the MVP, to keep behavior predictable, the Cognition service **will generate a response for each new user input** – but we consider that an internal policy of the service, not something enforced by the core integration. This means our integration code should not assume multiple calls or extraneous triggers; it just sends context updates and waits for whatever the Cognition service outputs.

**Interface:** We standardize on an `evaluate_context` method when interacting with the Cognition service. This method encapsulates sending the current context to the Cognition service for evaluation. What constitutes “context” can evolve, but at minimum it will include the latest user input and any additional info the Cognition service should consider (e.g., results from memory and domain experts that the core has collated).

**Example usage:**

```python
# Prepare context payload for cognition
context_payload = {
    "user_input": latest_user_message,
    "memory_snippets": recent_memories,
    "expert_insights": domain_insights
}
response = await cognition_client.session.request("evaluate_context", params=context_payload)
if response:
    # The cognition service decided to output something (as MVP, likely a full response)
    final_message = response.get("message")
    deliver_to_user(final_message)
```

In this pseudo-code, the core gathers relevant pieces (the user’s message, plus any memory or expert info) and sends it via `evaluate_context`. The Cognition service will process this (probably running the big model behind the scenes, possibly using tools via MCP internally) and then return a result. The result could be a direct answer (as in MVP it is), or it could be a decision or some structured action in future. The core does not dictate this; it only carries the information and receives the outcome.

**Internal decision-making:** It’s worth emphasizing that the core does not loop or call `evaluate_context` repeatedly for the same input. It sends one context update per significant event. In the current design, the significant event is typically “the user asked something new,” so it triggers one call. If the Cognition service wanted to wait for a memory update or combine multiple triggers, it would be that service’s responsibility to do so internally (perhaps by not immediately responding until certain criteria are met). For MVP simplicity, our Cognition service will respond immediately to each call, but future iterations might, for instance, sometimes return an empty result or a “waiting” status if it chooses not to act yet. The integration code doesn’t change in that case – we always just send the context and trust the service.

Using `evaluate_context` as the single entrypoint makes the interface very clean. The core is essentially saying: “Here’s the latest state, please assess it.” This aligns with how an AI agent works, considering all available context to decide the next step.

**Cognition SSE specifics:** The Cognition service connection is also over SSE. After calling `evaluate_context`, the response might stream back (for example, if the Cognition service sends a partial or token-streaming reply). The MCP client will handle streaming under the hood. If streaming is enabled, we could iterate over the response in chunks. But since this is MVP and likely using a provider that returns the full message at once, we keep it simple and wait for the final result. The code can be extended to handle streaming if needed by reading incremental `response.content` events from the session.

Finally, once a response is received and delivered to the user, the cycle continues as new inputs arrive. The Cognition client remains connected throughout the session, listening for any other context notifications (though in our design the core explicitly pushes context when needed rather than the Cognition service pulling it).

## Putting It All Together: Event Bus and Flow

_(This section provides a quick recap of how these clients work in concert, ensuring separation of concerns.)_

Within Cortex Core, an **in-memory event bus** broadcasts key events such as “UserMessageReceived”, “MemoryRetrieved”, “DomainExpertResult”, etc. Each MCP client ties into this event system:

- The Memory client subscribes to `UserMessageReceived` events. When it sees a new user message, it calls `store_memory` (as fire-and-forget). It might also subscribe to a `MemoryQueryRequest` event if we explicitly query memory on demand.
- The Domain Expert clients subscribe to events that signal a need for their expertise. For instance, the core (or cognition) might emit `NeedDomainInfo(topic=X)` – or in a simpler design, we automatically query certain experts on each user question. For MVP, one approach is to query all domain experts in parallel for every user question and let the Cognition service figure out what to do with their info. In any case, each expert client takes the event and formulates the appropriate MCP call, then posts the result as a new event like `DomainExpertResult` when it arrives.
- The Cognition client subscribes to everything relevant: definitely `UserMessageReceived`, and possibly also `MemoryRetrieved` and `DomainExpertResult`. Essentially, any change in context triggers the cognition evaluation. In a straightforward MVP flow, we might gather all domain and memory results first, then emit a single `ContextReady` event that contains user input + all gathered info, which the cognition client handles by calling `evaluate_context`. Alternatively, the cognition client might call `evaluate_context` immediately on the user input and the Cognition service itself could invoke tools for memory and knowledge (if it’s designed to do so via MCP calls internally). The architecture allows either strategy. The simplest initial implementation is a linear pipeline:

  1. User message -> Memory + Domain requests in parallel.
  2. Once responses are back -> Cognition evaluate_context with all data.

  But **keep this logic outside the MCP clients**. Perhaps in the event handling code or a small orchestrator in the core, we combine results then call cognition. The MCP clients remain just communication pieces with each service.

This design ensures **strong separation of concerns**: the event bus orchestrator knows the high-level sequence, the clients know how to talk to their service, and each service knows how to perform its task. No layer has to do the other’s job. For example, the Cognition client isn’t making memory calls; the cognition service might do so internally, but from the core’s perspective, those are separate responsibilities.

Throughout the integration, we adhere to the principles from our Implementation Philosophy:

- **Minimal Abstractions:** The code is straightforward. Each service client is a thin wrapper around the MCP SDK calls, with perhaps some convenience methods. We avoid abstracting them behind generic interfaces unnecessarily; if two services require different interactions, we handle them separately.
- **Ruthless Simplicity:** If something isn’t needed for the MVP, it isn’t built. For example, since we decided SSE is the sole transport, we removed all code for stdio fallback, simplifying connection logic. The clients do not implement complex retry logic or state machines beyond what the MCP library provides. They connect and send requests as needed, relying on the underlying SDK (which is well-tested) for low-level details.
- **Domain Isolation:** Memory, Domain Experts, and Cognition each have their domain logic in their own service. The core doesn’t attempt to interpret how memory search works or how the LLM reasoning works – it just passes messages. This keeps our core code focused and easier to maintain.
- **Explicit Interfaces:** As shown, we explicitly call `request("store_memory", ...)` or `call_tool("wiki_search", ...)` or `request("evaluate_context", ...)` rather than burying these behind ambiguous calls. The method names and parameters are clear. New developers reading the code can immediately grasp what is happening at the integration layer.
- **Pragmatism:** We use the official MCP SDK and follow its patterns instead of reinventing the wheel. Our code examples directly use available SDK methods like `initialize()`, `list_tools()`, etc. This not only speeds up development but also ensures compatibility with MCP’s expected behavior. We’re not wrapping these calls in yet another abstraction – that would be redundant. Instead, we might provide small helpers or higher-level logic in the orchestrator, but the integration points themselves remain as simple as possible.

## Additional Code Snippets (FastMCP SDK Usage)

To further illustrate how the AI assistant (Cortex Core) can implement these integrations, here are a few focused examples using the FastMCP-based SDK. These can serve as templates or starting points when writing the actual integration code in the project:

- **General SSE Client Connection (Sync Example):** If the core were to use a synchronous approach (for example, in a separate thread or process per service), we could use the `sseclient` library to listen for events. However, using the official SDK is preferred to handle JSON-RPC seamlessly. Still, a simple sync pattern might look like:

  ```python
  import sseclient, requests, json

  url = "http://service-host/sse"
  # Establish SSE stream
  response = requests.get(url, stream=True)
  client = sseclient.SSEClient(response)
  # Send an initial JSON-RPC message (initialize) via the messages endpoint
  requests.post("http://service-host/sse/messages",
                json={"jsonrpc":"2.0", "id":1, "method":"initialize", "params":{}})

  for event in client.events():
      if event.data:  # JSON content from service
          message = json.loads(event.data)
          print("Received event:", message)
          # ... handle the message ...
  ```

  This low-level example shows the mechanics: GET on /sse to get events, POST to /sse/messages to send commands. In our core implementation, we will rely on the SDK to manage this (as shown in earlier async examples), which is cleaner. But it’s useful to understand what happens under the hood.

- **Using FastMCP to Define Tools (Server side) vs. Client side:** The assistant might be curious how the server defines tools (which can clarify what the client expects). A quick FastMCP server example:

  ```python
  from fastmcp import FastMCP

  mcp = FastMCP("CalculatorService")

  @mcp.tool()
  def evaluate_expression(expression: str) -> float:
      """Evaluate a mathematical expression and return the result."""
      return eval(expression)

  # Running this server (via `fastmcp install`) will expose an SSE endpoint
  # with a 'evaluate_expression' tool.
  ```

  On the client side, after connecting and initializing, calling this tool is as simple as:

  ```python
  result = await calculator_client.session.call_tool("evaluate_expression", {"expression": "2+2*2"})
  # result would be [TextContent(type="text", text="6")] perhaps, or directly a number depending on implementation.
  ```

  This symmetry between server definition and client invocation is why MCP is powerful. It allows the core to remain oblivious to how the tool is implemented (in this case, a Python eval) and just use it via a network call.

- **Error Handling:** While not the main focus of this guide, ensure to handle errors in responses. The SDK will typically raise exceptions if a JSON-RPC error comes back. For example, if the Cognition service returns an error for `evaluate_context` (say it couldn’t process the input), our client should catch that exception, log it, and perhaps try a fallback or at least not crash the whole core. Keep error handling simple and local to the client – e.g., wrap calls in try/except within that service’s client class, convert errors to events or codes that the core can handle gracefully. Do not try to parse or recover from errors beyond logging and notifying; let the service handle its own errors. This again keeps concerns separate.

## Conclusion

By following this guide, the AI assistant implementing the Cortex Core MVP should be able to integrate multiple external AI services cleanly via MCP SSE connections. The emphasis is on clarity and simplicity: each service’s client is straightforward, and their interactions are coordinated through the event bus without implicit magic. This lays a solid foundation for future expansion, where new services can be added by writing a new client in the same pattern, or where services can become more sophisticated without requiring changes to the core (for instance, the Cognition service could start selectively deciding when to respond, and the core wouldn’t need any changes for that to work).

Always refer back to the core principles (minimalism, isolation, explicitness) when in doubt. If a piece of integration logic feels convoluted or overly generic, simplify it – ask if the complexity is truly needed for MVP. In most cases, a direct approach will be preferred and will result in more maintainable code.

---

## Updates to Previous Documentation

To ensure consistency with the above guide, the following updates should be applied to earlier design and implementation documents:

- **Transport Protocol:** Any earlier references to supporting STDIO transport (in architecture or setup documents) should be removed or marked deprecated. All transport discussion should note SSE as the sole method. For example, if _Architecture and Design_ section 3 mentioned stdio, update it to state that services are connected via SSE (HTTP) only.
- **Cognition Service Behavior:** Update descriptions of the Cognition service in the design docs (such as any _Shared Architectural Context_ or _Component Overview_ documents) to reflect that it **evaluates context** rather than simply echoing a response per input. The documentation should clarify that cognition may eventually decide to act or not act on an input, even though in the MVP it still responds every time. Remove any language that implies the core expects a one-to-one response for each message – instead emphasize the core provides context and consumes whatever the cognition outputs.
- **Method Naming:** In any code snippets or API specifications, rename the Cognition service’s invocation method to `evaluate_context` (if it was previously called something like `generate_response` or `process_input`). Ensure consistency in all mentions, so that the integration code and the service API docs use `evaluate_context` uniformly.
- **Event Flow:** Revisit any sequence diagrams or event flow descriptions. Incorporate the pipeline described here (Memory/Domain updates feeding into Cognition’s context evaluation). Make sure it’s clear that the core is orchestrating via events and not by sequentially calling services in a tight loop without the event bus. This may involve adjusting the order or trigger conditions documented in the _In-Memory Event Bus_ section to match the SSE-driven pattern.
- **Code Examples:** Align previous code examples with the new approach. For instance, if an earlier example showed a pseudo-code for handling user input by directly calling cognition and memory sequentially, refactor that example to use the event-driven approach and SSE client calls. Also insert the updated code snippets (or similar ones) provided in this guide into the relevant sections of documentation where developers expect guidance on implementation. This ensures the team working on those parts has a unified reference.
- **Principles Reinforcement:** If not already explicit, add a note in the introduction or philosophy section that simplicity in integration (using one protocol, one method call for cognition, etc.) is a deliberate choice. Emphasize that this decision was made to reduce moving parts in the MVP, which has now been executed by standardizing on SSE and the `evaluate_context` approach.

By applying these updates, all documentation will consistently reflect the refined design: a Cortex Core that connects to its AI services exclusively via SSE-based MCP clients, treating cognition as a context evaluator, and embodying the clean, simple architecture we strive for.
