# Testing and Validation Implementation Guide

## Introduction and Philosophy Alignment

This guide describes how to implement the **Testing and Validation** phase (Project Breakdown section 10) for the Cortex Core MVP. The goal is to ensure the system is reliable and adheres to the project's minimalist architecture. All testing work should follow the same **ruthless simplicity** and **separation of concerns** as the code itself. We will create a lean test suite that validates each component in isolation and the critical end-to-end flows, without introducing unnecessary complexity or test-only code. The AI assistant (or developer) should treat each test area as an independent task, working in isolation with limited context, and ensure each piece fits together through well-defined interfaces (no shared mutable state or cross-coupling between tests).

## Overall Testing Strategy

The testing strategy emphasizes **critical path coverage** and **clear boundaries**:

- **Manual End-to-End Testing First:** Before writing automated tests, run the whole system as a user would. This sanity-checks the primary flow in a real environment and guides what automated tests are crucial.
- **Integration Tests as a Priority:** Focus on tests that cover the main **input → processing → output** pipeline across components. These tests give confidence that the system works as a whole and that our simple interfaces between modules are functioning.
- **Targeted Unit Tests:** Write unit tests for individual components with non-trivial logic (event handling, auth, data parsing). These ensure each part behaves correctly in isolation and help catch edge cases or errors in specific logic.
- **Avoid Over-Engineering:** Keep tests straightforward. Use actual implementations for in-process components where possible (e.g. in-memory event bus or Pydantic models) and only use stubs/mocks for external integrations or unpredictable outputs (e.g. LLM responses, external services). Do not introduce complex test harnesses or unnecessary abstraction layers just for testing.
- **Iterative Improvement:** As you test, if you encounter difficulty due to the code structure, consider simplifying the code rather than adding complex test workarounds. The design should make testing easy; hard-to-test code might need refactoring for clarity. Always choose the simplest fix for any failing test, aligning with our philosophy that simpler code is more maintainable and testable.

## Testing Tools and Setup

We will use familiar, developer-friendly tools to implement the tests:

- **Pytest** for writing and running tests. Pytest is simple, requires minimal boilerplate, and will automatically discover tests named `test_*.py` or `Test*` classes. It also supports fixtures which can help manage setup/teardown if needed.
- **Async Support:** If any code (like FastAPI endpoints or SSE handlers) is asynchronous, include `pytest-asyncio` so we can write `async def` test functions. This allows awaiting asynchronous calls (e.g. calling async endpoints or SSE event consumption).
- **Test Client (FastAPI)**: Use FastAPI’s `TestClient` (for sync tests) or `AsyncClient` from HTTPX for simulating HTTP calls to the API. This lets us exercise the API endpoints in integration tests similarly to real requests, within the test process.
- **Coverage**: Use the pytest coverage plugin (`pytest-cov`) or the built-in `coverage` tool to measure test coverage. This isn’t about chasing 100% coverage, but to ensure critical modules are exercised. We can run tests with `pytest --cov=.` to see overall coverage and identify any major gaps on the core functionality.
- **No Existing Infrastructure Assumed**: We’ll set up everything from scratch in the `tests/` directory. If not already present, add a `tests` folder and create an empty `__init__.py` in it (for Python to recognize it as a package if needed). We’ll also create a `conftest.py` for common test configuration if necessary (for example, defining a pytest fixture for the FastAPI app or an event bus instance if it’s shared). However, avoid global fixtures unless needed—each test should be as self-contained as possible.
- **Dependency Installation**: Ensure dev/test dependencies are listed (in a requirements file or setup config). For example, add Pytest and related tools to the dev dependencies. Once ready, running `python -m pytest` from the project root should collect and execute all tests.

## Test Suite Structure and Conventions

We will organize tests to mirror the project’s vertical slice architecture (API, core, backend, etc.), making it easy to locate tests for each part of the system. A clear structure also helps the AI assistant work on one area at a time. Below is a recommended **folder and file layout** for the test suite:

```
project-root/
├── app/
├── api/
├── core/
├── backend/
├── models/
├── ... (other modules)
└── tests/
    ├── api/              # Tests for API layer (HTTP endpoints, request/response behavior)
    │   ├── test_auth_api.py           # e.g., tests for auth endpoints or auth requirements
    │   ├── test_input_endpoint.py     # tests for the /input route
    │   ├── test_output_endpoint.py    # tests for the /output SSE route (or its handler logic)
    │   └── test_config_endpoints.py   # tests for config CRUD endpoints (/workspace, /conversation)
    ├── core/             # Tests for core internal logic and components
    │   ├── test_eventbus.py           # tests for EventBus publish/subscribe
    │   ├── test_orchestrator.py       # tests for orchestrator logic handling events (may call backend stubs)
    │   ├── test_response_handling.py  # tests for how LLM output is parsed/handled (if not part of orchestrator)
    │   └── test_models.py            # tests for Pydantic models and schema validation (could also be separate models/ folder)
    ├── backend/          # Tests for backend service integration (Memory, Cognition clients)
    │   ├── test_memory_client.py      # tests for Memory service client or adapter
    │   └── test_cognition_client.py   # tests for Cognition/LLM client integration
    └── architecture/     # (Optional) architectural enforcement tests
        └── test_layer_integrity.py    # ensures architecture rules (e.g., no forbidden imports) if needed
```

**Naming and conventions:**

- Test file names start with `test_` and typically include the name of the module or feature they cover. This makes it obvious what’s being tested (e.g., `test_eventbus.py` corresponds to `core/eventbus.py`).
- Test functions within files also start with `test_` and should have descriptive names, e.g., `test_subscribe_filters_events_by_user()` or `test_publish_fanout_to_multiple_subscribers()`. Use one assertable behavior per test function when possible. This yields smaller, focused tests that are easier to debug when failing.
- Keep tests **small and independent**. Each test function should set up its own data and not rely on other tests having run first. For example, if testing the EventBus, instantiate a fresh EventBus in each test rather than using a module-level shared instance that could carry state between tests.
- Use **Arrange-Act-Assert pattern** inside tests for clarity:
  1. _Arrange:_ set up the scenario and inputs,
  2. _Act:_ execute the function or endpoint under test,
  3. _Assert:_ verify the outcome.
     Document any non-obvious context with brief comments or docstrings so an AI reading the test understands the intent without needing external context.
- **Separation by test type**: We are not creating completely separate directories for “unit” vs “integration” vs “e2e” tests. Instead, the separation is logical:
  - Tests in `tests/core` and other internal modules are mostly unit tests (internal logic in isolation).
  - Tests in `tests/api` are mostly integration tests (they test the API endpoints which involve multiple components).
  - End-to-end manual tests are described in documentation (not a separate automated test file). If any fully end-to-end test code is added, it could live in a special file or even as a script, but usually integration tests suffice for CI.
- **Discoverability:** By mirroring the project structure, if the AI assistant works on, say, the `core/eventbus.py` implementation, it can easily locate the corresponding `tests/core/test_eventbus.py` to update or run. This one-to-one mapping of component to test helps working in isolation.

With the structure in place, below we break down the test implementation into three main areas: Unit tests, Integration tests, and End-to-End validation. Each can be tackled independently and later run together as a full suite.

## Unit Tests Implementation

Unit tests target individual functions or classes in isolation. They should run quickly and not depend on external systems or the full application stack. Following the project philosophy, we write unit tests for **core components with logic** while keeping them simple. No elaborate mocking frameworks are needed – use plain Python objects or lightweight stubs to stand in for dependencies if required. Key areas to cover with unit tests include:

- **EventBus Functionality:** The EventBus is central to decoupled communication. We will create `tests/core/test_eventbus.py` to verify that subscribing and publishing works as expected.

  - Test that subscribers only receive events relevant to them (e.g., filtering by user or resource ID). For example, subscribe a handler for `user_id="A"`, publish one event targeted to A and another to B, and assert that the handler only got the A event.
  - Test fan-out to multiple subscribers. Subscribe two different handlers (or two subscribers) for the same filter and ensure both receive the event published for that filter.
  - Test unsubscribing (if the EventBus supports it): subscribe, then unsubscribe and publish an event, ensuring the unsubscribed handler no longer receives events.
  - **Stub Example** – The test can use the real `EventBus` class directly. For instance:

    ```python
    # tests/core/test_eventbus.py
    from core.eventbus import EventBus, Event

    def test_subscribe_filters_by_user():
        """Events are delivered only to subscribers with matching user_id filter."""
        bus = EventBus()
        captured = []
        # Subscribe a simple handler that appends events to 'captured'
        bus.subscribe(user_id="user1", handler=lambda evt: captured.append(evt))
        # Publish two events: one for user1, one for user2
        bus.publish(Event(user_id="user1", data="hello"))
        bus.publish(Event(user_id="user2", data="should be ignored"))
        # Only the event for user1 should be captured
        assert len(captured) == 1
        assert captured[0].data == "hello"
        assert captured[0].user_id == "user1"
    ```

    In this example, the `Event` class would be the event data structure used by EventBus. The test uses a lambda as a stand-in subscriber to record events. This avoids needing any complex mocks and uses the actual implementation.

  - Similar tests can be written for other filtering fields or scenarios (e.g., if EventBus filters by conversation or resource ID). Keep each test focused on one aspect of EventBus behavior.

- **Auth Utilities:** If the project uses an Auth0 integration or JWT verification function (e.g., a `get_current_user` dependency in FastAPI or a helper in `api/auth.py`), we’ll test it in isolation in `tests/api/test_auth_api.py` or a dedicated `tests/core/test_auth_utils.py`.

  - If using a static secret for JWTs in development/testing: create a known valid token (possibly using a library like PyJWT in the test or a hard-coded token generated with the test secret) and ensure the auth function accepts it and returns the expected user identity (e.g., user ID or claims). Also test that an invalid or tampered token triggers the correct error (e.g., raises an `HTTPException` or returns None).
  - If the code calls out to Auth0 or another external service to validate tokens, instead of actually calling the external service in a unit test, simulate the expected responses. For example, if `AuthClient.validate(token)` is normally used, in the unit test you might monkeypatch that method to return a known payload. However, since we prefer not to rely on network calls, the design might already allow injecting a secret or toggling to a test mode.
  - Example (pseudo-code):

    ```python
    # tests/api/test_auth_api.py
    import pytest
    from api.auth import verify_jwt_token, AuthError

    VALID_TOKEN = "<jwt signed with test secret for user_id=123>"
    INVALID_TOKEN = "<some malformed or wrong-signature token>"

    def test_verify_jwt_token_valid():
        user = verify_jwt_token(VALID_TOKEN)
        assert user.id == "123"

    def test_verify_jwt_token_invalid():
        with pytest.raises(AuthError):
            verify_jwt_token(INVALID_TOKEN)
    ```

    In this snippet, `verify_jwt_token` would be a function that decodes a JWT and returns a user or raises an error. We provide pre-made tokens. In practice, generating `VALID_TOKEN` might involve using the same secret as the app uses (perhaps defined in settings). The assistant should ensure the test environment knows the secret (via config or directly in the function if using dependency override). This way, we test the logic without needing the full FastAPI request context.

- **Data Model Validation:** The Pydantic models (defined in `models/` or wherever domain and API schemas are) can be tested to ensure they enforce required fields and types. Since Pydantic is well-tested, we focus only on any **custom logic or constraints** we added.

  - For example, if we have a `Message` model that should require `content` and `conversation_id`, try creating it with missing fields and confirm Pydantic raises a `ValidationError`.
  - If we have any validators (methods with `@validator` in Pydantic models), test their behavior with valid and invalid inputs.
  - Example:

    ```python
    # tests/core/test_models.py
    import pytest
    from models import Message

    def test_message_model_requires_content():
        with pytest.raises(Exception):  # Pydantic raises ValidationError (could import that specifically)
            Message(conversation_id="conv123", content=None)
    def test_message_model_all_fields_ok():
        msg = Message(conversation_id="conv123", content="Hello", sender="user")
        assert msg.content == "Hello"
        assert msg.sender == "user"
    ```

    This ensures our models are not silently accepting bad data. Keep these tests minimal—just enough to catch if someone accidentally changes a field to optional or adds a constraint.

- **LLM Output Parsing (if applicable):** If the project uses structured LLM outputs via Pydantic-AI or similar (for example, parsing a JSON string from the LLM into a `ToolRequest` or `FinalAnswer` model), write unit tests for that parsing logic.
  - For instance, feed a sample JSON that represents a `ToolRequest` to whatever function or model that parses it, and verify it produces the correct model instance. Also test that invalid or differently structured output results in an error or fallback as expected.
  - This can be done by calling the model’s parse method or the helper function we use after getting LLM text. Use a dummy/fake LLM output string in the test (no need to call the actual LLM).
  - If using Pydantic’s `BaseModel.parse_raw` or similar, test those in isolation.

Each unit test module should run fast and not require the app to start or any external services. Use real implementations of our code, and simple stubs for inputs. **Avoid any heavy-duty mocking frameworks or test-specific abstractions**—if a function is hard to test without complex mocks, reconsider if the design can be simplified. For example, if verifying a JWT normally calls an HTTP endpoint, it might be simpler to have a mode where it just uses a secret key, which both the app and tests can use. The overall idea is that our code design already favors straightforward calls, so unit tests can directly invoke those calls with controlled inputs and verify outputs.

## Integration Tests Implementation

Integration tests ensure that components work together correctly through the main flows of the application. We will simulate actual API calls and event flows using the real FastAPI app and internal components, but we’ll stub out external dependencies (like network calls to Auth0, Memory service, or the LLM) to keep tests deterministic and fast. The aim is to cover the **critical end-to-end logic** within the system boundary (from receiving an input request to producing an assistant response event). Key integration test scenarios include:

- **End-to-End Message Flow (Input to Output):** Create a test that simulates a user sending a message and receiving the assistant’s response. This will involve multiple parts:

  1. **Setup test doubles:** Use FastAPI’s dependency override system to inject fake implementations for anything that would hit external services. For example:
     - Override the authentication dependency to accept any token and return a test user (so we don’t need real JWT verification here, since that is tested in unit tests). If our `/input` endpoint depends on `get_current_user`, override it to return a `User(id="testuser", ...)` object unconditionally.
     - Override the Memory service client call to return a preset conversation history. For instance, if the orchestrator calls `memory_client.get_history(conversation_id)`, override that function or dependency to return a fixed list of messages (or empty list if simulating no history).
     - Override the LLM or cognition service call to return a fixed response. For example, if there’s a function `llm_client.generate_reply(prompt)` or similar, replace it with a dummy function that returns a static assistant answer like "Hello, this is a test response.".
  2. **Perform the input request:** Using the FastAPI test client, send a POST request to the `/input` endpoint with a sample JSON payload, e.g. `{"content": "Hello"}` (and include the auth header if required, e.g. `Authorization: Bearer <dummy_token>`). Verify that the response status is 200 OK and maybe the response body is the acknowledgment (if the endpoint returns something like a message ID or just an OK).
  3. **Trigger response processing:** In a real running system, the input would cause an event that the backend processes asynchronously, then an SSE event is emitted on `/output/stream`. In a test environment, we have two ways to simulate this:
     - **Option 1:** Directly call the event handling function in the test. If the input endpoint published an event to an `EventBus` (or similar) and returned immediately, our test can now manually invoke the function that processes events (let’s say `core.orchestrator.handle_event(event)`). Construct an event object similar to what the input would have published (including the user/workspace/conversation info and message content), and call the handler. This should perform the sequence: call memory client (stubbed), call LLM (stubbed), then publish an output event via EventBus.
     - **Option 2:** Use the EventBus to capture the output. If we prefer not to call the handler directly, we can subscribe to the EventBus for output events in the test. For example, subscribe with the test user’s ID or conversation ID _before_ sending the input. Then after the input POST (which put an event on the bus), either trigger the orchestrator (if it doesn’t run automatically in test), or if the orchestrator is running in the same thread (unlikely without an async background task), you might wait briefly. Most likely we will do manual trigger as in Option 1 for simplicity.
  4. **Verify the output event:** Check that the EventBus now contains or has emitted an event for the assistant’s reply. If we used a subscriber approach, the subscriber callback would have been called. For example, our test can subscribe a dummy handler to EventBus that appends events to a list (similar to unit test) before sending input, then after calling the handler, check that the list contains one event which is an output message event. The content of that event’s data should match the dummy LLM response we configured. If the design uses an SSE endpoint reading from EventBus, we essentially confirm the correct data made it onto the EventBus.
  5. This test effectively covers the **critical path**: API input → internal processing (memory + LLM) → event output. We use real internal logic (event publishing, orchestrator code) combined with stubbed external calls. There is no need for shared memory or a full orchestrator service running – the test orchestrates the calls in a controlled order.

  For example, an integration test function outline might look like:

  ```python
  # tests/api/test_input_output.py (or tests/core/test_orchestrator.py)
  from fastapi.testclient import TestClient
  from app.main import app
  from core import orchestrator
  from core.eventbus import Event, EventBus

  def test_full_message_flow(monkeypatch):
      # Arrange: override dependencies
      app.dependency_overrides[get_current_user] = lambda: User(id="testuser")  # bypass auth
      app.dependency_overrides[get_memory_client] = lambda: DummyMemoryClient() # uses dummy data
      app.dependency_overrides[get_llm_client] = lambda: DummyLLMClient()       # returns dummy response

      client = TestClient(app)
      # Also prepare to capture output events
      output_events = []
      EventBus.get_global().subscribe(user_id="testuser", handler=lambda evt: output_events.append(evt))

      # Act: send input request
      resp = client.post("/input", json={"content": "Hello"})
      assert resp.status_code == 200

      # Act: simulate background processing (directly call handler for test)
      # Assuming orchestrator.handle_event processes the last published event or accepts an event:
      event = ...  # retrieve the event that /input would have created (maybe return from resp.json or inspect EventBus)
      orchestrator.handle_event(event)

      # Assert: an output event was published
      assert len(output_events) == 1
      out_evt = output_events[0]
      assert out_evt.user_id == "testuser"
      assert "Hello, this is a test response" in out_evt.data["content"]
  ```

  In this pseudo-code, `DummyMemoryClient` and `DummyLLMClient` are simple stub classes defined in the test (or in a test utilities module) that implement the same interface as the real clients but with hard-coded behavior. For example, `DummyMemoryClient.get_history(conv_id)` could return a static list of previous messages (or empty list), and `DummyLLMClient.generate_reply(prompt)` could return a predetermined string. We also override `get_current_user` to avoid dealing with real JWTs. The event capturing uses a hypothetical `EventBus.get_global()` if EventBus is a singleton or has a global instance. If not, we might use the same EventBus instance that the orchestrator uses (perhaps imported from the app). The specifics depend on implementation, but the pattern is: **use dependency injection or monkeypatching to intercept external calls, then drive the request through FastAPI and the orchestrator, and finally verify outcomes.**

- **Authentication and Authorization Integration:** Test that protected endpoints actually enforce authentication. For example, using the TestClient:

  - Call the `/input` endpoint **without** an `Authorization` header and assert that the response status is 401 Unauthorized (or whatever the app is configured to return for missing/invalid auth).
  - Call with an invalid token (if the auth logic is local and can detect a bad token) and ensure it also returns 401/403.
  - These tests verify that the FastAPI dependency (e.g., `Depends(get_current_user)`) is wired up. Since in our main flow test we overrode get_current_user to always succeed, we do a separate test for the auth system. For this, we might not override the auth dependency so it uses the real logic with our test secret. We can supply a token header using a token created in the auth unit test. Alternatively, test the dependency function directly by calling it with a `Request` containing a header. But simplest is to do:
    ```python
    resp = client.post("/input", json={"content": "Hi"})  # no auth header
    assert resp.status_code == 401
    ```
    If the framework raises an exception for missing auth, FastAPI will turn that into a 401. Similarly, if we want to test a valid token end-to-end, we could generate a token (with test secret) and include it:
    ```python
    client.headers.update({"Authorization": f"Bearer {VALID_TOKEN}"})
    resp = client.post("/input", json={"content": "Hi"})
    assert resp.status_code == 200
    ```
    This ensures the dependency allows a real token. However, since token validation logic was already unit-tested, it’s acceptable in integration to simply bypass it via overrides except one or two checks like above.

- **Configuration Endpoints (Workspace/Conversation CRUD):** Write tests for any simple CRUD endpoints (if implemented, e.g., `POST /workspace`, `POST /conversation`, etc., from previous project sections).

  - These are typically straightforward since they might just store data in memory or return static responses. Use the TestClient to call them and verify that the correct status and response body is returned.
  - For instance, a `POST /workspace` might return `{"id": "...", "name": "...", ...}`. The test can assert that an ID is present and matches the input name, etc., depending on the logic.
  - If these endpoints require auth, either override the auth dependency or include a token as done above.
  - Also test edge cases like missing required field returns a 422 (FastAPI does this automatically via Pydantic validation). You can intentionally omit a field in the JSON to see the validation error structure. This ensures our Pydantic models are integrated correctly with FastAPI.
  - Because these endpoints likely use in-memory storage (like a global list or dict) for the MVP, tests might share state if not careful. It’s best to reset or isolate state:
    - If using a global list for created workspaces, one test’s creation might remain in memory for the next test. To avoid interdependence, reinitialize or clear that structure at the start of the test. This can be done by having the app expose a dependency for the storage that we override in tests (similar to others), or by calling a clear function if provided in code (perhaps not in production code, but maybe a helper only tests use). If no easy hook, design the tests to use unique IDs and at least not assume a clean slate (but isolation is ideal).
    - Alternatively, consider using `monkeypatch` to stub out any module-level storage with a fresh object for each test.

- **Multi-Component Interaction (if any):** Aside from the main message flow, if there are other interactions to test, do so here. For example, if the orchestrator has a function that calls both Memory and LLM in sequence, we could write an integration test for that function alone (without going through HTTP). However, likely the main orchestrator function was already invoked in the end-to-end flow test. So we might be covered. Another potential integration test could be to simulate two different users or workspaces to ensure events segregate properly (like user A doesn’t get user B’s message). This could involve:
  - Subscribing two handlers for two different user IDs, publishing events for each user, and verifying isolation. (But this is very similar to the EventBus unit tests—so it might be redundant to do at the integration level unless done via the API to ensure, say, if two users call /input, only their SSE gets their response.)

For all integration tests, **use the real FastAPI app and internal logic** but **replace external interactions**:

- Use **FastAPI dependency overrides** to supply dummy implementations for: authentication, Memory service, Cognition/LLM service, and possibly database or configuration storage if those are abstracted. This is preferred over monkeypatching internal functions because FastAPI provides a clean mechanism for swapping out dependencies during tests.
- If some dependency isn’t easily override-able (for example, if the code directly calls `requests.get()` to an external service or directly instantiates a client inside a function), you may need to use `monkeypatch` (from Pytest) to intercept that call. For instance, monkeypatch the `requests.get` or the specific function call to return a predefined response. Keep such monkeypatch usage localized in the test function via the `monkeypatch` fixture (as in the example above). **After the test, everything should be restored** to avoid side effects.
- **No shared state between tests:** Be mindful of any global singletons (like a global EventBus or global config). If the app uses a global EventBus, ensure each test either uses a fresh instance or cleans subscribers. One approach is to design the EventBus with a `.clear()` method for tests, or instantiate a new one for testing (maybe via dependency injection). If needed, set `EventBus._subscribers = {}` at the end of a test (not pretty, but effective). Ideally, the app should allow providing a test instance (e.g., via `app.dependency_overrides` if EventBus is injected), but if not, manual cleanup is fine.
- The integration tests essentially **stitch together unit-tested components** and ensure the wiring between them (via events, function calls, and FastAPI routes) works. We should aim for a few high-value integration tests rather than exhaustive combinations. The critical one is the full message pipeline; others (auth enforcement, config CRUD) are supplementary but still important for a complete validation of the MVP.

## End-to-End Validation (Manual Testing)

After writing unit and integration tests, it’s important to perform a manual end-to-end test of the running system. This step ensures that nothing is misconfigured in the deployment or environment and that real interactions behave as expected, especially around asynchronous flows and network boundaries which automated tests might simulate or shortcut. An AI assistant might not execute these steps itself, but it should prompt a human collaborator to run them or at least validate logically that the flow would work. The manual testing process should cover:

- **Startup and Environment:** Launch the entire Cortex Core FastAPI service (e.g., `uvicorn app.main:app`) along with any required auxiliary services. For the MVP, that likely means ensuring the **Memory service** and **Cognition/LLM service** (if separate) are running and accessible. If they are simple Python processes or mocked during development, start those too. Check logs to see that all services start without errors.
- **Authentication (if enabled):** Obtain a JWT for testing. If an `/auth/login` endpoint exists and is implemented, use it to get a token by providing test user credentials. If no auth endpoint, use a workaround:
  - Either disable authentication checks in the dev environment (e.g., configure the app to skip auth for testing),
  - Or generate a JWT manually using the known secret key (if using a custom auth) and include the necessary claims (like user ID, etc.). This could be done with a tool or an online JWT generator if the secret is known.
  - In any case, you need a valid `Authorization: Bearer <token>` to call protected endpoints in the next steps.
- **Sending an Input Message:** Using a tool like `curl`, HTTPie, or a REST client, send a POST request to the `/input` endpoint with a sample message. For example:
  ```bash
  curl -H "Authorization: Bearer $TOKEN" \
       -H "Content-Type: application/json" \
       -d '{"content": "Hello"}' \
       http://localhost:8000/input
  ```
  Expect a `200 OK` response. The body might be an acknowledgment (depending on implementation, maybe something like `{"status": "received"}` or just an empty success). If you receive an error or non-200, note the response and check the server logs for any exceptions (this might indicate an issue that our tests didn’t catch).
- **Receiving the Output (Stream):** Open a connection to the SSE output stream endpoint, typically `/output/stream`. You can do this in one of several ways:
  - Easiest might be to open a web browser at `http://localhost:8000/output/stream?token=<TOKEN>` if the endpoint reads auth from query (or if you include the Authorization header via a tool). Browsers can render SSE streams.
  - Or use `curl` with the `-N` flag (to not buffer output) to connect:
    ```bash
    curl -H "Authorization: Bearer $TOKEN" http://localhost:8000/output/stream
    ```
    This should hang and wait, printing events as they arrive.
  - Or use a specialized SSE client script or library.
    Once connected, you should see an event come through after you send the input (the assistant’s reply). It might look like a line starting with `data: ` followed by the JSON payload of the message. Verify the content is the dummy or actual LLM response. **Note:** If using the real LLM service, you’ll get whatever it responds with (which might be non-deterministic). In a dev/test scenario, consider configuring the cognition service to a deterministic mode or a known prompt->response mapping.
- **Edge Cases:** While you have the system up, try a few incorrect scenarios:
  - Call the `/input` without a token (or with an invalid token) to ensure you get a 401 Unauthorized and a meaningful error message.
  - If your system supports multiple workspaces or conversations, test that:
    - You can create a new workspace via the API (`POST /workspace` if available) and get a proper response.
    - You can then create a conversation in that workspace (`POST /conversation` with the workspace id).
    - Then send an `/input` message with that conversation id (if the design requires linking input to a conversation) and ensure it still works (i.e., uses the new conversation context). This might be as simple as including `conversation_id` in the JSON if that’s part of the input schema.
    - These steps ensure that the multi-tenant or multi-session aspects are functioning.
  - Try sending a larger or more complex input to the assistant (maybe a longer sentence or a question) and see that it still responds, just to cover that nothing breaks with different content.
- **Observability:** Watch the logs from the services (Cortex Core app, Memory service, LLM service) during these tests. Look for any error tracebacks or warnings that our tests might not have covered. For example, an SSE connection error, or an unexpected missing key in an event. If any are found, we should go back and add a test for that scenario or adjust the implementation to handle it.
- **Cleanup:** Once done, you can stop the services. We have now manually verified the entire flow.

This manual validation step is important to ensure that the system truly works as a cohesive product in a real environment. It complements the automated tests: if any discrepancy is found (for example, the SSE stream doesn’t send events as expected), we should update our code and also consider adding or refining an automated test for it if possible. The philosophy of **“test as a user would use it”** is upheld here: we confirm the MVP provides the intended behavior end-to-end.

_(Note: If desired, some of these end-to-end checks can be automated with an **end-to-end test script**, such as spinning up the FastAPI server in a thread and using an SSE client in the test. However, that introduces complexity and is brittle in CI. Given our goal of minimalism, manual testing is acceptable at this stage for final verification, while integration tests handle the logic in isolation.)_

## Dependency Stubbing and Use of Mocks vs Real Components

Throughout our testing approach, we carefully choose where to use real implementations and where to use fakes or mocks, to balance thoroughness with simplicity:

- **Real In-Memory Components:** Use the actual implementations for anything that runs in-memory and is fast:
  - The EventBus class is used directly in tests (real implementation) since it doesn’t depend on external resources. This ensures we test the actual behavior.
  - Pydantic models and schema validation – use them directly; they’re reliable and have no side effects.
  - FastAPI app and route logic – we run the real app in testing (via TestClient) to exercise actual request handling, dependency injection, and response generation.
- **Stubs for External Services:** Replace calls that go out of our process or to nondeterministic services:
  - **LLM/Cognition Service:** Never call the real LLM in tests. Its response could vary and slow the tests. Instead, stub it with a predictable implementation. This can be as simple as a function or dummy class that returns a constant string or a trivial transformation of the prompt. The goal is to simulate a typical valid response (so the rest of the system can parse it) without any external dependency.
  - **Memory Service:** Similarly, do not call an actual memory service or database in tests. If the memory service is an external HTTP call or database query, stub it. If the memory is just an in-process store for MVP, you can use it (just ensure it’s empty/reset for each test). But likely memory is an external microservice (as per architecture). So use a dummy memory client in tests that returns a fixed set of messages for any conversation (or perhaps keyed by conversation id if needed).
  - **Auth Service:** If using Auth0 or another external auth, do not call it in tests. Instead:
    - In unit tests for auth logic, simulate the expected token verification outcome (e.g., decode a JWT using a test secret or monkeypatch the Auth0 SDK method).
    - In integration tests for endpoints, override the auth dependency entirely to bypass real verification (treating any token as valid and mapping it to a test user). This isolates testing of business logic from the third-party auth integration.
  - **Outbound Networking:** If any part of the code makes HTTP requests (via `requests`, `httpx`, etc.), always intercept those in tests. Use monkeypatch to replace the specific call with a lambda or fake function that returns a prepared `Response` object or data. For example, if the code calls `requests.post("https://memory.service/...", json=...)`, monkeypatch `requests.post` in the test to just return an object with `.json()` method returning a preset payload.
- **Minimal Mocking Frameworks:** We rely on manual stubs and monkeypatching rather than heavy mocking libraries. This is in line with minimal dependencies and clarity. The fake classes/functions we introduce in tests (like `DummyLLMClient`) are simple and defined in the test itself or a local test utility module. This way, the assistant or developer can easily see what the stub does by reading the test code (no magic). For example, `DummyLLMClient.generate_reply(prompt)` could just return `f"Echo: {prompt}"` or a fixed string; it's easy to implement and understand.
- **No Test-Specific API in Production Code:** We avoid adding methods or branches in the production code just to facilitate testing. Instead, we use the existing extension points (dependency injection, configuration, etc.). For instance, rather than modifying the orchestrator to accept a “mode” flag for testing, we simply override the dependencies it uses. The one exception might be adding small helper hooks that do not affect normal operation, such as a method to reset in-memory stores between tests or retrieving an event from the EventBus for verification. Even then, prefer to do these resets from the test side if possible (like re-initializing a global object). Each test should treat the system as black-box as possible, configuring it via public interfaces (DI, function parameters). This keeps the boundary between test code and app code clear.
- **Global vs Instance Components:** If a component is designed as a singleton (e.g., a global event bus or a global config), be mindful in tests. It may be acceptable to use a global in production for simplicity, but in tests you might reinitialize it. For example, if `core.eventbus` module has a global `event_bus = EventBus()`, you might reassign `event_bus = EventBus()` in the test to start fresh, then restore it after. Alternatively, if the design permits, create a new EventBus instance in tests and have the code use that (via dependency override or passing it into functions). Adjust as needed to ensure tests don’t contaminate each other.

By following these guidelines, tests remain reliable and deterministic. They won’t fail intermittently due to calling external services, and they won’t pass or fail depending on the order tests run (since we isolate state). This approach also reflects **clear separation of concerns**: tests of our core logic use real core code, but do not entangle with external systems or require complex scaffolding.

## Continuous Integration and Automation

To prevent regressions, it’s important to automate running the test suite regularly (for example, on every commit or pull request). Setting up a lean Continuous Integration (CI) workflow will ensure that if an AI assistant or developer introduces a bug, tests catch it early. Here’s a pragmatic approach to CI for this project:

- **Use a Simple Test Workflow:** If using GitHub, set up a GitHub Actions workflow (e.g., `.github/workflows/tests.yml`) that uses the official Python setup. This workflow can be very straightforward:
  1. Checkout the repository.
  2. Set up Python (specify the version consistent with your project, e.g., 3.10).
  3. Install dependencies. For example:
     ```yaml
     - uses: actions/checkout@v3
     - uses: actions/setup-python@v4
       with:
         python-version: "3.10"
     - name: Install dependencies
       run: pip install -e ".[dev]"
     ```
     The `-e ".[dev]"` assumes your project’s `setup.py/setup.cfg/pyproject.toml` is configured with a `dev` extra that includes pytest and any test tools. Alternatively, explicitly install pytest and others.
  4. Run tests:
     ```yaml
     - name: Run tests
       run: pytest --maxfail=1 --disable-warnings -q
     ```
     (Using options like `--maxfail=1` and `-q` for quick feedback and cleaner output is optional.)
  5. Optionally, run coverage:
     ```yaml
     - name: Run tests with coverage
       run: pytest --cov=core --cov=api --cov-report=xml
     ```
     And then use a coverage action to report it, or simply print the coverage to console. Focusing coverage on our `core` and `api` modules can give a clearer picture of core logic tested.
- **Run CI on Every Push/PR:** This ensures that any change triggers the test suite. For an AI assistant contributing code, it means the assistant (or user) will get immediate feedback if tests fail, reinforcing the quality.
- **Keep CI Lean:** We are not deploying anywhere yet, so the CI doesn’t need to build containers or run multi-step releases. Just installing dependencies and running tests is enough. The whole suite should run quickly (likely within seconds or a minute) since external calls are stubbed.
- **Include Architecture Checks:** If we have an **architecture test** (like `tests/architecture/test_layer_integrity.py` or a `check_imports.sh` script to ensure layer boundaries), include that in the CI run. For instance, if `check_imports.sh` returns non-zero when a violation is found, run it as another step:
  ```yaml
  - name: Check Layer Integrity
    run: ./check_imports.sh
  ```
  This will catch any accidental cross-layer imports or other architectural rule breaches automatically.
- **Linters and Formatters:** While not strictly part of testing, enforcing code style helps maintain simplicity. If using tools like `ruff` or `black` (as suggested by the dev guide), the CI can also run these (or at least run `ruff --check` and `black --check` to ensure no lint errors and consistent formatting).
- **Fail Fast Philosophy:** The CI should fail on the first sign of trouble (test failure or lint error) to get feedback quickly. This aligns with failing fast in development as well.
- **Badge and Reporting:** If this project is on a platform where you can add a badge (for test passing or coverage), you might set that up to keep visibility. But the primary goal is just to automatically validate correctness.
- **No Flaky Tests:** Because we avoid external dependencies in tests, we expect consistent results. Make sure any randomness (e.g., UUIDs in responses) is handled in assertions (perhaps by not comparing exact values where nondeterministic, or by seeding random generators if needed). The CI should be as green as your local runs.

By incorporating this basic CI pipeline, every change will be validated against the test suite, preventing regressions from creeping in. The AI assistant can rely on the feedback from these automated runs to know if a newly written piece of code has broken something else, all without a complex orchestration. In summary, the CI setup is the safety net that continuously enforces the correctness established by our tests, allowing rapid yet safe iteration on the Cortex Core codebase.

## Conclusion and Next Steps

Following this guide, an AI assistant (or any developer) can implement the **Testing and Validation** mini-project in a methodical, isolated way. We started with a high-level test strategy rooted in the project's minimalistic philosophy, then drilled down into concrete steps for unit tests, integration tests, and manual end-to-end checks. Each test module and scenario outlined above can be implemented independently, but together they ensure comprehensive coverage of the Cortex Core’s functionality:

- The **unit tests** guard the correctness of fundamental pieces (event bus, models, auth logic, parsing).
- The **integration tests** ensure the main user journey and interactions between components work (handling input through to producing output, under proper authentication and with external services abstracted).
- The **manual testing** confirms that the deployed system behaves as expected in the real world.
- The careful use of **test doubles vs real components** keeps tests reliable and simple, reinforcing the design’s separation of concerns.
- The **test organization** mirrors the code structure, enabling parallel development and clarity, which is especially useful for an AI working with limited context.
- Finally, the **CI setup** automates validation, aligning with the goal of maintaining a bug-resistant implementation.

With this testing foundation in place, the Cortex Core MVP will be validated to meet its requirements. The test suite itself should remain simple and maintainable, reflecting the overall philosophy: no superfluous abstractions, just focused, clear validation of behavior. Going forward, any new feature or refactor should be accompanied by corresponding tests as per these conventions, ensuring the core remains robust as it evolves.
