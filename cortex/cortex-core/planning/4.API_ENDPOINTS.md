# Cortex Core MVP – API Endpoints Implementation Guide (Input, Output, Config)

## Overview

This guide outlines how to implement the **API Endpoints** portion of the Cortex Core MVP, focusing on the Input, Output, and Config endpoints. These endpoints form the external interface of the AI assistant, handling incoming messages, streaming outgoing responses, and basic configuration (workspaces and conversations). The implementation is designed so an AI assistant developer can work **in isolation**, with clear separation of concerns and modular design. Each endpoint will be a thin FastAPI handler that validates input (using Pydantic models), delegates business logic to internal services or an EventBus, and enforces authentication via a dependency. By following this guide, the endpoints will slot cleanly into the overall architecture without bleeding into other system components.

**Scope of Endpoints**:

- **Input Endpoint** – `POST /input`: Accepts user messages (with minimal metadata) and acknowledges receipt.
- **Output Endpoint** – `GET /output/stream`: Streams responses to the client via Server-Sent Events (SSE).
- **Config Endpoints** – `POST`/`GET /config/workspaces` and `POST`/`GET /config/conversations`: Manage conversational contexts (create/list workspaces and conversations).

All endpoints will require a valid user JWT (except any auth-specific endpoints, which are out of scope here). The design ensures these API handlers remain **ultra-thin** wrappers: they perform request parsing and response formatting only, deferring all processing to the appropriate internal layers.

## Design Principles and Separation of Concerns

Our implementation follows the project’s architectural and implementation philosophy closely, emphasizing simplicity and clear boundaries:

- **Ruthless Simplicity**: Each endpoint does only what is absolutely necessary for the API layer – no additional logic or premature abstractions. We use FastAPI and Pydantic directly (no custom frameworks) to keep things as simple as possible. Validation and error handling leverage FastAPI/Pydantic’s built-in features, producing clear error responses automatically for invalid input.
- **Well-Defined Boundaries**: The API layer is isolated from core business logic. Endpoints will **not** perform processing like storing data or generating responses themselves. Instead, they delegate those tasks to internal services (e.g. via a `workspace_service` for storing a new workspace, or by publishing an event to an internal `EventBus` for processing a message). This separation means the API module simply coordinates with other components, and different teams/agents can work on services or event handling in parallel.
- **Clear Modularity**: We implement separate FastAPI router modules for each endpoint group (Input, Output, Config). This modular design (one router for chat input, one for output streaming, one for config management) keeps concerns separated and files manageable. Each router can be included in the main application independently.
- **Consistency and Convention**: All endpoints follow a consistent pattern – use of `APIRouter`, dependency injection for auth, Pydantic models for request/response, and returning standardized responses. They share common behaviors (like auth enforcement and request parsing), which makes the overall API predictable.
- **Statelessness and Security**: Endpoints rely on token-based authentication. We use an injected FastAPI dependency (e.g. `Depends(get_current_user)`) to automatically verify JWTs on each request. No session state is kept in the API layer; if authentication fails, the dependency will raise an `HTTPException(401)` and the request is aborted. This keeps the API stateless and secure by default.
- **One-Way Streaming for Output**: The design uses **Server-Sent Events (SSE)** for the output stream endpoint, which provides a unidirectional, event-driven channel from server to client. Only the output endpoint uses streaming; all others are standard request/response. This fits the architecture of separate input and output channels and avoids the complexity of maintaining WebSocket connections. The SSE implementation will be kept minimal (using FastAPI’s streaming response capability) and integrated with the internal EventBus for real-time messages.

These principles align with the project’s **implementation philosophy** – keeping the code minimal yet structured, focusing on current requirements, and trusting simple mechanisms (FastAPI, Pydantic, in-memory stores) to do their job. The result is an API layer that is easy to understand, easy to test, and adaptable as other system components evolve.

## Project Structure and Module Organization

To maintain clean separation, we will organize the API code into dedicated modules and use Pydantic models for data schemas. Below is an outline of relevant modules in the project structure:

```
cortex-core/
├── app/
│   ├── api/
│   │   ├── input.py          # Router for POST /input (send message)
│   │   ├── output.py         # Router for GET /output/stream (receive SSE)
│   │   └── config.py         # Router for workspace & conversation endpoints
│   ├── models/
│   │   ├── api/
│   │   │   ├── request.py    # Pydantic request models (InputMessage, WorkspaceCreate, ConversationCreate, etc.)
│   │   │   └── response.py   # Pydantic response models (Workspace, Conversation, etc.)
│   │   └── domain/           # Domain models (if any, e.g., for internal use)
│   └── services/
│       ├── event_bus.py          # EventBus implementation (pub/sub for events)
│       ├── workspace_service.py  # Service or repository for workspaces (in-memory for MVP)
│       ├── conversation_service.py # Service for conversations (in-memory for MVP)
│       └── memory_client.py      # Client for Memory Service (via MCP or stub)
├── tests/
│   └── api/
│       ├── test_input_endpoint.py
│       ├── test_output_endpoint.py
│       └── test_config_endpoints.py
└── ... (other components such as auth, core orchestrator, etc.)
```

Each API module defines an **APIRouter** with its endpoints. In the main application (e.g., `app/main.py` or equivalent), these routers will be included and mounted (e.g., `app.include_router(input_router)`, etc.). This structure allows the API layer to be developed and maintained independently from the internal logic. The Pydantic models in `models/api` define the expected request bodies and response schemas, ensuring type safety and input validation automatically. Internal services (in `app/services/`) handle actual data operations or event publishing, so the API endpoints only make calls to those services.

With this setup, an AI assistant can implement the Input, Output, and Config endpoints without needing to know the detailed internals of event processing or database storage – those are abstracted behind service interfaces and the EventBus. The assistant can focus on the API contract (routes, request/response format, and delegation calls).

---

## Input Endpoint – POST `/input` (Send User Message)

**Purpose**: The Input endpoint receives a user’s message and initiates processing of that message within the Cortex Core system. It does **not** produce an immediate answer (that will come asynchronously via the output stream). Instead, it validates and acknowledges the input, then hands off the message to the core orchestrator (via events or service calls).

**Route Definition**: `POST /input` (e.g., part of a `chat_router` or standalone input router). This is a protected endpoint – it requires an authenticated user.

**Request Body**: JSON with the user's message and context. We define a Pydantic model `InputMessage` for the request:

```python
# In app/models/api/request.py
from pydantic import BaseModel
from typing import Optional

class InputMessage(BaseModel):
    content: str
    conversation_id: Optional[str] = None
```

- `content` (str): The text of the user’s message (required).
- `conversation_id` (Optional[str]): An identifier for the conversation context. If provided, the message is part of that conversation. If omitted or null, the system will use a default conversation for the user (for MVP, we assume each user has at least one active conversation by default, simplifying context management).

Pydantic will automatically validate that `content` is present and is a string. If the JSON payload is missing required fields or has type issues, FastAPI will return a **422 Unprocessable Entity** with details, without any custom code needed.

**Authentication**: The endpoint will use a dependency like `Depends(get_current_user)` to enforce JWT auth. The `get_current_user` dependency (implemented elsewhere) will parse and verify the `Authorization: Bearer <token>` header. If the token is valid, it returns the current user (e.g., a `User` object or dict with user info); if invalid, FastAPI throws an HTTP 401 error automatically. This means inside our endpoint function, we can assume we have a `current_user` available and authenticated.

**Processing Flow**: The implementation of the POST /input endpoint will follow these steps:

1. **Request Validation** – FastAPI reads the JSON body and uses `InputMessage` to parse and validate it. The endpoint function signature will include an `input_msg: InputMessage` parameter to receive the parsed data. If validation fails, the request will never reach our function (FastAPI will handle returning an error), keeping our code clean.
2. **Determine Conversation Context** – Using the data and current user, determine the target conversation. If `input_msg.conversation_id` is provided, we use it. If not, we fall back to a default conversation for that user. (For example, the system might have created a default conversation in a default workspace upon user registration or first request. In this MVP, we can simplify by using a single conversation per user if none is specified.) This logic can be delegated to a conversation service or a simple helper function (to keep the endpoint code short).
3. **Publish Event to EventBus** – We create an event representing the new user message and publish it to the internal EventBus. For instance, we might publish an event of type `"user_message"` or `"input_message"`, including the message content, the conversation ID, and the user’s ID. This decouples the HTTP request from the processing logic — other parts of the system (the core orchestrator, LLM interface, etc.) will subscribe to this event and handle it asynchronously. The API endpoint does not wait for any response from this publish action; it’s fire-and-forget.
4. **Persist the Message (via Memory Service)** – Immediately after publishing, we also persist the user’s message to the memory store (so that it’s not lost even if the response hasn’t been generated yet). In the Cortex architecture, this would typically be done by calling the **Memory Service** through the Model Context Protocol (MCP). For the MVP, we can simulate this by calling a `memory_service` or repository method to save the message in an in-memory store. This call should be non-blocking or quick (the memory service might just acknowledge receipt). We do this to ensure the conversation history is updated. (This step can be as simple as one line: e.g., `memory_service.save_message(user_id, conversation_id, content)` – we are not processing the content, just storing it.)
5. **Return Acknowledgement** – Having handed off the work, the endpoint returns an acknowledgment response. We don’t have a real result to return yet (the answer will come via SSE), so a minimal JSON confirmation is enough. For example, we return an HTTP 202 Accepted status with body `{"status": "received", "conversation_id": "<id>"}`. (HTTP 202 is appropriate since the request is accepted for processing but not yet completed; alternatively, 200 OK can be used with a similar body, but using 202 clarifies that work is pending.)

All of the above should be done _without writing any complex logic in the endpoint itself_. Steps 3 and 4 are delegated calls: the actual `event_bus.publish` and `memory_service.save_message` should be implemented in their respective modules. The endpoint function just calls them.

**FastAPI Router Implementation**: In code, the `app/api/input.py` module might look like:

```python
# app/api/input.py
from fastapi import APIRouter, Depends, HTTPException, status
from app.models.api.request import InputMessage
from app.services import event_bus, memory_service, conversation_service
from app.auth.dependencies import get_current_user  # assuming this is where auth dependency lives

router = APIRouter(prefix="", tags=["chat"])  # No prefix, use root for /input or could use "/chat"

@router.post("/input", status_code=status.HTTP_202_ACCEPTED)
async def receive_user_message(input_msg: InputMessage, current_user=Depends(get_current_user)):
    # current_user is provided by auth dependency, which ensures authentication
    user_id = current_user.id  # or current_user["user_id"], depending on implementation of get_current_user

    # Determine conversation ID to use
    conv_id = input_msg.conversation_id
    if not conv_id:
        conv_id = conversation_service.get_or_create_default_conversation(user_id)
        # The service returns an existing default conversation ID, or creates one if needed.

    # Publish the event for the new message (non-blocking)
    event_payload = {
        "type": "user_message",
        "user_id": user_id,
        "conversation_id": conv_id,
        "content": input_msg.content
    }
    event_bus.publish("user_message", event_payload)

    # Save the message to memory (conversation history)
    try:
        memory_service.save_message(user_id=user_id, conversation_id=conv_id, message=input_msg.content)
    except Exception as e:
        # If memory service fails, we can log and still continue – not crashing the endpoint.
        # (For MVP, assume this always succeeds or handle gracefully.)
        print(f"Warning: failed to save message to memory: {e}")

    # Return acknowledgment
    return {"status": "received", "conversation_id": conv_id}
```

Key points in this implementation:

- We set `status_code=status.HTTP_202_ACCEPTED` for the router, so FastAPI will automatically respond with 202 if the function returns normally. The response body is a simple dict confirming receipt. (No Pydantic response model is strictly needed for this ack, but we could define one if we wanted to enforce structure. For brevity, returning a dict is fine.)
- The `InputMessage` model ensures we have a valid `content` string. If a client omits `content` or provides a non-string, FastAPI/Pydantic will reject the request with a clear error before calling our function. This keeps the handler logic focused and **lean** – no manual validation necessary.
- `get_current_user` ensures `current_user` is available. If the user is not authenticated, the handler won’t execute at all (FastAPI will have raised 401).
- The conversation ID resolution is delegated to `conversation_service.get_or_create_default_conversation`. This service could check if the user already has a default conversation and return its ID, or if not, create a new conversation (perhaps also creating a default workspace if needed). For the scope of this endpoint, we don’t implement that logic here – we just call the service and trust it to do the right thing.
- Event publishing is a one-liner to the `event_bus`. We include all relevant info in the event payload. We don’t wait for any result; we assume the EventBus handles it asynchronously.
- The memory service call is done in a try/except just in case, but ideally it should not throw in normal operation. Even if it fails, we might still return 202 (maybe with a warning logged) because the message was handed to processing. For MVP, this in-memory save is likely to succeed (or we could even skip error handling for simplicity).
- There is _no other logic_: we do not parse the message content, do not call any AI model here, do not decide on responses – all that is out of scope for the API layer. This endpoint simply **enqueues the work and confirms receipt**.

**Example Acknowledgment Response**:
On success, the client would immediately receive a response like:

```json
{
  "status": "received",
  "conversation_id": "abc12345-6789"
}
```

The exact conversation_id returned might be the one provided in the request, or one generated if none was given. The client can use this ID to subscribe to the output stream if not already doing so.

### Testing the Input Endpoint

We will write tests to ensure the `/input` endpoint behaves as expected. Using FastAPI’s TestClient (or HTTPX for async support), we can simulate requests. The tests should cover: successful message sending, validation error for bad input, and authentication requirements.

Assuming we have a FastAPI `app` with the router included, and a fixture `client` for the TestClient, here are example tests (using pytest style):

```python
from fastapi.testclient import TestClient

client = TestClient(app)  # assume app includes the input router and uses a dummy auth for tests

def test_input_message_success():
    # Simulate a valid authenticated request to /input
    token = "valid.jwt.token"  # In a real test, this could be a JWT signed with the test secret or a dummy if auth is bypassed.
    headers = {"Authorization": f"Bearer {token}"}
    payload = {"content": "Hello, world!", "conversation_id": None}

    response = client.post("/input", json=payload, headers=headers)
    assert response.status_code == 202  # we expect 202 Accepted
    data = response.json()
    assert data["status"] == "received"
    assert "conversation_id" in data  # conversation_id should be returned (either provided or generated)
    # If conversation_id was None in request, the response should provide the new/default conv ID (not None).
    assert data["conversation_id"] is not None

def test_input_message_validation_error():
    # Missing the required 'content' field should result in a 422 error
    token = "valid.jwt.token"
    headers = {"Authorization": f"Bearer {token}"}
    bad_payload = {"conversation_id": "workspace1:conv1"}  # content missing

    response = client.post("/input", json=bad_payload, headers=headers)
    assert response.status_code == 422
    # The response JSON should contain details about the validation error
    error = response.json()
    assert error["detail"][0]["loc"] == ["body", "content"]  # Pydantic indicates 'content' is missing

def test_input_message_unauthorized():
    # No Authorization header -> should be 401 Unauthorized
    payload = {"content": "Test message"}
    response = client.post("/input", json=payload)  # no token provided
    assert response.status_code == 401
```

In these tests, we assume that in the test setup `get_current_user` is configured to accept the dummy token (or has been overridden to a no-op that returns a test user). In a real scenario, you might generate a JWT with a known secret for testing. The key point is that with correct auth, the endpoint returns a 202 and the expected JSON, and without auth or with invalid data, it fails accordingly. The use of Pydantic means we don't have to assert the validation logic ourselves – FastAPI's response for a missing field is standardized (we just check for 422). The above tests confirm the endpoint is just doing validation and delegation (since we don't inspect internal event bus calls here, we trust that if status is 202, the internals were called as per code).

---

## Output Endpoint – GET `/output/stream` (Stream Responses)

**Purpose**: The Output endpoint allows clients to receive the AI assistant’s responses in real-time via a streaming connection. This endpoint upgrades an HTTP connection to a **Server-Sent Events (SSE)** stream. When the user is listening on this stream, the server will push events (such as the assistant’s message chunks, final answers, or other notifications) as they become available. The streaming continues until the conversation response is complete or the connection is closed.

**Route Definition**: `GET /output/stream`. This is also a protected endpoint (requires a valid token). It does not consume a JSON body; instead, it might accept query parameters to identify which conversation’s output to stream (if multiple contexts are possible). For MVP, we can allow an optional `conversation_id` query parameter. If not provided, we default to the user's primary or only conversation.

**SSE Streaming**: FastAPI (built on Starlette) supports streaming responses. For SSE, the response should have the media type `text/event-stream` and send data in the format `data: <message>\n\n` for each event. We will implement this by creating an async generator that yields events from an internal subscription, and returning a `StreamingResponse` (or Starlette’s `EventSourceResponse`) using that generator. SSE is one-way (server -> client); the client just keeps the connection open to receive data.

**Internal Mechanism**: The Output endpoint will interface with the **EventBus** or another internal publisher-subscriber mechanism. When a client connects, we subscribe to the event stream for that user (and conversation). For example, the EventBus may allow subscribing to "output events" filtered by conversation or user. It could provide an asyncio Queue, an async iterator, or callback mechanism. In our implementation, we’ll assume an interface where we can do something like `subscription = event_bus.subscribe(user_id, conversation_id)` that returns an async queue/iterator of events. Our endpoint will then continually read from this queue and push events out.

**Processing Flow**: High-level steps for GET /output/stream:

1. **Authenticate** – As with other endpoints, use `Depends(get_current_user)` to ensure the user is authorized. We extract `user_id` from the current user context.
2. **Establish Subscription** – Determine which conversation’s events to stream. If a `conversation_id` query parameter is provided (e.g., `/output/stream?conversation_id=abc123`), subscribe to that conversation’s events. If not provided, subscribe to the default or latest conversation for the user (similar logic to the input default, possibly via a conversation service helper to get the active conv ID). We call the EventBus to create a subscription. For example: `event_queue = event_bus.subscribe(user_id=user_id, conversation_id=conv_id)`. This might register a listener in the EventBus that will receive events (like assistant responses) relevant to that user/conversation.
3. **Stream Setup** – Create an async generator function (or use an async iterator) that will pull events from the subscription and yield them in SSE format. This generator typically runs in a loop: waiting for new events on the queue, then yielding a formatted string like `data: <json_event>\n\n` each time. The loop should break if the client disconnects or if some termination condition is met.
4. **Handle Disconnects** – It’s important to stop the subscription when the client disconnects to avoid resource leaks. FastAPI/Starlette allows checking `client_disconnected` on the request or catching an exception if the response is cancelled. We will include logic to exit the loop and inform the EventBus to unsubscribe if the connection is closed. This ensures the internal queue or listener is cleaned up.
5. **Return Streaming Response** – Finally, return a `StreamingResponse` (with `media_type="text/event-stream"`) that streams from our generator. FastAPI will keep this connection open and continually send events as our generator yields them. There is no conventional “HTTP response body” that closes here; instead, the response stays open until our generator is exhausted (or an error occurs).

We do not perform any heavy transformation on the events – the endpoint should take whatever data the EventBus provides (likely already a JSON-able dict or string) and send it out. This keeps it simple and transparent. We may wrap the event data in the SSE `data:` format and perhaps end each event with a newline newline (which is required by SSE protocol to denote the end of an event message). If using a specialized SSE support (like Starlette’s `EventSourceResponse`), it can handle formatting as well if we give it proper event data.

Additionally, to keep the stream alive, some implementations send periodic comments or heartbeat pings. A simple approach is to yield a comment line (starting with `:`) or a dummy event every X seconds if no real data has been sent, to prevent timeouts. For MVP, this can be optional – if the underlying server or client needs it, we could include a small task to send a heartbeat. Otherwise, we rely on actual events to keep the connection open.

**FastAPI Router Implementation**: In `app/api/output.py`, the code might look like:

```python
# app/api/output.py
from fastapi import APIRouter, Depends, Request
from fastapi.responses import StreamingResponse
from app.services import event_bus, conversation_service
from app.auth.dependencies import get_current_user
import asyncio
import json

router = APIRouter(prefix="", tags=["chat"])

@router.get("/output/stream")
async def stream_response(request: Request, conversation_id: str = None, current_user=Depends(get_current_user)):
    user_id = current_user.id
    conv_id = conversation_id or conversation_service.get_default_conversation(user_id)

    # Subscribe to event bus for this user's conversation
    event_queue = event_bus.subscribe(user_id=user_id, conversation_id=conv_id)

    # Define an async generator to yield SSE events
    async def event_generator():
        try:
            while True:
                # If client disconnected, break out to stop streaming
                if await request.is_disconnected():
                    break
                # Wait for next event from the queue (with an optional timeout to send heartbeat)
                try:
                    event = await asyncio.wait_for(event_queue.get(), timeout=15.0)
                except asyncio.TimeoutError:
                    # No event in 15 seconds, send a heartbeat to keep connection alive
                    yield ": keep-alive\n\n"
                    continue
                # Got an event (assume it's a dict or already JSON string)
                if event is None:
                    # If a None event is used to signal stream termination, break
                    break
                # Format the event as an SSE data line. Ensure it's JSON string if not already.
                data_str = event if isinstance(event, str) else json.dumps(event)
                yield f"data: {data_str}\n\n"
        finally:
            # Cleanup: unsubscribe from the event bus to avoid stale listeners
            event_bus.unsubscribe(event_queue)

    # Return a streaming response with media type for SSE
    return StreamingResponse(event_generator(), media_type="text/event-stream")
```

Let’s break down this implementation:

- We accept an optional `conversation_id` as a query parameter. FastAPI will pass it as `conversation_id` (or None if not provided). We also accept `request: Request` to check the connection state.
- We determine the effective `conv_id` by using the provided one or falling back to a default from `conversation_service` (which might simply return the user's first conversation ID).
- We call `event_bus.subscribe(user_id, conv_id)`. The exact implementation of subscribe will vary. For MVP, it could return an `asyncio.Queue` that the EventBus will put events into whenever a new message for that conversation is ready. We assume such a mechanism exists. If the EventBus uses callback subscriptions instead, we’d adapt accordingly (e.g., use an AsyncGenerator yielding from a callback). Using a queue is a straightforward approach.
- The `event_generator()` is an async generator function that loops forever, pulling events from the queue and yielding them.
  - We use `request.is_disconnected()` to break out if the client has disconnected (to prevent an infinite loop if the client goes away).
  - We use `asyncio.wait_for` with a timeout to periodically send a heartbeat. Here, if no event arrives in 15 seconds, a `TimeoutError` triggers and we yield a comment line `: keep-alive\n\n`. This is an SSE comment event which most clients will ignore but it keeps the connection from idling out. This interval can be tuned or omitted if not needed.
  - If we get an actual event from the queue, we format it. If the event is already a JSON string, use it directly; if it's a Python dict/object, we `json.dumps` it to ensure it’s a string. Then we yield it prefixed by `data: ` and followed by two newlines (the SSE protocol delimiter).
  - We also consider if `event` could be a special termination signal (like None), in which case we break out of the loop gracefully. In normal operation, the stream might not naturally terminate on its own; it will typically be closed by the client or if the server decides to after sending a final event.
  - The `finally` block ensures that no matter how the loop exits (client disconnect or break), we call `event_bus.unsubscribe(event_queue)`. This would inform the EventBus to stop sending events to our queue and possibly clean up internal structures.
- Finally, we return `StreamingResponse(event_generator(), media_type="text/event-stream")`. FastAPI will start the generator and stream results as they are yielded. Each `yield` in the generator sends a chunk down to the client. The response remains open until the generator finishes (or is disconnected).

**Note**: We used `StreamingResponse` from FastAPI. Alternatively, we could use Starlette’s `EventSourceResponse` (from `starlette.responses` or an `sse_starlette` package) which might handle some SSE details for us (like setting headers such as `Cache-Control: no-cache` by default). The above approach is explicit and clear, which is fine for MVP. Ensure that the response header `Content-Type` is `text/event-stream`. (FastAPI’s StreamingResponse will set this from `media_type`.)

The Output endpoint has **no separate Pydantic model** for the response since it's a continuous stream. However, the data being sent are likely JSON messages (e.g., containing the assistant’s partial or full response). If needed, you could define a schema for those events (for instance, a model for `AssistantMessageEvent` with fields like `message`, `timestamp`, etc.), but typically the event content is defined by the core logic, not the API itself. The API just relays it.

**Testing the Output Endpoint**:
Testing a streaming endpoint is a bit different from testing a normal request, but we can still verify its behavior in a basic way. We want to ensure:

- It returns the correct status and content type.
- It requires authentication.
- It actually streams events (this can be simulated or tested with a dummy event bus).

For simplicity in tests, we might replace the real `event_bus.subscribe` with a fake that yields a known sequence of events, or we push an event to it after connecting. Since the TestClient in FastAPI is not fully async-stream aware, we might use `httpx.AsyncClient` for more control, or simply test initial response behavior.

Example tests might look like:

```python
import sseclient  # if using an SSE client library for testing, or use httpx manually
import threading, time

def test_output_stream_requires_auth():
    response = client.get("/output/stream", timeout=5)  # no headers
    # Because no auth header, get_current_user will throw 401 before streaming starts
    assert response.status_code == 401

def test_output_stream_connect_and_receive(monkeypatch):
    # Monkey-patch event_bus.subscribe to return a queue that we control in the test
    test_queue = asyncio.Queue()
    async def fake_subscribe(user_id, conversation_id):
        return test_queue
    monkeypatch.setattr(event_bus, "subscribe", fake_subscribe)

    # Now simulate sending an event after the client connects
    # We'll run the client.get in a separate thread because it's a blocking call that waits for stream events
    received_events = []
    def client_thread_func():
        headers = {"Authorization": "Bearer valid.jwt.token"}
        with client.stream("GET", "/output/stream?conversation_id=testconv", headers=headers) as stream_response:
            assert stream_response.status_code == 200
            assert stream_response.headers["content-type"] == "text/event-stream; charset=utf-8"
            # read a few lines from the stream (this client.stream yields an iterator over lines)
            for line in stream_response.iter_lines():
                if line:
                    received_events.append(line.decode("utf-8"))
                    break  # read only one event for test and break

    t = threading.Thread(target=client_thread_func)
    t.start()
    # Give the thread a moment to connect
    time.sleep(0.1)
    # Push an event into the queue as if the backend produced it
    test_event = {"message": "Hello from assistant"}
    await test_queue.put(test_event)
    # Give time for the event to be sent and received
    time.sleep(0.1)
    # Signal stream termination for cleanup
    await test_queue.put(None)
    t.join(timeout=5)

    # After the stream ended, we should have one event received
    assert any("Hello from assistant" in evt for evt in received_events)
```

In the above pseudocode test, we used `client.stream` which FastAPI’s TestClient provides to handle streaming responses. We monkey-patched the EventBus subscription to control what events get emitted (since we don’t have a real event bus in the test). We then started a thread to run the streaming request (to avoid blocking the main test flow), inserted an event into the queue, and then checked that the client received it. This verifies that the SSE endpoint properly streams data.

However, this is a fairly intricate test. For simpler testing, you might just do a GET request and confirm a 200 status and correct content-type, which shows that the route is defined and accessible. Full testing of the streaming logic might be done in an integration test environment or with a specialized SSE client. The main point is that the output endpoint should respect the contract (auth required, content-type is correct, no immediate closing of connection, etc.).

For MVP unit tests, you could also simulate the generator directly by calling `event_generator()` in isolation if you refactor it out, but that might not be necessary if the above approach works.

---

## Config Endpoints – Workspaces & Conversations

The Config endpoints handle management of user workspaces and conversations. This allows the client (or user) to organize chats into workspaces and to create or list conversation sessions. In the MVP, these are simple helper endpoints with **ephemeral storage** (no real database persistence, just in-memory structures), primarily to illustrate how such configuration might work. They are kept minimal: no complex business rules or relationships beyond associating records with the current user. All config endpoints are also protected (require auth).

We will implement four endpoints:

- `POST /config/workspaces` – Create a new workspace for the authenticated user.
- `GET /config/workspaces` – List all workspaces for the authenticated user.
- `POST /config/conversations` – Create a new conversation in a given workspace.
- `GET /config/conversations` – List conversations in a given workspace (or possibly all conversations for the user, filtered by workspace).

These could be organized under a single router (e.g., an APIRouter with prefix `/config`), or separate routers for workspaces and conversations. For simplicity, we can use one `config_router` in `app/api/config.py` to group them, using distinct paths. Alternatively, use two routers (one with prefix `/config/workspaces` and one with prefix `/config/conversations`), but here we'll illustrate with one module.

### Data Models for Config

Define Pydantic models for the request bodies and responses:

```python
# In app/models/api/request.py (continuing from earlier)
class WorkspaceCreate(BaseModel):
    name: str

class ConversationCreate(BaseModel):
    workspace_id: str
    title: Optional[str] = None  # optional, if we allow setting a title on creation
```

- `WorkspaceCreate`: has a required `name` (the name of the new workspace).
- `ConversationCreate`: requires a `workspace_id` (the workspace under which to create the conversation), and an optional `title` for the conversation. For MVP, we might not use `title` and just leave conversations untitled until they have messages, but the field is there if needed.

For responses, we can define models to represent the created objects. In an MVP in-memory scenario, a Workspace might have an ID and name (and perhaps the user who owns it), and a Conversation might have an ID, its workspace ID, and maybe a title or status. For example, in `app/models/api/response.py`:

```python
from pydantic import BaseModel
from typing import Optional

class Workspace(BaseModel):
    id: str
    name: str

class Conversation(BaseModel):
    id: str
    workspace_id: str
    title: Optional[str] = None
```

We include just the essential fields that we plan to return to the client. (We assume the user ID is not returned as part of the object for simplicity, since these endpoints are scoped to the current user anyway.)

Internally, our in-memory storage might be something like:

- A dictionary mapping user_id -> list of workspaces (each workspace with id and name).
- Another dictionary mapping workspace_id -> list of conversations (each conversation with id, and maybe title).
  For multi-user support, each workspace should know which user owns it (so that one user cannot access another’s workspaces). In our simple approach, since we always filter by current user, we can just ensure we use the user_id key when retrieving.

We will rely on a `workspace_service` and `conversation_service` to handle creation and lookup, rather than manipulating these dictionaries directly in the endpoint. For example, `workspace_service.create_workspace(user_id, name) -> Workspace` and `workspace_service.list_workspaces(user_id) -> List[Workspace]`. Similarly for conversations.

### Implementing Workspace Endpoints

**POST /config/workspaces** – Create a workspace for the user.
Steps:

1. **Auth**: Get `current_user` via dependency to identify the user.
2. **Validate Request**: The request body should conform to `WorkspaceCreate` model (just a name). Pydantic ensures a `name` is provided and is a string.
3. **Call Service**: Invoke the `workspace_service.create_workspace(user_id, name)`. This service will generate a unique workspace ID (e.g., using `uuid4()` or a simple counter) and store the workspace (e.g., append to user's workspace list in memory). It returns a Workspace object (or dict) with the new `id` and `name`.
4. **Return Response**: Respond with the created workspace data. We can use `response_model=Workspace` in the route decorator so FastAPI will serialize the response via that model (ensuring only id and name are returned, for example). The status code can be 201 Created.

**GET /config/workspaces** – List all workspaces for the user.
Steps:

1. **Auth**: `current_user` as usual.
2. **Call Service**: `workspace_service.list_workspaces(user_id)`. This returns a list of Workspace objects (or dicts) for that user. If the user has none, it might return an empty list.
3. **Return Response**: Return the list. We can specify `response_model=List[Workspace]` so the output is enforced as a list of Workspace model items. Status code 200 OK.

We are not doing pagination or detailed filtering in MVP; all of the user's workspaces are returned. The service might simply retrieve from a global in-memory store using the user_id key.

**FastAPI Implementation (Workspaces)**:

```python
# In app/api/config.py (part of the config router)
from fastapi import APIRouter, Depends, HTTPException, status
from typing import List
from app.models.api import request as api_req, response as api_res
from app.services import workspace_service, conversation_service
from app.auth.dependencies import get_current_user

router = APIRouter(prefix="/config", tags=["config"])

@router.post("/workspaces", response_model=api_res.Workspace, status_code=status.HTTP_201_CREATED)
def create_workspace(workspace: api_req.WorkspaceCreate, current_user=Depends(get_current_user)):
    user_id = current_user.id
    # Delegate to service to create a new workspace
    new_workspace = workspace_service.create_workspace(user_id=user_id, name=workspace.name)
    return new_workspace

@router.get("/workspaces", response_model=List[api_res.Workspace])
def list_workspaces(current_user=Depends(get_current_user)):
    user_id = current_user.id
    workspaces = workspace_service.list_workspaces(user_id=user_id)
    return workspaces
```

A few things to note:

- The request model `WorkspaceCreate` is used for the POST body, and the response model `Workspace` is used for the output. This makes FastAPI automatically document the endpoint and also filter out any extra fields (for example, if the service returns an object with an owner_id internally, the Pydantic `Workspace` model will ensure only id and name appear in the response).
- The `workspace_service` should handle ID generation and data persistence. For instance, it might do something like: generate a UUID, create a dict `{"id": uuid, "name": name}`, store it in a global `WORKSPACES[user_id]` list, and return a `Workspace` Pydantic model (which can be created by `api_res.Workspace(**workspace_dict)`). This logic is abstracted away from the endpoint.
- We trust the service to also ensure uniqueness of IDs, etc. If a name is provided that’s empty or so, Pydantic would have caught if it wasn’t a string (we might want to enforce non-empty string via a validator, but we can assume the name is at least one character as a business rule or just let any string).
- No additional error handling is shown here (e.g., checking if the workspace name already exists). For MVP, we assume users can have multiple workspaces with the same name or we don't enforce unique names (that's a business decision left out for now). The endpoint simply creates a new one every time it's called.

### Implementing Conversation Endpoints

**POST /config/conversations** – Create a new conversation within a workspace.
Steps:

1. **Auth**: Get current user.
2. **Validate Request**: Body uses `ConversationCreate` model, requiring a `workspace_id`. (We expect the client to specify which workspace the conversation belongs to. Alternatively, if they omit it, we could default to a "default workspace", but since the model requires it, we'll assume it's provided. If needed, the workspace service could provide a default workspace ID if none given, similar to conversation default logic.)
3. **Call Service**: `conversation_service.create_conversation(user_id, workspace_id, title=None)`. This will create a new conversation record. It should verify that the workspace exists and belongs to the user (to avoid creating conversations in someone else’s workspace). For MVP in-memory, the service can check if `workspace_id` is in the user's workspace list; if not, it could raise an error (which we would translate to 404 or 400). We'll include a basic check. Then generate a new `conversation_id` (UUID or similar), create the conversation object (with id, workspace_id, and maybe title if provided or default title like "Conversation 1"), store it (e.g., in a global dictionary mapping workspace_id to conversations list), and return the conversation data.
4. **Return Response**: Return the created conversation. Use `response_model=Conversation` and status 201 Created.

**GET /config/conversations** – List conversations in a workspace.
Steps:

1. **Auth**: current_user as usual.
2. **Determine Workspace**: We need to know which workspace’s conversations to list. We can take a query parameter, e.g. `/config/conversations?workspace_id=<id>`. (In our function signature, include `workspace_id: str` as a query param.) If the client does not provide it, we might default to a primary workspace (if the product defined one) or return all conversations across all the user's workspaces. However, the design doc specified "in a given workspace", so we will require this parameter for clarity. If not provided, we can return a 400 or choose to list all – for MVP, let's assume it is provided.
3. **Call Service**: `conversation_service.list_conversations(user_id, workspace_id)`. This returns the list of Conversation objects under that workspace for that user. If the workspace doesn’t exist or doesn’t belong to the user, the service could raise an error (which we handle by returning 404 or 400 accordingly).
4. **Return Response**: Return the list of conversations (`response_model=List[Conversation]`, status 200).

**FastAPI Implementation (Conversations)**:

```python
@router.post("/conversations", response_model=api_res.Conversation, status_code=status.HTTP_201_CREATED)
def create_conversation(conv_req: api_req.ConversationCreate, current_user=Depends(get_current_user)):
    user_id = current_user.id
    # Validate that the workspace exists for this user
    if not workspace_service.workspace_exists(user_id=user_id, workspace_id=conv_req.workspace_id):
        # If the workspace is not found or not owned by user, raise 404
        raise HTTPException(status_code=404, detail="Workspace not found")
    new_conv = conversation_service.create_conversation(user_id=user_id, workspace_id=conv_req.workspace_id, title=conv_req.title)
    return new_conv

@router.get("/conversations", response_model=List[api_res.Conversation])
def list_conversations(workspace_id: str, current_user=Depends(get_current_user)):
    user_id = current_user.id
    if not workspace_service.workspace_exists(user_id=user_id, workspace_id=workspace_id):
        raise HTTPException(status_code=404, detail="Workspace not found")
    conversations = conversation_service.list_conversations(user_id=user_id, workspace_id=workspace_id)
    return conversations
```

In the create route, we explicitly check that the provided workspace belongs to the current user using a helper `workspace_exists`. This is a bit of business logic (authorization check) that we allow here to prevent creating conversations in invalid contexts. We could also delegate this check to `conversation_service.create_conversation` (which could return an error or exception if workspace is invalid). Either approach is fine; doing it in the service might keep the endpoint even thinner. But it's a one-liner check, and it avoids trying to create and then failing inside the service, so it's acceptable minimal logic.

If the workspace doesn’t exist, we return 404 Not Found. (Alternatively, 400 Bad Request could be argued, but 404 for "workspace not found" is clear enough.)

The `conversation_service.create_conversation` will handle ID generation and storing the conversation. It likely also sets the conversation title: if `title` was provided in request, use it; otherwise maybe create a default title like "New Conversation" or leave it None/empty (client can later update the title once there's content). For listing, the service returns all convs for that workspace.

### Testing the Config Endpoints

We will test the workspace and conversation endpoints to ensure they create and retrieve data properly and enforce authentication. Since these endpoints rely on an in-memory store or service, tests can call them sequentially to simulate usage.

**Testing Workspaces**:

```python
def test_create_and_list_workspaces():
    token = "valid.jwt.token"
    headers = {"Authorization": f"Bearer {token}"}
    # Create a new workspace
    workspace_payload = {"name": "Project Alpha"}
    res = client.post("/config/workspaces", json=workspace_payload, headers=headers)
    assert res.status_code == 201
    created = res.json()
    assert created["name"] == "Project Alpha"
    assert "id" in created

    # List workspaces and ensure the new workspace is present
    res2 = client.get("/config/workspaces", headers=headers)
    assert res2.status_code == 200
    workspaces = res2.json()
    # The list should contain at least the one we added
    assert any(ws["name"] == "Project Alpha" for ws in workspaces)
    # The returned workspace in list should have the same id as the created one
    assert created["id"] in [ws["id"] for ws in workspaces]

def test_create_conversation_and_list():
    token = "valid.jwt.token"
    headers = {"Authorization": f"Bearer {token}"}
    # First, create a workspace to use for the conversation
    ws_payload = {"name": "Chat Workspace"}
    res = client.post("/config/workspaces", json=ws_payload, headers=headers)
    assert res.status_code == 201
    workspace_id = res.json()["id"]

    # Create a conversation in that workspace
    conv_payload = {"workspace_id": workspace_id}
    res2 = client.post("/config/conversations", json=conv_payload, headers=headers)
    assert res2.status_code == 201
    conv = res2.json()
    assert conv["workspace_id"] == workspace_id
    assert "id" in conv

    # List conversations for the workspace
    res3 = client.get(f"/config/conversations?workspace_id={workspace_id}", headers=headers)
    assert res3.status_code == 200
    conv_list = res3.json()
    # Check that the newly created conversation is in the list
    assert any(c["id"] == conv["id"] for c in conv_list)
```

In the first test, we POST to create a workspace and then GET to list them, verifying the created one appears. In the second test, we chain creation of a workspace and then a conversation in that workspace, followed by listing conversations. This ensures the relationships are working. We check that the conversation’s `workspace_id` matches, and the IDs are returned correctly.

We should also test some error cases:

- Creating a conversation with an invalid workspace_id should yield a 404.
- Access without a token yields 401 for these endpoints as well.

Example:

```python
def test_create_conversation_invalid_workspace():
    token = "valid.jwt.token"
    headers = {"Authorization": f"Bearer {token}"}
    bad_conv_payload = {"workspace_id": "nonexistent-id"}
    res = client.post("/config/conversations", json=bad_conv_payload, headers=headers)
    assert res.status_code == 404
    error = res.json()
    assert "Workspace not found" in error["detail"]
```

And a quick unauthorized test:

```python
def test_workspace_endpoints_unauthorized():
    # No token -> should not allow access
    res = client.get("/config/workspaces")
    assert res.status_code == 401
    res2 = client.post("/config/workspaces", json={"name": "X"})
    assert res2.status_code == 401
```

These tests ensure that authentication dependency is working (similar to the input test) and that our basic validation (like requiring workspace_id on conv create) is enforced by Pydantic. For example, if `workspace_id` was missing in the conversation payload, FastAPI would return 422 before hitting our function.

---

## Conclusion

By following this guide, the AI assistant should implement the Input, Output, and Config API endpoints with confidence and clarity. The endpoints are intentionally kept lightweight and focused:

- **Complete Isolation**: The API handlers contain no business logic or direct data management; they simply validate and forward requests to the appropriate internal channels. This means the assistant can work on these endpoints without needing to coordinate on core logic – those are abstracted behind services and events. The boundaries are well-defined: once the request is handed off (published or delegated), the endpoint’s job is done.
- **Modularity**: Each set of endpoints lives in its own module and uses Pydantic models defined in one place. This modular approach ensures that the code can be easily tested and maintained. The use of APIRouter allows integrating these endpoints into the larger application without conflicts, and any future changes (like adding new fields or adjusting validation) are localized to these modules.
- **Alignment with Architecture**: The design mirrors the overall Cortex architecture: one-way message input, one-way streaming output, and separate config management. Using SSE for output meets the requirement for real-time updates while keeping the implementation straightforward. The token-based security on each endpoint aligns with the stateless auth approach using Auth0 JWTs.
- **Minimal Implementation, Maximum Clarity**: In line with the project’s philosophy, we avoided unnecessary complexity. We leveraged FastAPI’s strengths (dependency injection, Pydantic models, automatic docs and validation) to write very little code in the endpoint functions themselves. This makes the endpoints easy to understand at a glance and less prone to errors. The heavy lifting (event distribution, data storage, AI processing) is handled elsewhere.

The assistant should also ensure all endpoints are registered in the main FastAPI app (e.g., `app.include_router(input.router)`, etc.), and that any required dependencies (EventBus instance, services) are properly initialized/injected. By writing comprehensive tests for each endpoint, as illustrated above, we can verify that the contract is satisfied (correct status codes, JSON responses, auth enforcement) without delving into internal logic. These tests act as a safety net as well as documentation of the expected behavior.

Following this implementation guide will result in a cleanly separated API layer for Cortex Core MVP, allowing different team members or AI agents to work concurrently on other parts (like the memory service, the core orchestrator that subscribes to events, etc.) without stepping on each other’s toes. The endpoints will be ready to plug into the overall system and handle real requests consistent with the design specifications.
