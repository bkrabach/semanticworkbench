# LiteLLM Integration Guide for Simplified Cortex

This document outlines how to implement LiteLLM integration in our simplified Cortex design, maintaining compatibility with multiple language model providers while reducing implementation complexity.

## Core LiteLLM Requirements

1. **Provider Abstraction**: Use LiteLLM to abstract differences between LLM providers (OpenAI, Anthropic, etc.).

2. **Unified Interface**: Provide a consistent interface for both streaming and non-streaming completions.

3. **Tool Support**: Maintain support for function/tool calling capabilities.

4. **Error Handling**: Implement robust but simplified error handling for LLM calls.

## Simplified LLM Service Implementation

Our implementation will be significantly simpler than the current one while maintaining the core functionality:

### Core Service Interface

```python
class LlmService:
    """Service for interacting with language models through LiteLLM"""
    
    def __init__(self, config=None):
        """Initialize the LLM service with optional configuration"""
        self.config = config or {}
        self.default_model = self.config.get("default_model", "openai/gpt-3.5-turbo")
        self.use_mock = self.config.get("use_mock", False)
        self.timeout = self.config.get("timeout", 60)
        
    async def get_completion(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        model: Optional[str] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
    ) -> str:
        """
        Get a completion from an LLM for the given prompt
        
        Args:
            prompt: The user prompt to send to the LLM
            system_prompt: Optional system prompt to set context
            temperature: Temperature for generation (0.0-2.0)
            max_tokens: Maximum tokens to generate
            model: The model to use (defaults to config setting)
            tools: Optional list of tools to provide to the model
            
        Returns:
            The text response from the model
        """
        pass
    
    async def get_streaming_completion(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        model: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """
        Get a streaming completion from an LLM
        
        Args:
            prompt: The user prompt to send to the LLM
            system_prompt: Optional system prompt to set context
            temperature: Temperature for generation (0.0-2.0)
            max_tokens: Maximum tokens to generate
            model: The model to use (defaults to config setting)
            
        Yields:
            Text chunks as they are generated by the model
        """
        pass
```

### Simplified Implementation

```python
import asyncio
import logging
from typing import Dict, List, Optional, Any, AsyncGenerator

# Conditional import for LiteLLM
try:
    from litellm import acompletion
    HAS_LITELLM = True
except ImportError:
    HAS_LITELLM = False
    logging.warning("LiteLLM not installed, LLM service will operate in mock mode")

class LlmService:
    """Simplified service for interacting with language models through LiteLLM"""
    
    def __init__(self, config=None):
        """Initialize the LLM service with optional configuration"""
        self.config = config or {}
        self.default_model = self.config.get("default_model", "openai/gpt-3.5-turbo")
        self.use_mock = self.config.get("use_mock", False) or not HAS_LITELLM
        self.timeout = self.config.get("timeout", 60)
        self.logger = logging.getLogger(__name__)
        
        if self.use_mock:
            self.logger.warning("LLM service running in mock mode - no actual API calls will be made")
    
    async def get_completion(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        model: Optional[str] = None,
        tools: Optional[List[Dict[str, Any]]] = None,
    ) -> str:
        """Get a completion from an LLM for the given prompt"""
        if self.use_mock:
            # In mock mode, just echo the prompt with info about the mock
            await asyncio.sleep(1)  # Simulate API delay
            tool_info = f" with {len(tools)} tools" if tools else ""
            return f"[MOCK LLM RESPONSE{tool_info}] Echo: {prompt}"
        
        try:
            # Prepare messages in the format LiteLLM expects
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            # Use the model specified or fall back to default
            model_name = model or self.default_model
            
            # Prepare kwargs for additional parameters
            kwargs = {}
            
            # Add tools if provided
            if tools:
                self.logger.info(f"Providing {len(tools)} tools to LLM")
                kwargs["tools"] = tools
                kwargs["tool_choice"] = "auto"  # Let the model decide when to use tools
            
            # Call LiteLLM async completion
            response = await acompletion(
                model=model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=self.timeout,
                **kwargs
            )
            
            # Extract content from response
            if response and hasattr(response, "choices") and response.choices:
                message = response.choices[0].message
                content = message.content or ""
                return content
            else:
                self.logger.error(f"Invalid response structure from LLM")
                return ""
                
        except Exception as e:
            self.logger.error(f"Error calling LLM: {str(e)}")
            return f"Error processing request: {str(e)}"
    
    async def get_streaming_completion(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: Optional[int] = None,
        model: Optional[str] = None,
    ) -> AsyncGenerator[str, None]:
        """Get a streaming completion from an LLM"""
        if self.use_mock:
            # In mock mode, simulate streaming with artificial delays
            words = f"[MOCK LLM STREAMING RESPONSE] Echo: {prompt}".split()
            for word in words:
                await asyncio.sleep(0.1)
                yield word + " "
            return
        
        try:
            # Prepare messages in the format LiteLLM expects
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            # Use the model specified or fall back to default
            model_name = model or self.default_model
            
            # Call LiteLLM async completion with streaming
            stream = await acompletion(
                model=model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=self.timeout,
                stream=True
            )
            
            # Yield content chunks as they come in
            async for chunk in stream:
                if (hasattr(chunk, "choices") and chunk.choices and 
                    hasattr(chunk.choices[0], "delta") and 
                    hasattr(chunk.choices[0].delta, "content") and 
                    chunk.choices[0].delta.content):
                    yield chunk.choices[0].delta.content
                    
        except Exception as e:
            self.logger.error(f"Error in streaming LLM call: {str(e)}")
            yield f"Error: {str(e)}"

# Singleton instance
_llm_service = None

def get_llm_service(config=None):
    """Get the global LLM service instance"""
    global _llm_service
    if _llm_service is None:
        _llm_service = LlmService(config)
    return _llm_service
```

## Key Simplifications

Compared to the original implementation, this simplified version:

1. **Reduces Type Complexity**: Eliminates the complex type definitions and type casting
2. **Simplifies Error Handling**: Cleaner error handling logic with fewer nested blocks
3. **Reduces Adapter Layers**: Directly uses LiteLLM without intermediate adapter functions
4. **Removes Unnecessary Classes**: No wrapper classes for responses
5. **Maintains Core Functionality**: Still supports all essential features (streaming, tools, etc.)

## Integration with Router

The LLM Service will be integrated with the Router component:

```python
async def _handle_message(self, message: InputMessage):
    """Process a message and generate a response"""
    try:
        # Show typing indicator
        await self._send_typing_indicator(message.conversation_id, True)
        
        # Get LLM service
        llm_service = get_llm_service()
        
        # Get response from LLM
        response_content = await llm_service.get_completion(
            prompt=message.content,
            system_prompt="You are a helpful assistant.",
        )
        
        # Save to database and send to client
        await self._send_message_to_client(
            message.conversation_id,
            response_content,
            "assistant"
        )
    finally:
        # Always turn off typing indicator
        await self._send_typing_indicator(message.conversation_id, False)
```

## Configuration Settings

```python
# Example configuration in environment variables
LLM_DEFAULT_MODEL=openai/gpt-3.5-turbo
LLM_USE_MOCK=false
LLM_TIMEOUT=60

# Provider API keys
OPENAI_API_KEY=your-openai-api-key
ANTHROPIC_API_KEY=your-anthropic-api-key
```

## Testing Approach

The simplified design makes testing more straightforward:

```python
import pytest
from unittest.mock import patch, AsyncMock

@pytest.mark.asyncio
async def test_llm_service_completion():
    """Test the LLM service completion method"""
    # Create a mock for acompletion
    mock_response = AsyncMock()
    mock_response.choices = [AsyncMock()]
    mock_response.choices[0].message.content = "Test response"
    
    with patch("litellm.acompletion", return_value=mock_response):
        # Create LLM service
        service = LlmService({"use_mock": False})
        
        # Test get_completion
        response = await service.get_completion("Test prompt")
        
        # Verify the response
        assert response == "Test response"
```

## Handling Tools/Function Calling

For tool usage, the implementation maintains the clean approach:

```python
# Example tool definition
tools = [
    {
        "type": "function",
        "function": {
            "name": "get_weather",
            "description": "Get current weather for a location",
            "parameters": {
                "type": "object",
                "properties": {
                    "location": {
                        "type": "string",
                        "description": "City name"
                    }
                },
                "required": ["location"]
            }
        }
    }
]

# Using tools with LLM service
response = await llm_service.get_completion(
    prompt="What's the weather in San Francisco?",
    tools=tools
)
```

## Implementation Timeline

1. **Day 1**: Basic LLM service implementation with completion support
2. **Day 2**: Add streaming support and integration with Router
3. **Day 3**: Implement tool calling capabilities
4. **Day 4**: Add testing and error handling improvements
5. **Day 5**: Performance optimization and documentation

## Next Steps

1. Implement the simplified LlmService class
2. Create configuration handling
3. Build integration with the Router component
4. Add comprehensive error handling
5. Create tests for the LLM service